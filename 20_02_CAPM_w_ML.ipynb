{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1wiwP3D_NdgZtL-XKCyq_squ3agep2frz",
      "authorship_tag": "ABX9TyMCkWLWyrAqDGNwOQAKaz8F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Agrigori7/ML-on-CAPM/blob/main/20_02_CAPM_w_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LWdbg2a8mhSo",
        "outputId": "3d209934-5d47-438d-f138-6bcdcd04464b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Building Dataset (daily % units) ---\n",
            "Using Fama–French Daily Factors (Mkt-RF, RF) as market / RF.\n",
            "Data Ready: 2700 rows. Columns: ['nvda_px', 'excess_mkt', 'rf', 'nvda_ret', 'excess_nvda']\n",
            "\n",
            "=== OLS Summary: CAPM (Full Sample) ===\n",
            "==============================================================================\n",
            "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "const          0.1735      0.045      3.870      0.000       0.086       0.261\n",
            "excess_mkt     1.7383      0.039     45.130      0.000       1.663       1.814\n",
            "==============================================================================\n",
            "\n",
            "Diagnostics:\n",
            "alpha     : 0.1735\n",
            "beta      : 1.7383\n",
            "alpha_t   : 3.8700\n",
            "beta_t    : 45.1304\n",
            "R2        : 0.4302\n",
            "BP_p      : 0.9440\n",
            "RESET_p   : 0.3035\n",
            "JB_p      : 0.0000\n",
            "DW        : 2.0850\n",
            "\n",
            "--- Regime thresholds (FULL sample) ---\n",
            "q25 = -0.400 pps | q75 = 0.603 pps\n",
            "\n",
            "Regime counts:\n",
            "Regime\n",
            "Normal (Mid 50%)    1351\n",
            "Bull (Top 25%)       675\n",
            "Bear (Bot 25%)       674\n",
            "Name: count, dtype: int64\n",
            "\n",
            "===================================================\n",
            "   DESIGN A: IN-SAMPLE PERFORMANCE (OVERALL + BY REGIME)\n",
            "===================================================\n",
            "Model                              GBR       OLS        RF       SVR\n",
            "Sample                                                              \n",
            "Bear (Bot 25%) (in-sample)    0.430417  0.309769  0.379675  0.322288\n",
            "Bull (Top 25%) (in-sample)    0.331622  0.251133  0.284361  0.259456\n",
            "Full Sample (in-sample)       0.478302  0.430171  0.459776  0.433945\n",
            "Normal (Mid 50%) (in-sample)  0.057616  0.038266  0.065406  0.035897\n",
            "\n",
            "=== Kruskal–Wallis tests on |errors| across regimes (in-sample) ===\n",
            "Model: OLS  H = 43.118, p = 0.0000\n",
            "Model: RF   H = 31.579, p = 0.0000\n",
            "Model: SVR  H = 39.665, p = 0.0000\n",
            "Model: GBR  H = 21.828, p = 0.0000\n",
            "\n",
            "=== Wald Tests: Are CAPM betas different across regimes? (Full sample) ===\n",
            "==============================================================================\n",
            "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "const          0.1819      0.056      3.234      0.001       0.072       0.292\n",
            "excess_mkt     1.7166      0.206      8.317      0.000       1.312       2.121\n",
            "MKT_Bear       0.0331      0.224      0.148      0.883      -0.406       0.473\n",
            "MKT_Bull       0.0102      0.213      0.048      0.962      -0.407       0.428\n",
            "==============================================================================\n",
            "\n",
            "--- Beta Equality Tests (Wald, Full Sample) ---\n",
            "Bear vs Normal : t=0.148, p=0.8826\n",
            "Bull vs Normal : t=0.048, p=0.9620\n",
            "Bear vs Bull   : t=0.204, p=0.8386\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# ============================================================\n",
        "# DESIGN A (STRUCTURAL / FULL SAMPLE):\n",
        "# CAPM vs ML fit, and regime-conditional performance (25/50/25)\n",
        "# NVDA 2015–2025\n",
        "# ============================================================\n",
        "\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import yfinance as yf\n",
        "from pandas_datareader import data as pdr\n",
        "\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.stats.diagnostic import het_breuschpagan, linear_reset\n",
        "from statsmodels.stats.stattools import durbin_watson\n",
        "from scipy.stats import jarque_bera, kruskal\n",
        "\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "np.random.seed(42)\n",
        "\n",
        "START = \"2015-01-01\"\n",
        "END   = \"2025-12-31\"\n",
        "\n",
        "# -------------------------\n",
        "# 1) DATA\n",
        "# -------------------------\n",
        "def _flatten_yf(px: pd.DataFrame) -> pd.DataFrame:\n",
        "    px = px.copy()\n",
        "    if isinstance(px.columns, pd.MultiIndex):\n",
        "        px.columns = [c[0] if isinstance(c, tuple) else c for c in px.columns]\n",
        "    px.columns.name = None\n",
        "    return px\n",
        "\n",
        "def _yf_adj_close(ticker, start, end):\n",
        "    df = yf.download(ticker, start=start, end=end, progress=False, auto_adjust=True)\n",
        "    df = _flatten_yf(df)\n",
        "    for col in [\"Adj Close\", \"Close\", ticker]:\n",
        "        if col in df.columns:\n",
        "            return df[[col]].rename(columns={col: ticker})\n",
        "    raise ValueError(f\"Could not find price column for {ticker}.\")\n",
        "\n",
        "def build_dataset():\n",
        "    print(\"--- Building Dataset (daily % units) ---\")\n",
        "\n",
        "    nvda = _yf_adj_close(\"NVDA\", START, END).rename(columns={\"NVDA\": \"nvda_px\"})\n",
        "\n",
        "    ff_raw = pdr.DataReader(\n",
        "        \"F-F_Research_Data_Factors_daily\",\n",
        "        \"famafrench\",\n",
        "        start=START,\n",
        "        end=END\n",
        "    )[0]\n",
        "\n",
        "    # already in percent\n",
        "    ff = ff_raw[[\"Mkt-RF\", \"RF\"]].rename(columns={\"Mkt-RF\": \"excess_mkt\", \"RF\": \"rf\"})\n",
        "    print(\"Using Fama–French Daily Factors (Mkt-RF, RF) as market / RF.\")\n",
        "\n",
        "    df = nvda.join(ff, how=\"inner\").dropna()\n",
        "    df[\"nvda_ret\"] = df[\"nvda_px\"].pct_change() * 100.0\n",
        "    df[\"excess_nvda\"] = df[\"nvda_ret\"] - df[\"rf\"]\n",
        "    df = df.dropna()\n",
        "\n",
        "    print(f\"Data Ready: {df.shape[0]} rows. Columns: {list(df.columns)}\")\n",
        "    return df\n",
        "\n",
        "df = build_dataset()\n",
        "\n",
        "# -------------------------\n",
        "# 2) CAPM OLS + diagnostics\n",
        "# -------------------------\n",
        "def run_ols_diagnostics(df_subset, label=\"Full Sample\"):\n",
        "    X = sm.add_constant(df_subset[\"excess_mkt\"])\n",
        "    y = df_subset[\"excess_nvda\"]\n",
        "    ols = sm.OLS(y, X).fit()\n",
        "\n",
        "    resid = ols.resid\n",
        "    bp_lm, bp_pvalue, _, _ = het_breuschpagan(resid, ols.model.exog)\n",
        "    reset_res = linear_reset(ols, power=2, use_f=True)\n",
        "    jb_stat, jb_pvalue = jarque_bera(resid)\n",
        "    dw_stat = durbin_watson(resid)\n",
        "\n",
        "    diagnostics = {\n",
        "        \"alpha\": float(ols.params[\"const\"]),\n",
        "        \"beta\": float(ols.params[\"excess_mkt\"]),\n",
        "        \"alpha_t\": float(ols.tvalues[\"const\"]),\n",
        "        \"beta_t\": float(ols.tvalues[\"excess_mkt\"]),\n",
        "        \"R2\": float(ols.rsquared),\n",
        "        \"BP_p\": float(bp_pvalue),\n",
        "        \"RESET_p\": float(reset_res.pvalue),\n",
        "        \"JB_p\": float(jb_pvalue),\n",
        "        \"DW\": float(dw_stat)\n",
        "    }\n",
        "\n",
        "    print(f\"\\n=== OLS Summary: {label} ===\")\n",
        "    print(ols.summary().tables[1])\n",
        "\n",
        "    print(\"\\nDiagnostics:\")\n",
        "    for k, v in diagnostics.items():\n",
        "        print(f\"{k:10s}: {v:.4f}\")\n",
        "\n",
        "    return ols, diagnostics\n",
        "\n",
        "full_ols, full_diag = run_ols_diagnostics(df, \"CAPM (Full Sample)\")\n",
        "\n",
        "# -------------------------\n",
        "# 3) ML models fit on FULL sample (Design A)\n",
        "# -------------------------\n",
        "def fit_models_full_sample(df_all):\n",
        "    X = df_all[[\"excess_mkt\"]].values\n",
        "    y = df_all[\"excess_nvda\"].values\n",
        "\n",
        "    models = {}\n",
        "\n",
        "    # OLS via sklearn (same functional form as CAPM)\n",
        "    ols = LinearRegression().fit(X, y)\n",
        "    models[\"OLS\"] = ols\n",
        "\n",
        "    # RF\n",
        "    rf = RandomForestRegressor(\n",
        "        n_estimators=300,\n",
        "        max_depth=5,\n",
        "        min_samples_leaf=5,\n",
        "        random_state=42\n",
        "    ).fit(X, y)\n",
        "    models[\"RF\"] = rf\n",
        "\n",
        "    # SVR (scaled)\n",
        "    svr = Pipeline([\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"svm\", SVR(kernel=\"rbf\", C=10, gamma=\"scale\"))\n",
        "    ]).fit(X, y)\n",
        "    models[\"SVR\"] = svr\n",
        "\n",
        "    # GBR (robust loss)\n",
        "    gbr = GradientBoostingRegressor(\n",
        "        n_estimators=500,\n",
        "        learning_rate=0.05,\n",
        "        max_depth=2,\n",
        "        loss=\"huber\",\n",
        "        random_state=42\n",
        "    ).fit(X, y)\n",
        "    models[\"GBR\"] = gbr\n",
        "\n",
        "    return models\n",
        "\n",
        "models = fit_models_full_sample(df)\n",
        "\n",
        "# -------------------------\n",
        "# 4) Regimes: FULL-SAMPLE quantiles (25/50/25)\n",
        "# -------------------------\n",
        "q25 = df[\"excess_mkt\"].quantile(0.25)\n",
        "q75 = df[\"excess_mkt\"].quantile(0.75)\n",
        "\n",
        "def assign_regime(x):\n",
        "    if x < q25:\n",
        "        return \"Bear (Bot 25%)\"\n",
        "    elif x > q75:\n",
        "        return \"Bull (Top 25%)\"\n",
        "    else:\n",
        "        return \"Normal (Mid 50%)\"\n",
        "\n",
        "df_eval = df.copy()\n",
        "df_eval[\"Regime\"] = df_eval[\"excess_mkt\"].apply(assign_regime)\n",
        "\n",
        "print(\"\\n--- Regime thresholds (FULL sample) ---\")\n",
        "print(f\"q25 = {q25:.3f} pps | q75 = {q75:.3f} pps\")\n",
        "print(\"\\nRegime counts:\")\n",
        "print(df_eval[\"Regime\"].value_counts())\n",
        "\n",
        "# -------------------------\n",
        "# 5) In-sample fit + regime-conditional performance\n",
        "# -------------------------\n",
        "def metrics_block(y_true, y_pred):\n",
        "    return {\n",
        "        \"R2\": r2_score(y_true, y_pred),\n",
        "        \"RMSE\": np.sqrt(mean_squared_error(y_true, y_pred)),\n",
        "        \"MAE\": mean_absolute_error(y_true, y_pred)\n",
        "    }\n",
        "\n",
        "results = []\n",
        "abs_errors = {}  # (model, regime) -> abs error array\n",
        "\n",
        "X_all = df_eval[[\"excess_mkt\"]].values\n",
        "y_all = df_eval[\"excess_nvda\"].values\n",
        "\n",
        "# global predictions on the SAME observations\n",
        "preds_all = {name: mdl.predict(X_all) for name, mdl in models.items()}\n",
        "\n",
        "# overall metrics\n",
        "for name, y_hat in preds_all.items():\n",
        "    m = metrics_block(y_all, y_hat)\n",
        "    results.append({\"Sample\": \"Full Sample (in-sample)\", \"Model\": name, **m})\n",
        "\n",
        "# regime metrics + store abs errors\n",
        "for regime, g in df_eval.groupby(\"Regime\"):\n",
        "    Xg = g[[\"excess_mkt\"]].values\n",
        "    yg = g[\"excess_nvda\"].values\n",
        "    for name, mdl in models.items():\n",
        "        y_hat_g = mdl.predict(Xg)\n",
        "        m = metrics_block(yg, y_hat_g)\n",
        "        results.append({\"Sample\": f\"{regime} (in-sample)\", \"Model\": name, **m})\n",
        "        abs_errors[(name, regime)] = np.abs(yg - y_hat_g)\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "print(\"\\n===================================================\")\n",
        "print(\"   DESIGN A: IN-SAMPLE PERFORMANCE (OVERALL + BY REGIME)\")\n",
        "print(\"===================================================\")\n",
        "print(results_df.pivot(index=\"Sample\", columns=\"Model\", values=\"R2\"))\n",
        "\n",
        "# -------------------------\n",
        "# 6) Kruskal–Wallis: do |errors| differ across regimes?\n",
        "# -------------------------\n",
        "regime_order = [\"Bear (Bot 25%)\", \"Normal (Mid 50%)\", \"Bull (Top 25%)\"]\n",
        "model_list = [\"OLS\", \"RF\", \"SVR\", \"GBR\"]\n",
        "\n",
        "print(\"\\n=== Kruskal–Wallis tests on |errors| across regimes (in-sample) ===\")\n",
        "for m in model_list:\n",
        "    samples = [abs_errors[(m, r)] for r in regime_order]\n",
        "    H, p = kruskal(*samples)\n",
        "    print(f\"Model: {m:3s}  H = {H:.3f}, p = {p:.4f}\")\n",
        "\n",
        "# -------------------------\n",
        "# 7) Wald test: beta equality across regimes (CAPM slope shifts)\n",
        "# -------------------------\n",
        "print(\"\\n=== Wald Tests: Are CAPM betas different across regimes? (Full sample) ===\")\n",
        "\n",
        "df_test = df_eval.copy()\n",
        "df_test[\"Bear\"] = (df_test[\"Regime\"] == \"Bear (Bot 25%)\").astype(int)\n",
        "df_test[\"Bull\"] = (df_test[\"Regime\"] == \"Bull (Top 25%)\").astype(int)\n",
        "\n",
        "df_test[\"MKT_Bear\"] = df_test[\"excess_mkt\"] * df_test[\"Bear\"]\n",
        "df_test[\"MKT_Bull\"] = df_test[\"excess_mkt\"] * df_test[\"Bull\"]\n",
        "\n",
        "X_rb = sm.add_constant(df_test[[\"excess_mkt\", \"MKT_Bear\", \"MKT_Bull\"]])\n",
        "y_rb = df_test[\"excess_nvda\"]\n",
        "\n",
        "regime_beta_model = sm.OLS(y_rb, X_rb).fit(cov_type=\"HC3\")\n",
        "print(regime_beta_model.summary().tables[1])\n",
        "\n",
        "bear_vs_norm = regime_beta_model.t_test(\"MKT_Bear = 0\")\n",
        "bull_vs_norm = regime_beta_model.t_test(\"MKT_Bull = 0\")\n",
        "bear_vs_bull = regime_beta_model.t_test(\"MKT_Bear - MKT_Bull = 0\")\n",
        "\n",
        "print(\"\\n--- Beta Equality Tests (Wald, Full Sample) ---\")\n",
        "print(f\"Bear vs Normal : t={float(bear_vs_norm.tvalue):.3f}, p={float(bear_vs_norm.pvalue):.4f}\")\n",
        "print(f\"Bull vs Normal : t={float(bull_vs_norm.tvalue):.3f}, p={float(bull_vs_norm.pvalue):.4f}\")\n",
        "print(f\"Bear vs Bull   : t={float(bear_vs_bull.tvalue):.3f}, p={float(bear_vs_bull.pvalue):.4f}\")\n",
        "\n",
        "# -------------------------\n",
        "# 8) Visual proof: fitted curves vs CAPM line\n",
        "# -------------------------\n",
        "# Sort by market excess return for a clean curve plot\n",
        "plot_df = df_eval[[\"excess_mkt\", \"excess_nvda\"]].copy().sort_values(\"excess_mkt\")\n",
        "x_sorted = plot_df[\"excess_mkt\"].values.reshape(-1, 1)\n",
        "y_sorted = plot_df[\"excess_nvda\"].values\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(plot_df[\"excess_mkt\"], plot_df[\"excess_nvda\"], alpha=0.25, s=10)\n",
        "\n",
        "# CAPM line (statsmodels)\n",
        "capm_line = full_ols.predict(sm.add_constant(plot_df[\"excess_mkt\"]))\n",
        "plt.plot(plot_df[\"excess_mkt\"], capm_line, linewidth=2, label=\"CAPM (OLS line)\")\n",
        "\n",
        "# ML fitted curves\n",
        "for name in [\"GBR\", \"SVR\", \"RF\"]:\n",
        "    y_hat = models[name].predict(x_sorted)\n",
        "    plt.plot(x_sorted, y_hat, linewidth=2, label=f\"{name} (fitted curve)\")\n",
        "\n",
        "plt.axvline(q25, linestyle=\"--\", linewidth=1, label=\"q25\")\n",
        "plt.axvline(q75, linestyle=\"--\", linewidth=1, label=\"q75\")\n",
        "plt.title(\"Full-sample fit: CAPM line vs ML curves (Design A)\")\n",
        "plt.xlabel(\"Market Excess Return (Mkt-RF, %)\")\n",
        "plt.ylabel(\"NVDA Excess Return (%)\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Diebold–Mariano Test: OLS vs GBR (Squared Error)\n",
        "# ============================================================\n",
        "\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "from scipy import stats\n",
        "\n",
        "def diebold_mariano_test(y_true, yhat_model1, yhat_model2, maxlags=None):\n",
        "    \"\"\"\n",
        "    DM test using squared-error loss.\n",
        "    H0: E[d_t] = 0\n",
        "    where d_t = e1^2 - e2^2\n",
        "\n",
        "    model1 = baseline (e.g. OLS)\n",
        "    model2 = challenger (e.g. GBR)\n",
        "\n",
        "    Positive mean_d -> model2 better than model1\n",
        "    \"\"\"\n",
        "\n",
        "    e1 = y_true - yhat_model1\n",
        "    e2 = y_true - yhat_model2\n",
        "\n",
        "    d = e1**2 - e2**2   # loss differential\n",
        "\n",
        "    d = d[~np.isnan(d)]\n",
        "    n = len(d)\n",
        "\n",
        "    if n < 30:\n",
        "        print(\"Too few observations for reliable DM test.\")\n",
        "        return None\n",
        "\n",
        "    # Automatic lag selection (Newey-West)\n",
        "    if maxlags is None:\n",
        "        maxlags = int(np.floor(4 * (n / 100.0) ** (2 / 9)))\n",
        "        maxlags = max(1, maxlags)\n",
        "\n",
        "    # Regress d_t on constant with HAC variance\n",
        "    X = np.ones((n, 1))\n",
        "    model = sm.OLS(d, X).fit(cov_type=\"HAC\", cov_kwds={\"maxlags\": maxlags})\n",
        "\n",
        "    mean_d = model.params[0]\n",
        "    se_mean = model.bse[0]\n",
        "    dm_stat = mean_d / se_mean\n",
        "\n",
        "    # One-sided test (GBR better than OLS)\n",
        "    p_value = 1 - stats.norm.cdf(dm_stat)\n",
        "\n",
        "    return {\n",
        "        \"mean_d\": float(mean_d),\n",
        "        \"dm_stat\": float(dm_stat),\n",
        "        \"p_value\": float(p_value),\n",
        "        \"n_obs\": n,\n",
        "        \"lags_used\": maxlags\n",
        "    }"
      ],
      "metadata": {
        "id": "opTI6bNsmn52"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select Bear regime (Design A example)\n",
        "bear_df = df_eval[df_eval[\"Regime\"] == \"Bear (Bot 25%)\"]\n",
        "\n",
        "X_bear = bear_df[[\"excess_mkt\"]].values\n",
        "y_bear = bear_df[\"excess_nvda\"].values\n",
        "\n",
        "yhat_ols = models[\"OLS\"].predict(X_bear)\n",
        "yhat_gbr = models[\"GBR\"].predict(X_bear)\n",
        "\n",
        "dm_result = diebold_mariano_test(\n",
        "    y_true=y_bear,\n",
        "    yhat_model1=yhat_ols,   # baseline\n",
        "    yhat_model2=yhat_gbr    # challenger\n",
        ")\n",
        "\n",
        "print(\"\\n=== Diebold–Mariano: OLS vs GBR (Bear Regime) ===\")\n",
        "for k, v in dm_result.items():\n",
        "    print(f\"{k}: {v}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDficmX6z6KT",
        "outputId": "0843d888-6ca6-4f76-c57d-a9a09aa1710c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Diebold–Mariano: OLS vs GBR (Bear Regime) ===\n",
            "mean_d: 1.0095464816841995\n",
            "dm_stat: 5.948687911939031\n",
            "p_value: 1.3515022434518187e-09\n",
            "n_obs: 674\n",
            "lags_used: 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import norm\n",
        "\n",
        "def dm_test(loss1, loss2, h=1, lags=None):\n",
        "    \"\"\"\n",
        "    Diebold–Mariano test with HAC (Newey–West) variance.\n",
        "    loss1, loss2: arrays of losses (e.g., squared error) for model 1 and 2\n",
        "    Tests H0: E[d]=0 where d = loss1 - loss2\n",
        "    \"\"\"\n",
        "    d = np.asarray(loss1) - np.asarray(loss2)\n",
        "    d = d[~np.isnan(d)]\n",
        "    n = len(d)\n",
        "    if n < 30:\n",
        "        return {\"mean_d\": np.nan, \"dm_stat\": np.nan, \"p_value\": np.nan, \"n_obs\": n, \"lags_used\": 0}\n",
        "\n",
        "    # default lag choice (common): floor(n^(1/3))\n",
        "    if lags is None:\n",
        "        lags = int(np.floor(n ** (1/3)))\n",
        "\n",
        "    d_mean = d.mean()\n",
        "\n",
        "    # HAC variance of mean(d)\n",
        "    # gamma_k = cov(d_t, d_{t-k})\n",
        "    gamma0 = np.mean((d - d_mean) * (d - d_mean))\n",
        "    hac = gamma0\n",
        "\n",
        "    for k in range(1, lags + 1):\n",
        "        cov_k = np.mean((d[k:] - d_mean) * (d[:-k] - d_mean))\n",
        "        weight = 1.0 - k / (lags + 1.0)  # Bartlett\n",
        "        hac += 2.0 * weight * cov_k\n",
        "\n",
        "    var_dbar = hac / n\n",
        "    dm_stat = d_mean / np.sqrt(var_dbar)\n",
        "\n",
        "    # two-sided p-value\n",
        "    p_value = 2.0 * (1.0 - norm.cdf(np.abs(dm_stat)))\n",
        "\n",
        "    return {\"mean_d\": d_mean, \"dm_stat\": dm_stat, \"p_value\": p_value, \"n_obs\": n, \"lags_used\": lags}\n",
        "\n",
        "\n",
        "def dm_by_regime(df, y_col, pred_ols, pred_gbr, regime_col=\"Regime\", regimes=(\"Bear\",\"Normal\",\"Bull\"), lags=None):\n",
        "    out = []\n",
        "    y = df[y_col].values\n",
        "\n",
        "    for r in regimes:\n",
        "        mask = (df[regime_col].values == r)\n",
        "        y_r = y[mask]\n",
        "        ols_r = np.asarray(pred_ols)[mask]\n",
        "        gbr_r = np.asarray(pred_gbr)[mask]\n",
        "\n",
        "        # squared error losses\n",
        "        loss_ols = (y_r - ols_r) ** 2\n",
        "        loss_gbr = (y_r - gbr_r) ** 2\n",
        "\n",
        "        res = dm_test(loss_ols, loss_gbr, lags=lags)\n",
        "        res[\"Regime\"] = r\n",
        "        # interpretation shortcut\n",
        "        if np.isfinite(res[\"mean_d\"]):\n",
        "            if res[\"p_value\"] < 0.05 and res[\"mean_d\"] > 0:\n",
        "                res[\"Conclusion\"] = \"GBR better (lower MSE)\"\n",
        "            elif res[\"p_value\"] < 0.05 and res[\"mean_d\"] < 0:\n",
        "                res[\"Conclusion\"] = \"OLS better (lower MSE)\"\n",
        "            else:\n",
        "                res[\"Conclusion\"] = \"No significant difference\"\n",
        "        else:\n",
        "            res[\"Conclusion\"] = \"Too few obs\"\n",
        "        out.append(res)\n",
        "\n",
        "    return pd.DataFrame(out)[[\"Regime\",\"n_obs\",\"lags_used\",\"mean_d\",\"dm_stat\",\"p_value\",\"Conclusion\"]]\n",
        "\n",
        "\n",
        "# ---- Example usage ----\n",
        "# df must have df[\"Regime\"] already (Bear/Normal/Bull)\n",
        "# pred_ols_full and pred_gbr_full must be aligned to df rows (same length, same order)\n",
        "# e.g., from Design A in-sample fit:\n",
        "# pred_ols_full = models[\"OLS\"].predict(X_full)\n",
        "# pred_gbr_full = models[\"GBR\"].predict(X_full)\n",
        "\n",
        "dm_table = dm_by_regime(\n",
        "    df=df,\n",
        "    y_col=\"excess_nvda\",\n",
        "    pred_ols=pred_ols_full,\n",
        "    pred_gbr=pred_gbr_full,\n",
        "    regime_col=\"Regime\",\n",
        "    regimes=(\"Bear\",\"Normal\",\"Bull\"),\n",
        "    lags=None  # or set fixed lags like 6 for comparability\n",
        ")\n",
        "\n",
        "print(dm_table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "TxWgJVdR1JTT",
        "outputId": "36275286-f823-48b4-b722-13bc83bfcd17"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'pred_ols_full' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3981283193.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0my_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"excess_nvda\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0mpred_ols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpred_ols_full\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0mpred_gbr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpred_gbr_full\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mregime_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Regime\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pred_ols_full' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install yfinance pandas-datareader statsmodels scikit-learn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8H4cHRXs5XxJ",
        "outputId": "bb7c2f72-96df-4d8c-e11a-7f4c12b9b865"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.12/dist-packages (0.2.66)\n",
            "Requirement already satisfied: pandas-datareader in /usr/local/lib/python3.12/dist-packages (0.10.0)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.12/dist-packages (0.14.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from yfinance) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.12/dist-packages (from yfinance) (2.0.2)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.12/dist-packages (from yfinance) (2.32.4)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.12/dist-packages (from yfinance) (0.0.12)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from yfinance) (4.9.2)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.12/dist-packages (from yfinance) (2025.2)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.12/dist-packages (from yfinance) (2.4.7)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.12/dist-packages (from yfinance) (3.19.0)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.12/dist-packages (from yfinance) (4.13.5)\n",
            "Requirement already satisfied: curl_cffi>=0.7 in /usr/local/lib/python3.12/dist-packages (from yfinance) (0.14.0)\n",
            "Requirement already satisfied: protobuf>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from yfinance) (5.29.6)\n",
            "Requirement already satisfied: websockets>=13.0 in /usr/local/lib/python3.12/dist-packages (from yfinance) (15.0.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from pandas-datareader) (6.0.2)\n",
            "Requirement already satisfied: scipy!=1.9.2,>=1.8 in /usr/local/lib/python3.12/dist-packages (from statsmodels) (1.16.3)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.12/dist-packages (from statsmodels) (1.0.2)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.12/dist-packages (from statsmodels) (26.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.8.3)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (4.15.0)\n",
            "Requirement already satisfied: cffi>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from curl_cffi>=0.7->yfinance) (2.0.0)\n",
            "Requirement already satisfied: certifi>=2024.2.2 in /usr/local/lib/python3.12/dist-packages (from curl_cffi>=0.7->yfinance) (2026.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.3.0->yfinance) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.3.0->yfinance) (2025.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31->yfinance) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31->yfinance) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31->yfinance) (2.5.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12.0->curl_cffi>=0.7->yfinance) (3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.3.0->yfinance) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qWWVMoZp660V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# CAPM vs ML (Design A): Train on FULL sample, evaluate by regimes\n",
        "# NVDA 2015–2025 | Full-sample fit + regime-wise error comparison\n",
        "# Includes: CAPM diagnostics, ML fits, regime metrics, Kruskal-Wallis,\n",
        "#           Wald beta-equality test, and Diebold–Mariano tests (OLS vs GBR)\n",
        "# ============================================================\n",
        "\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import yfinance as yf\n",
        "from pandas_datareader import data as pdr\n",
        "\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.stats.diagnostic import het_breuschpagan, linear_reset\n",
        "from statsmodels.stats.stattools import durbin_watson\n",
        "from scipy.stats import jarque_bera, kruskal, t as student_t\n",
        "\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Global Config\n",
        "# ------------------------------------------------------------\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "np.random.seed(42)\n",
        "\n",
        "START = \"2010-01-01\"\n",
        "END   = \"2025-12-31\"\n",
        "\n",
        "# ============================================================\n",
        "# 1) DATA INGESTION & PREPROCESSING (daily % units)\n",
        "# ============================================================\n",
        "\n",
        "def _flatten_yf(px: pd.DataFrame) -> pd.DataFrame:\n",
        "    px = px.copy()\n",
        "    if isinstance(px.columns, pd.MultiIndex):\n",
        "        px.columns = [c[0] if isinstance(c, tuple) else c for c in px.columns]\n",
        "    px.columns.name = None\n",
        "    return px\n",
        "\n",
        "def _yf_adj_close(ticker: str, start: str, end: str) -> pd.DataFrame:\n",
        "    try:\n",
        "        df = yf.download(ticker, start=start, end=end, progress=False, auto_adjust=True)\n",
        "        df = _flatten_yf(df)\n",
        "        for col in [\"Adj Close\", \"Close\", ticker]:\n",
        "            if col in df.columns:\n",
        "                return df[[col]].rename(columns={col: ticker})\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching {ticker}: {e}\")\n",
        "    return pd.DataFrame()\n",
        "\n",
        "def build_dataset() -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    NVDA prices from yfinance. Market + RF from Fama–French daily factors (preferred).\n",
        "    Units:\n",
        "      - Fama-French Mkt-RF and RF are already in daily percent.\n",
        "      - NVDA returns computed as daily percent and excess_nvda = nvda_ret - rf.\n",
        "    \"\"\"\n",
        "    print(\"--- Building Dataset (daily % units) ---\")\n",
        "\n",
        "    nvda = _yf_adj_close(\"NVDA\", START, END).rename(columns={\"NVDA\": \"nvda_px\"})\n",
        "\n",
        "    try:\n",
        "        ff_raw = pdr.DataReader(\n",
        "            \"F-F_Research_Data_Factors_daily\",\n",
        "            \"famafrench\",\n",
        "            start=START,\n",
        "            end=END\n",
        "        )[0]\n",
        "        ff = ff_raw[[\"Mkt-RF\", \"RF\"]].rename(columns={\"Mkt-RF\": \"excess_mkt\", \"RF\": \"rf\"})\n",
        "        print(\"Using Fama–French Daily Factors (Mkt-RF, RF) as market / RF.\")\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(\n",
        "            f\"Fama–French download failed ({e}). \"\n",
        "            \"For this script, please ensure famafrench source works.\"\n",
        "        )\n",
        "\n",
        "    df = nvda.join(ff, how=\"inner\").dropna()\n",
        "    df[\"nvda_ret\"] = df[\"nvda_px\"].pct_change() * 100.0\n",
        "    df[\"excess_nvda\"] = df[\"nvda_ret\"] - df[\"rf\"]\n",
        "    df = df.dropna()\n",
        "\n",
        "    print(f\"Data Ready: {df.shape[0]} rows. Columns: {list(df.columns)}\")\n",
        "    return df\n",
        "\n",
        "df = build_dataset()\n",
        "\n",
        "# ============================================================\n",
        "# 2) CAPM (Full sample) + Diagnostics\n",
        "# ============================================================\n",
        "\n",
        "def run_capm_diagnostics(df_subset: pd.DataFrame, label: str = \"Sample\"):\n",
        "    X = sm.add_constant(df_subset[\"excess_mkt\"])\n",
        "    y = df_subset[\"excess_nvda\"]\n",
        "\n",
        "    ols = sm.OLS(y, X).fit()\n",
        "\n",
        "    resid = ols.resid\n",
        "    bp_lm, bp_pvalue, _, _ = het_breuschpagan(resid, ols.model.exog)\n",
        "    reset_res = linear_reset(ols, power=2, use_f=True)\n",
        "    jb_stat, jb_pvalue = jarque_bera(resid)\n",
        "    dw_stat = durbin_watson(resid)\n",
        "\n",
        "    diagnostics = {\n",
        "        \"label\": label,\n",
        "        \"alpha\": float(ols.params[\"const\"]),\n",
        "        \"beta\": float(ols.params[\"excess_mkt\"]),\n",
        "        \"alpha_t\": float(ols.tvalues[\"const\"]),\n",
        "        \"beta_t\": float(ols.tvalues[\"excess_mkt\"]),\n",
        "        \"R2\": float(ols.rsquared),\n",
        "        \"BP_p\": float(bp_pvalue),\n",
        "        \"RESET_p\": float(reset_res.pvalue),\n",
        "        \"JB_p\": float(jb_pvalue),\n",
        "        \"DW\": float(dw_stat),\n",
        "    }\n",
        "\n",
        "    print(f\"\\n=== OLS Summary: {label} ===\")\n",
        "    print(ols.summary().tables[1])\n",
        "    print(\"\\nDiagnostics:\")\n",
        "    for k, v in diagnostics.items():\n",
        "        if k == \"label\":\n",
        "            continue\n",
        "        print(f\"{k:10s}: {v:.4f}\")\n",
        "\n",
        "    return ols, diagnostics\n",
        "\n",
        "capm_full, capm_diag_full = run_capm_diagnostics(df, label=\"CAPM (Full Sample)\")\n",
        "\n",
        "# ============================================================\n",
        "# 3) Define regimes on FULL sample quantiles (Design A)\n",
        "#    Bear = bottom 25%, Bull = top 25%, Normal = middle 50%\n",
        "# ============================================================\n",
        "\n",
        "q25 = df[\"excess_mkt\"].quantile(0.25)\n",
        "q75 = df[\"excess_mkt\"].quantile(0.75)\n",
        "\n",
        "df = df.copy()\n",
        "df[\"Regime\"] = \"Normal (Mid 50%)\"\n",
        "df.loc[df[\"excess_mkt\"] < q25, \"Regime\"] = \"Bear (Bot 25%)\"\n",
        "df.loc[df[\"excess_mkt\"] > q75, \"Regime\"] = \"Bull (Top 25%)\"\n",
        "\n",
        "print(\"\\n--- Regime thresholds (FULL sample) ---\")\n",
        "print(f\"q25 = {q25:.3f} pps | q75 = {q75:.3f} pps\")\n",
        "print(\"\\nRegime counts:\")\n",
        "print(df[\"Regime\"].value_counts())\n",
        "\n",
        "REGIMES = [\"Bear (Bot 25%)\", \"Normal (Mid 50%)\", \"Bull (Top 25%)\"]\n",
        "\n",
        "# ============================================================\n",
        "# 4) Train ALL models on FULL sample (Design A)\n",
        "# ============================================================\n",
        "\n",
        "X_full = df[[\"excess_mkt\"]].values\n",
        "y_full = df[\"excess_nvda\"].values\n",
        "\n",
        "# OLS (sklearn)\n",
        "ols_model = LinearRegression()\n",
        "ols_model.fit(X_full, y_full)\n",
        "pred_ols_full = ols_model.predict(X_full)\n",
        "\n",
        "# RF\n",
        "rf_model = RandomForestRegressor(\n",
        "    n_estimators=300,\n",
        "    max_depth=None,\n",
        "    min_samples_leaf=2,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf_model.fit(X_full, y_full)\n",
        "pred_rf_full = rf_model.predict(X_full)\n",
        "\n",
        "# SVR (scaled)\n",
        "svr_model = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"svm\", SVR(kernel=\"rbf\", C=10, gamma=\"scale\"))\n",
        "])\n",
        "svr_model.fit(X_full, y_full)\n",
        "pred_svr_full = svr_model.predict(X_full)\n",
        "\n",
        "# GBR (robust loss)\n",
        "gbr_model = GradientBoostingRegressor(\n",
        "    n_estimators=500,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=3,\n",
        "    loss=\"huber\",\n",
        "    random_state=42\n",
        ")\n",
        "gbr_model.fit(X_full, y_full)\n",
        "pred_gbr_full = gbr_model.predict(X_full)\n",
        "\n",
        "preds = {\n",
        "    \"OLS\": pred_ols_full,\n",
        "    \"RF\": pred_rf_full,\n",
        "    \"SVR\": pred_svr_full,\n",
        "    \"GBR\": pred_gbr_full\n",
        "}\n",
        "\n",
        "# ============================================================\n",
        "# 5) Metrics (overall + by regime), using errors too\n",
        "# ============================================================\n",
        "\n",
        "def metrics_block(y_true, y_hat):\n",
        "    err = y_true - y_hat\n",
        "    return {\n",
        "        \"RMSE\": float(np.sqrt(np.mean(err**2))),\n",
        "        \"MAE\": float(np.mean(np.abs(err))),\n",
        "        \"R2\": float(r2_score(y_true, y_hat))\n",
        "    }\n",
        "\n",
        "rows = []\n",
        "\n",
        "# Overall (in-sample)\n",
        "for m, yhat in preds.items():\n",
        "    met = metrics_block(y_full, yhat)\n",
        "    rows.append({\"Sample\": \"Full Sample (in-sample)\", \"Model\": m, **met})\n",
        "\n",
        "# By regime (in-sample, no retrain)\n",
        "for r in REGIMES:\n",
        "    mask = (df[\"Regime\"] == r).values\n",
        "    y_r = y_full[mask]\n",
        "    for m, yhat in preds.items():\n",
        "        yhat_r = yhat[mask]\n",
        "        met = metrics_block(y_r, yhat_r)\n",
        "        rows.append({\"Sample\": f\"{r} (in-sample)\", \"Model\": m, **met})\n",
        "\n",
        "metrics_df = pd.DataFrame(rows)\n",
        "print(\"\\n===================================================\")\n",
        "print(\"   DESIGN A: IN-SAMPLE PERFORMANCE (OVERALL + BY REGIME)\")\n",
        "print(\"===================================================\")\n",
        "print(metrics_df.pivot(index=\"Sample\", columns=\"Model\", values=\"R2\").loc[\n",
        "    [\"Full Sample (in-sample)\"] + [f\"{r} (in-sample)\" for r in REGIMES]\n",
        "])\n",
        "\n",
        "# ============================================================\n",
        "# 6) Kruskal–Wallis: do |errors| differ across regimes? (per model)\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n=== Kruskal–Wallis tests on |errors| across regimes (in-sample) ===\")\n",
        "for m, yhat in preds.items():\n",
        "    abs_errs = []\n",
        "    for r in REGIMES:\n",
        "        mask = (df[\"Regime\"] == r).values\n",
        "        abs_errs.append(np.abs(y_full[mask] - yhat[mask]))\n",
        "    H, p = kruskal(*abs_errs)\n",
        "    print(f\"Model: {m:3s}  H = {H:.3f}, p = {p:.4f}\")\n",
        "\n",
        "# ============================================================\n",
        "# 7) Wald test: are CAPM betas different across regimes? (Full sample)\n",
        "#    Pooled model with interactions (Normal is baseline)\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n=== Wald Tests: Are CAPM betas different across regimes? (Full sample) ===\")\n",
        "\n",
        "df_w = df.copy()\n",
        "df_w[\"Bear\"] = (df_w[\"Regime\"] == \"Bear (Bot 25%)\").astype(int)\n",
        "df_w[\"Bull\"] = (df_w[\"Regime\"] == \"Bull (Top 25%)\").astype(int)\n",
        "\n",
        "df_w[\"MKT_Bear\"] = df_w[\"excess_mkt\"] * df_w[\"Bear\"]\n",
        "df_w[\"MKT_Bull\"] = df_w[\"excess_mkt\"] * df_w[\"Bull\"]\n",
        "\n",
        "X_w = sm.add_constant(df_w[[\"excess_mkt\", \"MKT_Bear\", \"MKT_Bull\"]])\n",
        "y_w = df_w[\"excess_nvda\"]\n",
        "\n",
        "wald_mod = sm.OLS(y_w, X_w).fit(cov_type=\"HC3\")\n",
        "print(wald_mod.summary().tables[1])\n",
        "\n",
        "bear_vs_norm = wald_mod.t_test(\"MKT_Bear = 0\")\n",
        "bull_vs_norm = wald_mod.t_test(\"MKT_Bull = 0\")\n",
        "bear_vs_bull = wald_mod.t_test(\"MKT_Bear - MKT_Bull = 0\")\n",
        "\n",
        "print(\"\\n--- Beta Equality Tests (Wald, Full Sample) ---\")\n",
        "print(f\"Bear vs Normal : t={float(bear_vs_norm.tvalue):.3f}, p={float(bear_vs_norm.pvalue):.4f}\")\n",
        "print(f\"Bull vs Normal : t={float(bull_vs_norm.tvalue):.3f}, p={float(bull_vs_norm.pvalue):.4f}\")\n",
        "print(f\"Bear vs Bull   : t={float(bear_vs_bull.tvalue):.3f}, p={float(bear_vs_bull.pvalue):.4f}\")\n",
        "\n",
        "# ============================================================\n",
        "# 8) Diebold–Mariano (OLS vs GBR) within EACH regime\n",
        "#    Here: loss differential d_t = e_OLS^2 - e_GBR^2\n",
        "#    H0: E[d_t] = 0 ; H1: E[d_t] > 0 (GBR better)\n",
        "#    HAC variance via Newey–West on d_t (lags L)\n",
        "# ============================================================\n",
        "\n",
        "def dm_test_one_sided(d: np.ndarray, lags: int = 6):\n",
        "    \"\"\"\n",
        "    One-sided DM test (GBR better): H1: E[d] > 0\n",
        "    d_t = loss_OLS(t) - loss_GBR(t)\n",
        "    HAC variance estimate (Newey–West) on d_t.\n",
        "    \"\"\"\n",
        "    d = np.asarray(d, dtype=float)\n",
        "    d = d[~np.isnan(d)]\n",
        "    T = len(d)\n",
        "    if T < 30:\n",
        "        return {\"mean_d\": np.nan, \"dm_stat\": np.nan, \"p_value\": np.nan, \"n_obs\": T, \"lags_used\": lags}\n",
        "\n",
        "    d_mean = d.mean()\n",
        "    d0 = d - d_mean\n",
        "\n",
        "    # Newey-West long-run variance\n",
        "    gamma0 = np.sum(d0 * d0) / T\n",
        "    lr_var = gamma0\n",
        "\n",
        "    L = min(lags, T - 1)\n",
        "    for k in range(1, L + 1):\n",
        "        gamma_k = np.sum(d0[k:] * d0[:-k]) / T\n",
        "        w_k = 1.0 - k / (L + 1.0)\n",
        "        lr_var += 2.0 * w_k * gamma_k\n",
        "\n",
        "    # Variance of the sample mean\n",
        "    var_mean = lr_var / T\n",
        "    if var_mean <= 0:\n",
        "        return {\"mean_d\": d_mean, \"dm_stat\": np.nan, \"p_value\": np.nan, \"n_obs\": T, \"lags_used\": L}\n",
        "\n",
        "    dm_stat = d_mean / np.sqrt(var_mean)\n",
        "\n",
        "    # One-sided p-value: P(T > dm_stat) under approx t distribution with T-1 df\n",
        "    p_value = 1.0 - student_t.cdf(dm_stat, df=T - 1)\n",
        "\n",
        "    return {\"mean_d\": float(d_mean), \"dm_stat\": float(dm_stat), \"p_value\": float(p_value), \"n_obs\": T, \"lags_used\": L}\n",
        "\n",
        "def dm_by_regime(df: pd.DataFrame,\n",
        "                 y_col: str,\n",
        "                 pred_ols: np.ndarray,\n",
        "                 pred_gbr: np.ndarray,\n",
        "                 regime_col: str = \"Regime\",\n",
        "                 regimes=REGIMES,\n",
        "                 lags: int = 6):\n",
        "    out = []\n",
        "    y = df[y_col].values\n",
        "    e_ols = y - pred_ols\n",
        "    e_gbr = y - pred_gbr\n",
        "\n",
        "    # squared errors\n",
        "    se_ols = e_ols**2\n",
        "    se_gbr = e_gbr**2\n",
        "\n",
        "    for r in regimes:\n",
        "        mask = (df[regime_col].values == r)\n",
        "        d = se_ols[mask] - se_gbr[mask]   # positive => GBR better\n",
        "        res = dm_test_one_sided(d, lags=lags)\n",
        "        out.append({\n",
        "            \"Regime\": r,\n",
        "            \"mean(d)=E[SE_OLS-SE_GBR]\": res[\"mean_d\"],\n",
        "            \"DM_stat\": res[\"dm_stat\"],\n",
        "            \"p_value (H1: GBR better)\": res[\"p_value\"],\n",
        "            \"n_obs\": res[\"n_obs\"],\n",
        "            \"lags\": res[\"lags_used\"],\n",
        "        })\n",
        "    return pd.DataFrame(out)\n",
        "\n",
        "print(\"\\n===================================================\")\n",
        "print(\"   Diebold–Mariano: OLS vs GBR (by regime)\")\n",
        "print(\"   H1: GBR has lower MSE than OLS in that regime\")\n",
        "print(\"===================================================\")\n",
        "\n",
        "dm_table = dm_by_regime(\n",
        "    df=df,\n",
        "    y_col=\"excess_nvda\",\n",
        "    pred_ols=pred_ols_full,\n",
        "    pred_gbr=pred_gbr_full,\n",
        "    regime_col=\"Regime\",\n",
        "    regimes=REGIMES,\n",
        "    lags=6\n",
        ")\n",
        "print(dm_table)\n",
        "\n",
        "# ============================================================\n",
        "# 9) Optional: Visual sanity check (fit lines/curves by regime)\n",
        "#    (In-sample fitted values, full-trained models)\n",
        "# ============================================================\n",
        "\n",
        "def plot_fit_by_regime(df: pd.DataFrame, preds: dict, regime: str):\n",
        "    sub = df[df[\"Regime\"] == regime].copy()\n",
        "    x = sub[\"excess_mkt\"].values\n",
        "    y = sub[\"excess_nvda\"].values\n",
        "    idx = sub.index\n",
        "\n",
        "    plt.figure(figsize=(9, 6))\n",
        "    plt.scatter(x, y, alpha=0.35, s=12)\n",
        "    # sort x for nice lines\n",
        "    srt = np.argsort(x)\n",
        "    x_s = x[srt]\n",
        "\n",
        "    for name, yhat_full in preds.items():\n",
        "        yhat = yhat_full[df[\"Regime\"].values == regime]\n",
        "        plt.plot(x_s, yhat[srt], linewidth=2, label=name)\n",
        "\n",
        "    plt.title(f\"Full-trained model fits inside regime: {regime}\")\n",
        "    plt.xlabel(\"Market excess return (Mkt-RF, %)\")\n",
        "    plt.ylabel(\"NVDA excess return (%)\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Uncomment these if you want regime-by-regime fit visuals:\n",
        "# for r in REGIMES:\n",
        "#     plot_fit_by_regime(df, preds, r)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrP3axf71thf",
        "outputId": "fe16127b-c939-44ab-928e-c3f9c64d9a04"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Building Dataset (daily % units) ---\n",
            "Using Fama–French Daily Factors (Mkt-RF, RF) as market / RF.\n",
            "Data Ready: 6537 rows. Columns: ['nvda_px', 'excess_mkt', 'rf', 'nvda_ret', 'excess_nvda']\n",
            "\n",
            "=== OLS Summary: CAPM (Full Sample) ===\n",
            "==============================================================================\n",
            "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "const          0.1238      0.038      3.261      0.001       0.049       0.198\n",
            "excess_mkt     1.6971      0.031     55.384      0.000       1.637       1.757\n",
            "==============================================================================\n",
            "\n",
            "Diagnostics:\n",
            "alpha     : 0.1238\n",
            "beta      : 1.6971\n",
            "alpha_t   : 3.2610\n",
            "beta_t    : 55.3842\n",
            "R2        : 0.3194\n",
            "BP_p      : 0.8400\n",
            "RESET_p   : 0.0159\n",
            "JB_p      : 0.0000\n",
            "DW        : 2.0062\n",
            "\n",
            "--- Regime thresholds (FULL sample) ---\n",
            "q25 = -0.490 pps | q75 = 0.610 pps\n",
            "\n",
            "Regime counts:\n",
            "Regime\n",
            "Normal (Mid 50%)    3288\n",
            "Bull (Top 25%)      1632\n",
            "Bear (Bot 25%)      1617\n",
            "Name: count, dtype: int64\n",
            "\n",
            "===================================================\n",
            "   DESIGN A: IN-SAMPLE PERFORMANCE (OVERALL + BY REGIME)\n",
            "===================================================\n",
            "Model                              GBR       OLS        RF       SVR\n",
            "Sample                                                              \n",
            "Full Sample (in-sample)       0.367044  0.319442  0.406637  0.321862\n",
            "Bear (Bot 25%) (in-sample)    0.212135  0.135146  0.303946  0.147861\n",
            "Normal (Mid 50%) (in-sample)  0.045780  0.034502  0.065705  0.031520\n",
            "Bull (Top 25%) (in-sample)    0.278959  0.183280  0.321216  0.183018\n",
            "\n",
            "=== Kruskal–Wallis tests on |errors| across regimes (in-sample) ===\n",
            "Model: OLS  H = 97.774, p = 0.0000\n",
            "Model: RF   H = 57.342, p = 0.0000\n",
            "Model: SVR  H = 85.205, p = 0.0000\n",
            "Model: GBR  H = 61.897, p = 0.0000\n",
            "\n",
            "=== Wald Tests: Are CAPM betas different across regimes? (Full sample) ===\n",
            "==============================================================================\n",
            "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "const          0.0801      0.044      1.824      0.068      -0.006       0.166\n",
            "excess_mkt     1.7266      0.157     10.974      0.000       1.418       2.035\n",
            "MKT_Bear      -0.0886      0.172     -0.515      0.607      -0.426       0.249\n",
            "MKT_Bull       0.0329      0.165      0.200      0.841      -0.290       0.355\n",
            "==============================================================================\n",
            "\n",
            "--- Beta Equality Tests (Wald, Full Sample) ---\n",
            "Bear vs Normal : t=-0.515, p=0.6069\n",
            "Bull vs Normal : t=0.200, p=0.8413\n",
            "Bear vs Bull   : t=-1.285, p=0.1987\n",
            "\n",
            "===================================================\n",
            "   Diebold–Mariano: OLS vs GBR (by regime)\n",
            "   H1: GBR has lower MSE than OLS in that regime\n",
            "===================================================\n",
            "             Regime  mean(d)=E[SE_OLS-SE_GBR]   DM_stat  \\\n",
            "0    Bear (Bot 25%)                  1.099878  7.170930   \n",
            "1  Normal (Mid 50%)                  0.078923  4.749679   \n",
            "2    Bull (Top 25%)                  1.389045  4.235686   \n",
            "\n",
            "   p_value (H1: GBR better)  n_obs  lags  \n",
            "0              5.646594e-13   1617     6  \n",
            "1              1.062283e-06   3288     6  \n",
            "2              1.202901e-05   1632     6  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NEXT STEPS:\n",
        "\n",
        "1. Rolling-beta diagnostic (within 2015–2025): Compute β in 252-day (1-year) rolling windows and plot it over time. If β is stable, your full-sample CAPM is justified. If it drifts post-2022, you have evidence for a structural break that motivates regime-switching models.\n",
        "\n",
        "2. Design B: 2015–2021 train → 2022–2025 test: The AI era (2022+) is genuinely out-of-sample relative to the pre-AI training data. This gives you real out-of-sample DM statistics and solves the in-sample conflation issue far more than adding 15 years of pre-AI data would.\n",
        "\n",
        "3. Extend to 2010 only (15 years): This adds the GFC recovery, the 2010–2015 secular bull market, and a second full market cycle, while keeping NVDA in its post-Fermi/GPU-compute era and nearly doubling your observations to ~3,800 days. It is the best compromise between statistical power and structural homogeneity.\n"
      ],
      "metadata": {
        "id": "LKO_9NS869xl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# CAPM vs ML — Design A + Three Enhancements\n",
        "# NVDA | 2015–2025 primary | 2010–2025 extended\n",
        "#\n",
        "# ORIGINAL (Design A):\n",
        "#   Full-sample CAPM diagnostics, ML fits, regime metrics,\n",
        "#   Kruskal-Wallis, Wald beta-equality, Diebold–Mariano (OLS vs GBR)\n",
        "#\n",
        "# ENHANCEMENTS:\n",
        "#   ① Rolling-beta diagnostic (252-day window, 2015–2025)\n",
        "#   ② Design B: 2015–2021 train → 2022–2025 genuine OOS test\n",
        "#   ③ Extended window: 2010–2025 CAPM comparison + Chow test\n",
        "# ============================================================\n",
        "\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import yfinance as yf\n",
        "from pandas_datareader import data as pdr\n",
        "\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.regression.rolling import RollingOLS\n",
        "from statsmodels.stats.diagnostic import het_breuschpagan, linear_reset\n",
        "from statsmodels.stats.stattools import durbin_watson\n",
        "from scipy.stats import jarque_bera, kruskal, t as student_t\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "np.random.seed(42)\n",
        "\n",
        "# ── Global config ──────────────────────────────────────────\n",
        "START      = \"2015-01-01\"   # primary window\n",
        "START_EXT  = \"2010-01-01\"   # ③ extended window\n",
        "END        = \"2025-09-30\"\n",
        "TRAIN_END  = \"2021-12-31\"   # ② Design B cut-off\n",
        "TEST_START = \"2022-01-01\"   # ② Design B test start\n",
        "ROLL_WIN   = 252            # ① rolling-beta window (1 year)\n",
        "REGIMES    = [\"Bear (Bot 25%)\", \"Normal (Mid 50%)\", \"Bull (Top 25%)\"]\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# HELPERS\n",
        "# ============================================================\n",
        "\n",
        "def _flatten_yf(px: pd.DataFrame) -> pd.DataFrame:\n",
        "    px = px.copy()\n",
        "    if isinstance(px.columns, pd.MultiIndex):\n",
        "        px.columns = [c[0] if isinstance(c, tuple) else c for c in px.columns]\n",
        "    px.columns.name = None\n",
        "    return px\n",
        "\n",
        "\n",
        "def _yf_adj_close(ticker: str, start: str, end: str) -> pd.DataFrame:\n",
        "    try:\n",
        "        df = yf.download(ticker, start=start, end=end, progress=False, auto_adjust=True)\n",
        "        df = _flatten_yf(df)\n",
        "        for col in [\"Adj Close\", \"Close\", ticker]:\n",
        "            if col in df.columns:\n",
        "                return df[[col]].rename(columns={col: ticker})\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching {ticker}: {e}\")\n",
        "    return pd.DataFrame()\n",
        "\n",
        "\n",
        "def build_dataset(start: str = START, end: str = END, label: str = \"primary\") -> pd.DataFrame:\n",
        "    \"\"\"Download NVDA + Fama-French daily factors; returns daily-% excess-return frame.\"\"\"\n",
        "    print(f\"\\n--- Building dataset [{label}]: {start} → {end} ---\")\n",
        "    nvda = _yf_adj_close(\"NVDA\", start, end).rename(columns={\"NVDA\": \"nvda_px\"})\n",
        "    ff_raw = pdr.DataReader(\n",
        "        \"F-F_Research_Data_Factors_daily\", \"famafrench\", start=start, end=end\n",
        "    )[0]\n",
        "    ff = ff_raw[[\"Mkt-RF\", \"RF\"]].rename(columns={\"Mkt-RF\": \"excess_mkt\", \"RF\": \"rf\"})\n",
        "    df = nvda.join(ff, how=\"inner\").dropna()\n",
        "    df[\"nvda_ret\"]    = df[\"nvda_px\"].pct_change() * 100.0\n",
        "    df[\"excess_nvda\"] = df[\"nvda_ret\"] - df[\"rf\"]\n",
        "    df = df.dropna()\n",
        "    print(f\"  Ready: {df.shape[0]} rows | cols: {list(df.columns)}\")\n",
        "    return df\n",
        "\n",
        "\n",
        "def add_regimes(df: pd.DataFrame, q_col: str = \"excess_mkt\") -> pd.DataFrame:\n",
        "    \"\"\"Add 'Regime' column based on within-frame quantile thresholds.\"\"\"\n",
        "    df = df.copy()\n",
        "    q25 = df[q_col].quantile(0.25)\n",
        "    q75 = df[q_col].quantile(0.75)\n",
        "    df[\"Regime\"] = \"Normal (Mid 50%)\"\n",
        "    df.loc[df[q_col] < q25, \"Regime\"] = \"Bear (Bot 25%)\"\n",
        "    df.loc[df[q_col] > q75, \"Regime\"] = \"Bull (Top 25%)\"\n",
        "    print(f\"  Regime thresholds → q25={q25:.4f}, q75={q75:.4f}\")\n",
        "    print(df[\"Regime\"].value_counts().to_string())\n",
        "    return df, q25, q75\n",
        "\n",
        "\n",
        "def run_capm_diagnostics(df_sub: pd.DataFrame, label: str = \"Sample\"):\n",
        "    \"\"\"Full OLS CAPM + five diagnostic tests.\"\"\"\n",
        "    X = sm.add_constant(df_sub[\"excess_mkt\"])\n",
        "    y = df_sub[\"excess_nvda\"]\n",
        "    ols = sm.OLS(y, X).fit()\n",
        "    resid = ols.resid\n",
        "    bp_lm, bp_p, _, _  = het_breuschpagan(resid, ols.model.exog)\n",
        "    reset_p            = linear_reset(ols, power=2, use_f=True).pvalue\n",
        "    jb_stat, jb_p      = jarque_bera(resid)\n",
        "    dw                 = durbin_watson(resid)\n",
        "    diag = dict(\n",
        "        label=label,\n",
        "        alpha=float(ols.params[\"const\"]),     beta=float(ols.params[\"excess_mkt\"]),\n",
        "        alpha_t=float(ols.tvalues[\"const\"]),  beta_t=float(ols.tvalues[\"excess_mkt\"]),\n",
        "        R2=float(ols.rsquared), nobs=int(ols.nobs),\n",
        "        BP_p=float(bp_p), RESET_p=float(reset_p), JB_p=float(jb_p), DW=float(dw),\n",
        "    )\n",
        "    print(f\"\\n=== OLS Summary: {label} ===\")\n",
        "    print(ols.summary().tables[1])\n",
        "    print(\"\\nDiagnostics:\")\n",
        "    for k, v in diag.items():\n",
        "        if k not in (\"label\",):\n",
        "            print(f\"  {k:10s}: {v:.4f}\" if isinstance(v, float) else f\"  {k:10s}: {v}\")\n",
        "    return ols, diag\n",
        "\n",
        "\n",
        "def metrics_block(y_true: np.ndarray, y_hat: np.ndarray) -> dict:\n",
        "    err = y_true - y_hat\n",
        "    return dict(\n",
        "        RMSE=float(np.sqrt(np.mean(err**2))),\n",
        "        MAE =float(np.mean(np.abs(err))),\n",
        "        R2  =float(r2_score(y_true, y_hat)),\n",
        "    )\n",
        "\n",
        "\n",
        "def dm_test_one_sided(d: np.ndarray, lags: int = 6) -> dict:\n",
        "    \"\"\"HAC-based one-sided DM test. H1: E[d] > 0  (model A better than B).\"\"\"\n",
        "    d = np.asarray(d, dtype=float)\n",
        "    d = d[~np.isnan(d)]\n",
        "    T = len(d)\n",
        "    if T < 30:\n",
        "        return dict(mean_d=np.nan, dm_stat=np.nan, p_value=np.nan, n_obs=T)\n",
        "    d0    = d - d.mean()\n",
        "    gamma = np.sum(d0**2) / T\n",
        "    L     = min(lags, T - 1)\n",
        "    for k in range(1, L + 1):\n",
        "        gamma += 2.0 * (1.0 - k / (L + 1.0)) * np.sum(d0[k:] * d0[:-k]) / T\n",
        "    var_m = gamma / T\n",
        "    if var_m <= 0:\n",
        "        return dict(mean_d=float(d.mean()), dm_stat=np.nan, p_value=np.nan, n_obs=T)\n",
        "    dm    = d.mean() / np.sqrt(var_m)\n",
        "    pval  = 1.0 - student_t.cdf(dm, df=T - 1)\n",
        "    return dict(mean_d=float(d.mean()), dm_stat=float(dm), p_value=float(pval), n_obs=T)\n",
        "\n",
        "\n",
        "def dm_by_regime(df: pd.DataFrame, y_col: str,\n",
        "                 pred_ols: np.ndarray, pred_gbr: np.ndarray,\n",
        "                 lags: int = 6) -> pd.DataFrame:\n",
        "    \"\"\"Compute DM(OLS vs GBR) for each regime; positive d ⟹ GBR better.\"\"\"\n",
        "    y     = df[y_col].values\n",
        "    se_o  = (y - pred_ols)**2\n",
        "    se_g  = (y - pred_gbr)**2\n",
        "    rows  = []\n",
        "    for r in REGIMES:\n",
        "        mask = (df[\"Regime\"].values == r)\n",
        "        res  = dm_test_one_sided(se_o[mask] - se_g[mask], lags=lags)\n",
        "        rows.append({\"Regime\": r,\n",
        "                     \"mean(SE_OLS-SE_GBR)\": res[\"mean_d\"],\n",
        "                     \"DM_stat\"            : res[\"dm_stat\"],\n",
        "                     \"p_value (H1:GBR>)  \": res[\"p_value\"],\n",
        "                     \"n_obs\"              : res[\"n_obs\"]})\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "def wald_regime_betas(df: pd.DataFrame, label: str = \"\"):\n",
        "    \"\"\"HC3 Wald test for beta equality across Bear/Normal/Bull regimes.\"\"\"\n",
        "    dw = df.copy()\n",
        "    dw[\"Bear\"]     = (dw[\"Regime\"] == \"Bear (Bot 25%)\").astype(int)\n",
        "    dw[\"Bull\"]     = (dw[\"Regime\"] == \"Bull (Top 25%)\").astype(int)\n",
        "    dw[\"MKT_Bear\"] = dw[\"excess_mkt\"] * dw[\"Bear\"]\n",
        "    dw[\"MKT_Bull\"] = dw[\"excess_mkt\"] * dw[\"Bull\"]\n",
        "    mod = sm.OLS(dw[\"excess_nvda\"],\n",
        "                 sm.add_constant(dw[[\"excess_mkt\",\"MKT_Bear\",\"MKT_Bull\"]])\n",
        "                 ).fit(cov_type=\"HC3\")\n",
        "    print(f\"\\n=== Wald Beta-Equality [{label}] ===\")\n",
        "    print(mod.summary().tables[1])\n",
        "    for lbl, cstr in [(\"Bear vs Normal\",\"MKT_Bear = 0\"),\n",
        "                      (\"Bull vs Normal\",\"MKT_Bull = 0\"),\n",
        "                      (\"Bear vs Bull\",  \"MKT_Bear - MKT_Bull = 0\")]:\n",
        "        r = mod.t_test(cstr)\n",
        "        print(f\"  {lbl}: t={float(r.tvalue):.3f}, p={float(r.pvalue):.4f}\")\n",
        "    return mod\n",
        "\n",
        "\n",
        "def build_models(X_tr: np.ndarray, y_tr: np.ndarray) -> dict:\n",
        "    \"\"\"Fit OLS / RF / SVR / GBR on given training arrays.\"\"\"\n",
        "    ols = LinearRegression().fit(X_tr, y_tr)\n",
        "    rf  = RandomForestRegressor(\n",
        "            n_estimators=300, max_depth=None,\n",
        "            min_samples_leaf=2, random_state=42, n_jobs=-1).fit(X_tr, y_tr)\n",
        "    svr = Pipeline([(\"sc\", StandardScaler()),\n",
        "                    (\"sv\", SVR(kernel=\"rbf\", C=10, gamma=\"scale\"))]).fit(X_tr, y_tr)\n",
        "    gbr = GradientBoostingRegressor(\n",
        "            n_estimators=500, learning_rate=0.05,\n",
        "            max_depth=3, loss=\"huber\", random_state=42).fit(X_tr, y_tr)\n",
        "    return {\"OLS\": ols, \"RF\": rf, \"SVR\": svr, \"GBR\": gbr}\n",
        "\n",
        "\n",
        "def predict_all(models: dict, X: np.ndarray) -> dict:\n",
        "    return {name: m.predict(X) for name, m in models.items()}\n",
        "\n",
        "\n",
        "def regime_metrics_table(df_eval: pd.DataFrame, preds: dict,\n",
        "                          y_col: str = \"excess_nvda\",\n",
        "                          tag: str = \"in-sample\") -> pd.DataFrame:\n",
        "    y = df_eval[y_col].values\n",
        "    rows = []\n",
        "    for m, yhat in preds.items():\n",
        "        rows.append({\"Sample\": f\"Full ({tag})\", \"Model\": m, **metrics_block(y, yhat)})\n",
        "    for r in REGIMES:\n",
        "        mask = (df_eval[\"Regime\"].values == r)\n",
        "        for m, yhat in preds.items():\n",
        "            rows.append({\"Sample\": f\"{r} ({tag})\", \"Model\": m,\n",
        "                         **metrics_block(y[mask], yhat[mask])})\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 1) DATA INGESTION\n",
        "# ============================================================\n",
        "df     = build_dataset(start=START,     end=END, label=\"primary 2015–2025\")\n",
        "df_ext = build_dataset(start=START_EXT, end=END, label=\"extended 2010–2025\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 2) CAPM — Full-sample (2015–2025) diagnostics  [DESIGN A]\n",
        "# ============================================================\n",
        "capm_full, diag_full = run_capm_diagnostics(df, \"CAPM (Full 2015–2025)\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 3) Regime definitions on full sample  [DESIGN A]\n",
        "# ============================================================\n",
        "print(\"\\n--- Regime assignments (2015–2025) ---\")\n",
        "df, q25, q75 = add_regimes(df)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 4) Train ALL models on full sample  [DESIGN A]\n",
        "# ============================================================\n",
        "X_full   = df[[\"excess_mkt\"]].values\n",
        "y_full   = df[\"excess_nvda\"].values\n",
        "models_A = build_models(X_full, y_full)\n",
        "preds_A  = predict_all(models_A, X_full)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 5) In-sample metrics by regime  [DESIGN A]\n",
        "# ============================================================\n",
        "met_A = regime_metrics_table(df, preds_A, tag=\"in-sample\")\n",
        "print(\"\\n=== DESIGN A: In-sample R² by regime ===\")\n",
        "idx_A = [\"Full (in-sample)\"] + [f\"{r} (in-sample)\" for r in REGIMES]\n",
        "print(met_A.pivot(index=\"Sample\", columns=\"Model\", values=\"R2\").loc[idx_A])\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 6) Kruskal–Wallis: |errors| differ across regimes?  [DESIGN A]\n",
        "# ============================================================\n",
        "print(\"\\n=== Kruskal–Wallis on |errors| across regimes [Design A] ===\")\n",
        "for m, yhat in preds_A.items():\n",
        "    groups = [np.abs(y_full[(df[\"Regime\"] == r).values] - yhat[(df[\"Regime\"] == r).values])\n",
        "              for r in REGIMES]\n",
        "    H, p = kruskal(*groups)\n",
        "    print(f\"  {m:3s}  H={H:.3f}  p={p:.4f}\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 7) Wald beta-equality test  [DESIGN A]\n",
        "# ============================================================\n",
        "wald_A = wald_regime_betas(df, label=\"Design A, 2015–2025\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 8) Diebold–Mariano (OLS vs GBR, in-sample)  [DESIGN A]\n",
        "# ============================================================\n",
        "print(\"\\n=== Diebold–Mariano OLS vs GBR [Design A — in-sample] ===\")\n",
        "print(dm_by_regime(df, \"excess_nvda\", preds_A[\"OLS\"], preds_A[\"GBR\"]))\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# ENHANCEMENT ①  ROLLING-BETA DIAGNOSTIC (252-day, 2015–2025)\n",
        "# ============================================================\n",
        "print(\"\\n\\n\" + \"=\"*60)\n",
        "print(\"  ENHANCEMENT ①: ROLLING-BETA (252-day window, 2015–2025)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "X_roll   = sm.add_constant(df[\"excess_mkt\"])\n",
        "roll_res = RollingOLS(df[\"excess_nvda\"], X_roll, window=ROLL_WIN).fit()\n",
        "\n",
        "roll_beta  = roll_res.params[\"excess_mkt\"]\n",
        "roll_alpha = roll_res.params[\"const\"]\n",
        "roll_bse   = roll_res.bse[\"excess_mkt\"]\n",
        "\n",
        "ci_lo = roll_beta - 1.96 * roll_bse\n",
        "ci_hi = roll_beta + 1.96 * roll_bse\n",
        "\n",
        "print(f\"\\n  Static full-sample β : {diag_full['beta']:.4f}\")\n",
        "print(f\"  Rolling β range       : [{roll_beta.dropna().min():.4f},  {roll_beta.dropna().max():.4f}]\")\n",
        "print(f\"  Rolling α range       : [{roll_alpha.dropna().min():.4f}, {roll_alpha.dropna().max():.4f}]\")\n",
        "\n",
        "static_outside = ((diag_full[\"beta\"] < ci_lo) | (diag_full[\"beta\"] > ci_hi)).dropna()\n",
        "print(f\"  % windows static β outside rolling 95% CI: {static_outside.mean()*100:.1f}%\")\n",
        "\n",
        "fig1, (ax1, ax2) = plt.subplots(2, 1, figsize=(13, 8), sharex=True)\n",
        "\n",
        "ax1.plot(roll_beta.index, roll_beta.values, color=\"#636EFA\", lw=1.6,\n",
        "         label=\"Rolling β (252d)\")\n",
        "ax1.fill_between(roll_beta.index, ci_lo.values, ci_hi.values,\n",
        "                 alpha=0.18, color=\"#636EFA\", label=\"95% CI\")\n",
        "ax1.axhline(diag_full[\"beta\"], color=\"red\", ls=\"--\", lw=1.3,\n",
        "            label=f\"Static β={diag_full['beta']:.3f}\")\n",
        "ax1.axvline(pd.Timestamp(\"2022-01-01\"), color=\"orange\", ls=\":\", lw=1.5,\n",
        "            label=\"Design B split (Jan-2022)\")\n",
        "ax1.axvline(pd.Timestamp(\"2023-01-01\"), color=\"green\",  ls=\":\", lw=1.2,\n",
        "            label=\"ChatGPT era (Jan-2023)\")\n",
        "ax1.set_ylabel(\"Rolling β (market sensitivity)\")\n",
        "ax1.set_title(\"NVDA — Rolling CAPM Diagnostics (252-day window, 2015–2025)\")\n",
        "ax1.legend(fontsize=8, loc=\"upper left\", ncol=2)\n",
        "ax1.grid(alpha=0.3)\n",
        "\n",
        "ax2.plot(roll_alpha.index, roll_alpha.values, color=\"#00CC96\", lw=1.6,\n",
        "         label=\"Rolling α (%/day, 252d)\")\n",
        "ax2.axhline(diag_full[\"alpha\"], color=\"red\", ls=\"--\", lw=1.3,\n",
        "            label=f\"Static α={diag_full['alpha']:.4f}\")\n",
        "ax2.axhline(0, color=\"black\", lw=0.8)\n",
        "ax2.axvline(pd.Timestamp(\"2022-01-01\"), color=\"orange\", ls=\":\", lw=1.5)\n",
        "ax2.axvline(pd.Timestamp(\"2023-01-01\"), color=\"green\",  ls=\":\", lw=1.2)\n",
        "ax2.set_ylabel(\"Rolling α (%/day)\")\n",
        "ax2.set_xlabel(\"Date\")\n",
        "ax2.legend(fontsize=8, loc=\"upper left\")\n",
        "ax2.grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"rolling_beta_2015.png\", dpi=150)\n",
        "plt.close()\n",
        "print(\"  → Plot saved: rolling_beta_2015.png\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# ENHANCEMENT ②  DESIGN B: 2015–2021 TRAIN → 2022–2025 OOS TEST\n",
        "# ============================================================\n",
        "print(\"\\n\\n\" + \"=\"*60)\n",
        "print(\"  ENHANCEMENT ②: DESIGN B — Train 2015–2021 / Test 2022–2025\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "df_train = df[df.index <= TRAIN_END].copy()\n",
        "df_test  = df[df.index >= TEST_START].copy()\n",
        "\n",
        "X_tr = df_train[[\"excess_mkt\"]].values;  y_tr = df_train[\"excess_nvda\"].values\n",
        "X_te = df_test [[\"excess_mkt\"]].values;  y_te = df_test [\"excess_nvda\"].values\n",
        "\n",
        "print(f\"\\n  Train: {df_train.shape[0]} rows \"\n",
        "      f\"({df_train.index[0].date()} → {df_train.index[-1].date()})\")\n",
        "print(f\"  Test : {df_test.shape[0]}  rows \"\n",
        "      f\"({df_test.index[0].date()}  → {df_test.index[-1].date()})\")\n",
        "\n",
        "# ── CAPM on train and test sub-periods ──\n",
        "_, diag_train = run_capm_diagnostics(df_train, \"CAPM Train (2015–2021)\")\n",
        "_, diag_test  = run_capm_diagnostics(df_test,  \"CAPM Test  (2022–2025)\")\n",
        "\n",
        "# ── Train all models, predict OOS ──\n",
        "models_B = build_models(X_tr, y_tr)\n",
        "preds_B  = predict_all(models_B, X_te)\n",
        "\n",
        "# ── Regime definitions on the TEST window only ──\n",
        "print(\"\\n  Regime thresholds — test window (2022–2025):\")\n",
        "df_test, q25_b, q75_b = add_regimes(df_test)\n",
        "\n",
        "# ── OOS metrics by regime ──\n",
        "met_B = regime_metrics_table(df_test, preds_B, tag=\"OOS\")\n",
        "print(\"\\n=== DESIGN B: OOS R² by regime ===\")\n",
        "idx_B = [\"Full (OOS)\"] + [f\"{r} (OOS)\" for r in REGIMES]\n",
        "print(met_B.pivot(index=\"Sample\", columns=\"Model\", values=\"R2\").loc[idx_B])\n",
        "\n",
        "# ── OOS Kruskal–Wallis ──\n",
        "print(\"\\n=== Kruskal–Wallis on |OOS errors| across regimes [Design B] ===\")\n",
        "for m, yhat in preds_B.items():\n",
        "    groups = [np.abs(y_te[(df_test[\"Regime\"] == r).values]\n",
        "                     - yhat[(df_test[\"Regime\"] == r).values])\n",
        "              for r in REGIMES]\n",
        "    H, p = kruskal(*groups)\n",
        "    print(f\"  {m:3s}  H={H:.3f}  p={p:.4f}\")\n",
        "\n",
        "# ── Genuine OOS DM tests ──\n",
        "print(\"\\n=== Diebold–Mariano OLS vs GBR [Design B — GENUINE OOS] ===\")\n",
        "print(dm_by_regime(df_test, \"excess_nvda\", preds_B[\"OLS\"], preds_B[\"GBR\"]))\n",
        "\n",
        "# ── Wald beta-equality on TEST window ──\n",
        "wald_B = wald_regime_betas(df_test, label=\"Design B test window 2022–2025\")\n",
        "\n",
        "# ── Head-to-head Design A vs Design B R² ──\n",
        "print(\"\\n=== Design A (in-sample) vs Design B (OOS) — Full-window R² ===\")\n",
        "cmp = pd.DataFrame([\n",
        "    {\"Design\": \"A (in-sample 2015–2025)\",\n",
        "     **{m: metrics_block(y_full, preds_A[m])[\"R2\"] for m in preds_A}},\n",
        "    {\"Design\": \"B (OOS train2015–21 / test2022–25)\",\n",
        "     **{m: metrics_block(y_te,   preds_B[m])[\"R2\"] for m in preds_B}},\n",
        "]).set_index(\"Design\")\n",
        "print(cmp)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# ENHANCEMENT ③  EXTENDED WINDOW 2010–2025: CAPM + CHOW TEST\n",
        "# ============================================================\n",
        "print(\"\\n\\n\" + \"=\"*60)\n",
        "print(\"  ENHANCEMENT ③: EXTENDED WINDOW — 2010–2025 CAPM + CHOW TEST\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ── CAPM on 2010–2025 ──\n",
        "_, diag_ext = run_capm_diagnostics(df_ext, \"CAPM Extended (2010–2025)\")\n",
        "\n",
        "# ── Side-by-side parameter comparison ──\n",
        "print(\"\\n=== CAPM parameter comparison: 2015–2025 vs 2010–2025 ===\")\n",
        "comp = pd.DataFrame([diag_full, diag_ext]).set_index(\"label\")[[\n",
        "    \"alpha\",\"beta\",\"alpha_t\",\"beta_t\",\"R2\",\"nobs\",\"BP_p\",\"RESET_p\",\"JB_p\",\"DW\"]]\n",
        "print(comp.T.to_string())\n",
        "\n",
        "# ── Chow structural-break test at 2022-01-01 ──\n",
        "#    H0: single stable regression on 2010–2025\n",
        "#    H1: separate intercept AND slope for pre/post 2022\n",
        "print(\"\\n=== Chow Structural-Break Test at 2022-01-01 [2010–2025 sample] ===\")\n",
        "dc = df_ext.copy()\n",
        "dc[\"D_post\"]   = (dc.index >= pd.Timestamp(\"2022-01-01\")).astype(int)\n",
        "dc[\"MKT_post\"] = dc[\"excess_mkt\"] * dc[\"D_post\"]\n",
        "chow_mod = sm.OLS(dc[\"excess_nvda\"],\n",
        "                  sm.add_constant(dc[[\"excess_mkt\",\"D_post\",\"MKT_post\"]])\n",
        "                  ).fit(cov_type=\"HC3\")\n",
        "print(chow_mod.summary().tables[1])\n",
        "chow_f = chow_mod.f_test([\"D_post = 0\", \"MKT_post = 0\"])\n",
        "print(f\"\\n  Chow F (joint H0): F={float(chow_f.fvalue):.3f}, \"\n",
        "      f\"p={float(chow_f.pvalue):.4f}\")\n",
        "print(\"  p<0.05 → reject structural stability at 2022 break\")\n",
        "\n",
        "# ── Also test at 2015-01-01 ──\n",
        "dc2 = df_ext.copy()\n",
        "dc2[\"D_post\"]   = (dc2.index >= pd.Timestamp(\"2015-01-01\")).astype(int)\n",
        "dc2[\"MKT_post\"] = dc2[\"excess_mkt\"] * dc2[\"D_post\"]\n",
        "chow2_f = sm.OLS(dc2[\"excess_nvda\"],\n",
        "                 sm.add_constant(dc2[[\"excess_mkt\",\"D_post\",\"MKT_post\"]])\n",
        "                 ).fit(cov_type=\"HC3\").f_test([\"D_post = 0\", \"MKT_post = 0\"])\n",
        "print(f\"\\n  Chow F at 2015-01-01: F={float(chow2_f.fvalue):.3f}, \"\n",
        "      f\"p={float(chow2_f.pvalue):.4f}\")\n",
        "\n",
        "# ── Extended rolling-beta plot (2010–2025) ──\n",
        "print(\"\\n  Fitting rolling OLS on 2010–2025 extended dataset ...\")\n",
        "df_ext, q25_e, q75_e = add_regimes(df_ext)\n",
        "\n",
        "X_roll_ext = sm.add_constant(df_ext[\"excess_mkt\"])\n",
        "roll_ext   = RollingOLS(df_ext[\"excess_nvda\"], X_roll_ext, window=ROLL_WIN).fit()\n",
        "rb_ext     = roll_ext.params[\"excess_mkt\"]\n",
        "ra_ext     = roll_ext.params[\"const\"]\n",
        "rse_ext    = roll_ext.bse[\"excess_mkt\"]\n",
        "ci_lo_e    = rb_ext - 1.96 * rse_ext\n",
        "ci_hi_e    = rb_ext + 1.96 * rse_ext\n",
        "\n",
        "fig2, (ax3, ax4) = plt.subplots(2, 1, figsize=(14, 8), sharex=True)\n",
        "\n",
        "ax3.plot(rb_ext.index, rb_ext.values, color=\"#636EFA\", lw=1.6,\n",
        "         label=\"Rolling β (252d)\")\n",
        "ax3.fill_between(rb_ext.index, ci_lo_e.values, ci_hi_e.values,\n",
        "                 alpha=0.18, color=\"#636EFA\", label=\"95% CI\")\n",
        "ax3.axhline(diag_ext[\"beta\"],  color=\"red\",    ls=\"--\", lw=1.3,\n",
        "            label=f\"Static β 2010–2025={diag_ext['beta']:.3f}\")\n",
        "ax3.axhline(diag_full[\"beta\"], color=\"purple\", ls=\":\",  lw=1.2,\n",
        "            label=f\"Static β 2015–2025={diag_full['beta']:.3f}\")\n",
        "for vdate, col, lbl in [\n",
        "        (\"2015-01-01\",\"gray\",   \"Primary window start (2015)\"),\n",
        "        (\"2022-01-01\",\"orange\", \"Design B split (Jan-2022)\"),\n",
        "        (\"2023-01-01\",\"green\",  \"ChatGPT era (Jan-2023)\")]:\n",
        "    ax3.axvline(pd.Timestamp(vdate), color=col, ls=\":\", lw=1.2, label=lbl)\n",
        "ax3.set_ylabel(\"Rolling β\")\n",
        "ax3.set_title(\"NVDA — Rolling CAPM (252-day window, 2010–2025 extended)\")\n",
        "ax3.legend(fontsize=7.5, loc=\"upper left\", ncol=2)\n",
        "ax3.grid(alpha=0.3)\n",
        "\n",
        "ax4.plot(ra_ext.index, ra_ext.values, color=\"#00CC96\", lw=1.6,\n",
        "         label=\"Rolling α (%/day, 252d)\")\n",
        "ax4.axhline(diag_ext[\"alpha\"],  color=\"red\",    ls=\"--\", lw=1.3,\n",
        "            label=f\"Static α 2010–2025={diag_ext['alpha']:.4f}\")\n",
        "ax4.axhline(diag_full[\"alpha\"], color=\"purple\", ls=\":\",  lw=1.2,\n",
        "            label=f\"Static α 2015–2025={diag_full['alpha']:.4f}\")\n",
        "ax4.axhline(0, color=\"black\", lw=0.8)\n",
        "for vdate, col in [(\"2015-01-01\",\"gray\"),\n",
        "                   (\"2022-01-01\",\"orange\"),\n",
        "                   (\"2023-01-01\",\"green\")]:\n",
        "    ax4.axvline(pd.Timestamp(vdate), color=col, ls=\":\", lw=1.2)\n",
        "ax4.set_ylabel(\"Rolling α (%/day)\")\n",
        "ax4.set_xlabel(\"Date\")\n",
        "ax4.legend(fontsize=7.5, loc=\"upper left\")\n",
        "ax4.grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"rolling_beta_2010.png\", dpi=150)\n",
        "plt.close()\n",
        "print(\"  → Plot saved: rolling_beta_2010.png\")\n",
        "\n",
        "# ── ML on extended window (Design A–style) ──\n",
        "print(\"\\n=== ML models on 2010–2025 (Design A style, in-sample) ===\")\n",
        "X_e      = df_ext[[\"excess_mkt\"]].values\n",
        "y_e      = df_ext[\"excess_nvda\"].values\n",
        "models_E = build_models(X_e, y_e)\n",
        "preds_E  = predict_all(models_E, X_e)\n",
        "\n",
        "met_E = regime_metrics_table(df_ext, preds_E, tag=\"in-sample 2010–2025\")\n",
        "print(met_E.pivot(index=\"Sample\", columns=\"Model\", values=\"R2\").loc[\n",
        "    [\"Full (in-sample 2010–2025)\"]\n",
        "    + [f\"{r} (in-sample 2010–2025)\" for r in REGIMES]])\n",
        "\n",
        "# ── KW on extended window ──\n",
        "print(\"\\n=== Kruskal–Wallis on |errors| across regimes [2010–2025] ===\")\n",
        "for m, yhat in preds_E.items():\n",
        "    groups = [np.abs(y_e[(df_ext[\"Regime\"] == r).values]\n",
        "                     - yhat[(df_ext[\"Regime\"] == r).values])\n",
        "              for r in REGIMES]\n",
        "    H, p = kruskal(*groups)\n",
        "    print(f\"  {m:3s}  H={H:.3f}  p={p:.4f}\")\n",
        "\n",
        "# ── DM on extended window ──\n",
        "print(\"\\n=== Diebold–Mariano OLS vs GBR [2010–2025, in-sample] ===\")\n",
        "print(dm_by_regime(df_ext, \"excess_nvda\", preds_E[\"OLS\"], preds_E[\"GBR\"]))\n",
        "\n",
        "print(\"\\n\\n=== ALL DONE ===\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2d2rhwbT85nq",
        "outputId": "8c964032-c81b-40b3-8ca9-49f7640c2f26"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Building dataset [primary 2015–2025]: 2015-01-01 → 2025-09-30 ---\n",
            "  Ready: 2700 rows | cols: ['nvda_px', 'excess_mkt', 'rf', 'nvda_ret', 'excess_nvda']\n",
            "\n",
            "--- Building dataset [extended 2010–2025]: 2010-01-01 → 2025-09-30 ---\n",
            "  Ready: 3958 rows | cols: ['nvda_px', 'excess_mkt', 'rf', 'nvda_ret', 'excess_nvda']\n",
            "\n",
            "=== OLS Summary: CAPM (Full 2015–2025) ===\n",
            "==============================================================================\n",
            "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "const          0.1735      0.045      3.870      0.000       0.086       0.261\n",
            "excess_mkt     1.7383      0.039     45.130      0.000       1.663       1.814\n",
            "==============================================================================\n",
            "\n",
            "Diagnostics:\n",
            "  alpha     : 0.1735\n",
            "  beta      : 1.7383\n",
            "  alpha_t   : 3.8700\n",
            "  beta_t    : 45.1305\n",
            "  R2        : 0.4302\n",
            "  nobs      : 2700\n",
            "  BP_p      : 0.9440\n",
            "  RESET_p   : 0.3035\n",
            "  JB_p      : 0.0000\n",
            "  DW        : 2.0850\n",
            "\n",
            "--- Regime assignments (2015–2025) ---\n",
            "  Regime thresholds → q25=-0.4000, q75=0.6025\n",
            "Regime\n",
            "Normal (Mid 50%)    1351\n",
            "Bull (Top 25%)       675\n",
            "Bear (Bot 25%)       674\n",
            "\n",
            "=== DESIGN A: In-sample R² by regime ===\n",
            "Model                              GBR       OLS        RF       SVR\n",
            "Sample                                                              \n",
            "Full (in-sample)              0.497093  0.430171  0.526990  0.433936\n",
            "Bear (Bot 25%) (in-sample)    0.478854  0.309769  0.513430  0.322283\n",
            "Normal (Mid 50%) (in-sample)  0.062125  0.038266  0.109193  0.035877\n",
            "Bull (Top 25%) (in-sample)    0.365786  0.251133  0.411524  0.259446\n",
            "\n",
            "=== Kruskal–Wallis on |errors| across regimes [Design A] ===\n",
            "  OLS  H=43.121  p=0.0000\n",
            "  RF   H=9.964  p=0.0069\n",
            "  SVR  H=39.667  p=0.0000\n",
            "  GBR  H=13.283  p=0.0013\n",
            "\n",
            "=== Wald Beta-Equality [Design A, 2015–2025] ===\n",
            "==============================================================================\n",
            "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "const          0.1819      0.056      3.234      0.001       0.072       0.292\n",
            "excess_mkt     1.7166      0.206      8.317      0.000       1.312       2.121\n",
            "MKT_Bear       0.0331      0.224      0.148      0.883      -0.406       0.473\n",
            "MKT_Bull       0.0102      0.213      0.048      0.962      -0.407       0.427\n",
            "==============================================================================\n",
            "  Bear vs Normal: t=0.148, p=0.8826\n",
            "  Bull vs Normal: t=0.048, p=0.9620\n",
            "  Bear vs Bull: t=0.204, p=0.8386\n",
            "\n",
            "=== Diebold–Mariano OLS vs GBR [Design A — in-sample] ===\n",
            "             Regime  mean(SE_OLS-SE_GBR)   DM_stat  p_value (H1:GBR>)    n_obs\n",
            "0    Bear (Bot 25%)             1.414854  5.992171         1.686926e-09    674\n",
            "1  Normal (Mid 50%)             0.125825  4.971689         3.744389e-07   1351\n",
            "2    Bull (Top 25%)             0.879567  5.102123         2.186094e-07    675\n",
            "\n",
            "\n",
            "============================================================\n",
            "  ENHANCEMENT ①: ROLLING-BETA (252-day window, 2015–2025)\n",
            "============================================================\n",
            "\n",
            "  Static full-sample β : 1.7383\n",
            "  Rolling β range       : [1.0807,  2.7121]\n",
            "  Rolling α range       : [-0.2243, 0.5400]\n",
            "  % windows static β outside rolling 95% CI: 59.6%\n",
            "  → Plot saved: rolling_beta_2015.png\n",
            "\n",
            "\n",
            "============================================================\n",
            "  ENHANCEMENT ②: DESIGN B — Train 2015–2021 / Test 2022–2025\n",
            "============================================================\n",
            "\n",
            "  Train: 1762 rows (2015-01-05 → 2021-12-31)\n",
            "  Test : 938  rows (2022-01-03  → 2025-09-29)\n",
            "\n",
            "=== OLS Summary: CAPM Train (2015–2021) ===\n",
            "==============================================================================\n",
            "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "const          0.1806      0.054      3.347      0.001       0.075       0.286\n",
            "excess_mkt     1.5314      0.047     32.672      0.000       1.439       1.623\n",
            "==============================================================================\n",
            "\n",
            "Diagnostics:\n",
            "  alpha     : 0.1806\n",
            "  beta      : 1.5314\n",
            "  alpha_t   : 3.3470\n",
            "  beta_t    : 32.6718\n",
            "  R2        : 0.3775\n",
            "  nobs      : 1762\n",
            "  BP_p      : 0.9275\n",
            "  RESET_p   : 0.9930\n",
            "  JB_p      : 0.0000\n",
            "  DW        : 2.0761\n",
            "\n",
            "=== OLS Summary: CAPM Test  (2022–2025) ===\n",
            "==============================================================================\n",
            "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "const          0.1719      0.078      2.202      0.028       0.019       0.325\n",
            "excess_mkt     2.1025      0.066     32.018      0.000       1.974       2.231\n",
            "==============================================================================\n",
            "\n",
            "Diagnostics:\n",
            "  alpha     : 0.1719\n",
            "  beta      : 2.1025\n",
            "  alpha_t   : 2.2021\n",
            "  beta_t    : 32.0182\n",
            "  R2        : 0.5227\n",
            "  nobs      : 938\n",
            "  BP_p      : 0.8363\n",
            "  RESET_p   : 0.6162\n",
            "  JB_p      : 0.0000\n",
            "  DW        : 2.0932\n",
            "\n",
            "  Regime thresholds — test window (2022–2025):\n",
            "  Regime thresholds → q25=-0.5400, q75=0.6675\n",
            "Regime\n",
            "Normal (Mid 50%)    469\n",
            "Bull (Top 25%)      235\n",
            "Bear (Bot 25%)      234\n",
            "\n",
            "=== DESIGN B: OOS R² by regime ===\n",
            "Model                        GBR       OLS        RF       SVR\n",
            "Sample                                                        \n",
            "Full (OOS)              0.456735  0.484155  0.412855  0.481552\n",
            "Bear (Bot 25%) (OOS)    0.146281  0.160443  0.052217  0.205340\n",
            "Normal (Mid 50%) (OOS)  0.077308  0.080187 -0.000320  0.080271\n",
            "Bull (Top 25%) (OOS)    0.099716  0.245399  0.056838  0.185409\n",
            "\n",
            "=== Kruskal–Wallis on |OOS errors| across regimes [Design B] ===\n",
            "  OLS  H=6.590  p=0.0371\n",
            "  RF   H=1.837  p=0.3991\n",
            "  SVR  H=4.927  p=0.0851\n",
            "  GBR  H=7.344  p=0.0254\n",
            "\n",
            "=== Diebold–Mariano OLS vs GBR [Design B — GENUINE OOS] ===\n",
            "             Regime  mean(SE_OLS-SE_GBR)   DM_stat  p_value (H1:GBR>)    n_obs\n",
            "0    Bear (Bot 25%)            -0.111424 -0.355467             0.638719    234\n",
            "1  Normal (Mid 50%)            -0.018823 -0.363897             0.641950    469\n",
            "2    Bull (Top 25%)            -1.157983 -2.865869             0.997731    235\n",
            "\n",
            "=== Wald Beta-Equality [Design B test window 2022–2025] ===\n",
            "==============================================================================\n",
            "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "const          0.1402      0.096      1.456      0.145      -0.049       0.329\n",
            "excess_mkt     2.3942      0.425      5.628      0.000       1.560       3.228\n",
            "MKT_Bear      -0.3367      0.436     -0.772      0.440      -1.192       0.518\n",
            "MKT_Bull      -0.2696      0.442     -0.610      0.542      -1.136       0.597\n",
            "==============================================================================\n",
            "  Bear vs Normal: t=-0.772, p=0.4403\n",
            "  Bull vs Normal: t=-0.610, p=0.5418\n",
            "  Bear vs Bull: t=-0.395, p=0.6927\n",
            "\n",
            "=== Design A (in-sample) vs Design B (OOS) — Full-window R² ===\n",
            "                                         OLS        RF       SVR       GBR\n",
            "Design                                                                    \n",
            "A (in-sample 2015–2025)             0.430171  0.526990  0.433936  0.497093\n",
            "B (OOS train2015–21 / test2022–25)  0.484155  0.412855  0.481552  0.456735\n",
            "\n",
            "\n",
            "============================================================\n",
            "  ENHANCEMENT ③: EXTENDED WINDOW — 2010–2025 CAPM + CHOW TEST\n",
            "============================================================\n",
            "\n",
            "=== OLS Summary: CAPM Extended (2010–2025) ===\n",
            "==============================================================================\n",
            "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "const          0.1017      0.035      2.884      0.004       0.033       0.171\n",
            "excess_mkt     1.6437      0.031     52.532      0.000       1.582       1.705\n",
            "==============================================================================\n",
            "\n",
            "Diagnostics:\n",
            "  alpha     : 0.1017\n",
            "  beta      : 1.6437\n",
            "  alpha_t   : 2.8837\n",
            "  beta_t    : 52.5316\n",
            "  R2        : 0.4109\n",
            "  nobs      : 3958\n",
            "  BP_p      : 0.8475\n",
            "  RESET_p   : 0.1932\n",
            "  JB_p      : 0.0000\n",
            "  DW        : 2.0414\n",
            "\n",
            "=== CAPM parameter comparison: 2015–2025 vs 2010–2025 ===\n",
            "label    CAPM (Full 2015–2025)  CAPM Extended (2010–2025)\n",
            "alpha                 0.173545                   0.101711\n",
            "beta                  1.738336                   1.643712\n",
            "alpha_t               3.869995                   2.883739\n",
            "beta_t               45.130450                  52.531599\n",
            "R2                    0.430171                   0.410921\n",
            "nobs               2700.000000                3958.000000\n",
            "BP_p                  0.944040                   0.847452\n",
            "RESET_p               0.303482                   0.193247\n",
            "JB_p                  0.000000                   0.000000\n",
            "DW                    2.084965                   2.041364\n",
            "\n",
            "=== Chow Structural-Break Test at 2022-01-01 [2010–2025 sample] ===\n",
            "==============================================================================\n",
            "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "const          0.0853      0.039      2.196      0.028       0.009       0.161\n",
            "excess_mkt     1.4798      0.036     40.629      0.000       1.408       1.551\n",
            "D_post         0.0865      0.087      0.992      0.321      -0.084       0.257\n",
            "MKT_post       0.6226      0.076      8.228      0.000       0.474       0.771\n",
            "==============================================================================\n",
            "\n",
            "  Chow F (joint H0): F=34.474, p=0.0000\n",
            "  p<0.05 → reject structural stability at 2022 break\n",
            "\n",
            "  Chow F at 2015-01-01: F=17.514, p=0.0000\n",
            "\n",
            "  Fitting rolling OLS on 2010–2025 extended dataset ...\n",
            "  Regime thresholds → q25=-0.4000, q75=0.5900\n",
            "Regime\n",
            "Normal (Mid 50%)    1988\n",
            "Bull (Top 25%)       987\n",
            "Bear (Bot 25%)       983\n",
            "  → Plot saved: rolling_beta_2010.png\n",
            "\n",
            "=== ML models on 2010–2025 (Design A style, in-sample) ===\n",
            "Model                                        GBR       OLS        RF       SVR\n",
            "Sample                                                                        \n",
            "Full (in-sample 2010–2025)              0.461482  0.410921  0.490478  0.412107\n",
            "Bear (Bot 25%) (in-sample 2010–2025)    0.428857  0.304238  0.475222  0.309741\n",
            "Normal (Mid 50%) (in-sample 2010–2025)  0.045369  0.024279  0.075346  0.023181\n",
            "Bull (Top 25%) (in-sample 2010–2025)    0.314513  0.234037  0.363179  0.235351\n",
            "\n",
            "=== Kruskal–Wallis on |errors| across regimes [2010–2025] ===\n",
            "  OLS  H=42.323  p=0.0000\n",
            "  RF   H=18.015  p=0.0001\n",
            "  SVR  H=40.714  p=0.0000\n",
            "  GBR  H=20.528  p=0.0000\n",
            "\n",
            "=== Diebold–Mariano OLS vs GBR [2010–2025, in-sample] ===\n",
            "             Regime  mean(SE_OLS-SE_GBR)   DM_stat  p_value (H1:GBR>)    n_obs\n",
            "0    Bear (Bot 25%)             0.918780  6.934158         3.699707e-12    983\n",
            "1  Normal (Mid 50%)             0.097357  3.799321         7.472829e-05   1988\n",
            "2    Bull (Top 25%)             0.578906  5.637605         1.125341e-08    987\n",
            "\n",
            "\n",
            "=== ALL DONE ===\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bOOadriu2Zww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "so the idea here was to run all the models on the full dataset, train the ML models and then split the dataset in 3 subsets (top25, bottom25, middle50) and then recalculate beta for those three, compare with the full dataset. For that we need also a statistical test to compare the erros to see if they are significantly different. Besides that we need also a test for non-linearity. (for the full dataset and for each subset separetely). If we indeed prove the non-linearity that would be the basis to prefer the non-linear models (ML models)."
      ],
      "metadata": {
        "id": "CdBiPBbwBJts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# DESIGN A — CORRECTED: Full-sample training + regime analysis\n",
        "# With complete non-linearity testing + error comparison\n",
        "# ============================================================\n",
        "\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import yfinance as yf\n",
        "from pandas_datareader import data as pdr\n",
        "\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.stats.diagnostic import (\n",
        "    het_breuschpagan,\n",
        "    linear_reset,\n",
        "    linear_harvey_collier,\n",
        "    linear_rainbow,\n",
        "    het_white\n",
        ")\n",
        "from statsmodels.stats.stattools import durbin_watson\n",
        "from scipy.stats import jarque_bera, kruskal, t as student_t, f as f_dist\n",
        "\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "pd.set_option(\"display.width\", 120)\n",
        "np.random.seed(42)\n",
        "\n",
        "START = \"2015-01-01\"\n",
        "END   = \"2025-09-30\"\n",
        "REGIMES = [\"Bear (Bot 25%)\", \"Normal (Mid 50%)\", \"Bull (Top 25%)\"]\n",
        "\n",
        "# ============================================================\n",
        "# HELPER FUNCTIONS\n",
        "# ============================================================\n",
        "\n",
        "def _flatten_yf(px: pd.DataFrame) -> pd.DataFrame:\n",
        "    px = px.copy()\n",
        "    if isinstance(px.columns, pd.MultiIndex):\n",
        "        px.columns = [c[0] if isinstance(c, tuple) else c for c in px.columns]\n",
        "    px.columns.name = None\n",
        "    return px\n",
        "\n",
        "\n",
        "def _yf_adj_close(ticker: str, start: str, end: str) -> pd.DataFrame:\n",
        "    try:\n",
        "        df = yf.download(ticker, start=start, end=end, progress=False, auto_adjust=True)\n",
        "        df = _flatten_yf(df)\n",
        "        for col in [\"Adj Close\", \"Close\", ticker]:\n",
        "            if col in df.columns:\n",
        "                return df[[col]].rename(columns={col: ticker})\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching {ticker}: {e}\")\n",
        "    return pd.DataFrame()\n",
        "\n",
        "\n",
        "def build_dataset(start: str = START, end: str = END) -> pd.DataFrame:\n",
        "    \"\"\"Download NVDA + Fama-French daily factors.\"\"\"\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Building dataset: {start} → {end}\")\n",
        "    print('='*70)\n",
        "\n",
        "    nvda = _yf_adj_close(\"NVDA\", start, end).rename(columns={\"NVDA\": \"nvda_px\"})\n",
        "    ff_raw = pdr.DataReader(\n",
        "        \"F-F_Research_Data_Factors_daily\", \"famafrench\", start=start, end=end\n",
        "    )[0]\n",
        "    ff = ff_raw[[\"Mkt-RF\", \"RF\"]].rename(columns={\"Mkt-RF\": \"excess_mkt\", \"RF\": \"rf\"})\n",
        "\n",
        "    df = nvda.join(ff, how=\"inner\").dropna()\n",
        "    df[\"nvda_ret\"]    = df[\"nvda_px\"].pct_change() * 100.0\n",
        "    df[\"excess_nvda\"] = df[\"nvda_ret\"] - df[\"rf\"]\n",
        "    df = df.dropna()\n",
        "\n",
        "    print(f\"Dataset ready: {df.shape[0]} observations\")\n",
        "    print(f\"Date range: {df.index[0].date()} to {df.index[-1].date()}\")\n",
        "    return df\n",
        "\n",
        "\n",
        "def add_regimes(df: pd.DataFrame) -> tuple:\n",
        "    \"\"\"Add regime column based on market excess return quantiles.\"\"\"\n",
        "    df = df.copy()\n",
        "    q25 = df[\"excess_mkt\"].quantile(0.25)\n",
        "    q75 = df[\"excess_mkt\"].quantile(0.75)\n",
        "\n",
        "    df[\"Regime\"] = \"Normal (Mid 50%)\"\n",
        "    df.loc[df[\"excess_mkt\"] < q25, \"Regime\"] = \"Bear (Bot 25%)\"\n",
        "    df.loc[df[\"excess_mkt\"] > q75, \"Regime\"] = \"Bull (Top 25%)\"\n",
        "\n",
        "    print(f\"\\nRegime thresholds: q25 = {q25:.4f}%, q75 = {q75:.4f}%\")\n",
        "    print(\"\\nRegime distribution:\")\n",
        "    print(df[\"Regime\"].value_counts().to_string())\n",
        "\n",
        "    return df, q25, q75\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# COMPREHENSIVE NON-LINEARITY TESTING\n",
        "# ============================================================\n",
        "\n",
        "def test_nonlinearity_comprehensive(X, y, label=\"Full Sample\"):\n",
        "    \"\"\"\n",
        "    Battery of non-linearity tests for CAPM specification.\n",
        "\n",
        "    Tests performed:\n",
        "    1. Ramsey RESET (omitted powers)\n",
        "    2. Harvey-Collier (recursive residuals)\n",
        "    3. Rainbow test (parameter constancy)\n",
        "    4. White test (non-linear heteroskedasticity)\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"NON-LINEARITY TEST BATTERY: {label}\")\n",
        "    print('='*70)\n",
        "\n",
        "    # Fit baseline CAPM\n",
        "    X_const = sm.add_constant(X)\n",
        "    model = sm.OLS(y, X_const).fit()\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # 1. Ramsey RESET (powers 2, 3)\n",
        "    print(\"\\n1. RAMSEY RESET TEST (Omitted Non-linear Terms)\")\n",
        "    print(\"-\" * 70)\n",
        "    try:\n",
        "        reset_2 = linear_reset(model, power=2, use_f=True)\n",
        "        reset_3 = linear_reset(model, power=3, use_f=True)\n",
        "\n",
        "        print(f\"   Power=2: F = {reset_2.fvalue:.4f}, p = {reset_2.pvalue:.4f}\")\n",
        "        print(f\"   Power=3: F = {reset_3.fvalue:.4f}, p = {reset_3.pvalue:.4f}\")\n",
        "\n",
        "        results['RESET_p2'] = reset_2.pvalue\n",
        "        results['RESET_p3'] = reset_3.pvalue\n",
        "\n",
        "        if reset_2.pvalue < 0.05:\n",
        "            print(\"   ⚠️  REJECT linearity (p<0.05): Non-linear terms significant\")\n",
        "        else:\n",
        "            print(\"   ✓ Cannot reject linearity (p≥0.05)\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ⚠️  RESET test failed: {e}\")\n",
        "        results['RESET_p2'] = np.nan\n",
        "        results['RESET_p3'] = np.nan\n",
        "\n",
        "    # 2. Harvey-Collier test\n",
        "    print(\"\\n2. HARVEY-COLLIER TEST (Recursive Residuals)\")\n",
        "    print(\"-\" * 70)\n",
        "    try:\n",
        "        hc_stat = linear_harvey_collier(model)\n",
        "        # Returns (t-stat, p-value)\n",
        "        print(f\"   t-statistic = {hc_stat[0]:.4f}, p = {hc_stat[1]:.4f}\")\n",
        "\n",
        "        results['HC_p'] = hc_stat[1]\n",
        "\n",
        "        if hc_stat[1] < 0.05:\n",
        "            print(\"   ⚠️  REJECT linearity (p<0.05): Specification error detected\")\n",
        "        else:\n",
        "            print(\"   ✓ Cannot reject linearity (p≥0.05)\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ⚠️  Harvey-Collier test failed: {e}\")\n",
        "        results['HC_p'] = np.nan\n",
        "\n",
        "    # 3. Rainbow test\n",
        "    print(\"\\n3. RAINBOW TEST (Parameter Constancy)\")\n",
        "    print(\"-\" * 70)\n",
        "    try:\n",
        "        rainbow_stat = linear_rainbow(model, frac=0.5)\n",
        "        # Returns (F-stat, p-value)\n",
        "        print(f\"   F-statistic = {rainbow_stat[0]:.4f}, p = {rainbow_stat[1]:.4f}\")\n",
        "\n",
        "        results['Rainbow_p'] = rainbow_stat[1]\n",
        "\n",
        "        if rainbow_stat[1] < 0.05:\n",
        "            print(\"   ⚠️  REJECT linearity (p<0.05): Parameters not constant\")\n",
        "        else:\n",
        "            print(\"   ✓ Cannot reject parameter constancy (p≥0.05)\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ⚠️  Rainbow test failed: {e}\")\n",
        "        results['Rainbow_p'] = np.nan\n",
        "\n",
        "    # 4. White test (general form for non-linearity)\n",
        "    print(\"\\n4. WHITE TEST (Non-linear Heteroskedasticity)\")\n",
        "    print(\"-\" * 70)\n",
        "    try:\n",
        "        # White test with cross-products\n",
        "        white_stat = het_white(model.resid, model.model.exog)\n",
        "        # Returns (LM-stat, LM p-value, F-stat, F p-value)\n",
        "        print(f\"   LM statistic = {white_stat[0]:.4f}, p = {white_stat[1]:.4f}\")\n",
        "        print(f\"   F statistic  = {white_stat[2]:.4f}, p = {white_stat[3]:.4f}\")\n",
        "\n",
        "        results['White_p'] = white_stat[1]\n",
        "\n",
        "        if white_stat[1] < 0.05:\n",
        "            print(\"   ⚠️  REJECT homoskedasticity (p<0.05): Non-linear patterns in variance\")\n",
        "        else:\n",
        "            print(\"   ✓ Cannot reject homoskedasticity (p≥0.05)\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ⚠️  White test failed: {e}\")\n",
        "        results['White_p'] = np.nan\n",
        "\n",
        "    # Summary interpretation\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"SUMMARY INTERPRETATION:\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    reject_count = sum([\n",
        "        results.get('RESET_p2', 1) < 0.05,\n",
        "        results.get('RESET_p3', 1) < 0.05,\n",
        "        results.get('HC_p', 1) < 0.05,\n",
        "        results.get('Rainbow_p', 1) < 0.05,\n",
        "        results.get('White_p', 1) < 0.05\n",
        "    ])\n",
        "\n",
        "    print(f\"Tests rejecting linearity at 5% level: {reject_count}/5\")\n",
        "\n",
        "    if reject_count >= 3:\n",
        "        print(\"\\n🔴 STRONG EVIDENCE of non-linearity\")\n",
        "        print(\"   → Machine learning models theoretically justified\")\n",
        "    elif reject_count >= 2:\n",
        "        print(\"\\n🟡 MODERATE EVIDENCE of non-linearity\")\n",
        "        print(\"   → ML models may capture additional patterns\")\n",
        "    else:\n",
        "        print(\"\\n🟢 WEAK/NO EVIDENCE of non-linearity\")\n",
        "        print(\"   → Linear CAPM appears adequate\")\n",
        "\n",
        "    return results, model\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# REGIME-SPECIFIC BETA ESTIMATION\n",
        "# ============================================================\n",
        "\n",
        "def estimate_regime_betas(df: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Estimate CAPM beta separately for each regime.\n",
        "    Compare with full-sample beta using Chow-style F-test.\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"REGIME-SPECIFIC BETA ESTIMATION\")\n",
        "    print('='*70)\n",
        "\n",
        "    # Full sample beta\n",
        "    X_full = sm.add_constant(df[\"excess_mkt\"])\n",
        "    y_full = df[\"excess_nvda\"]\n",
        "    model_full = sm.OLS(y_full, X_full).fit()\n",
        "\n",
        "    beta_full = model_full.params[\"excess_mkt\"]\n",
        "    se_full = model_full.bse[\"excess_mkt\"]\n",
        "\n",
        "    print(f\"\\nFull sample β = {beta_full:.4f} (SE = {se_full:.4f})\")\n",
        "    print(f\"              R² = {model_full.rsquared:.4f}\")\n",
        "    print(f\"              N  = {len(df)}\")\n",
        "\n",
        "    # Regime-specific betas\n",
        "    regime_results = []\n",
        "\n",
        "    for regime in REGIMES:\n",
        "        subset = df[df[\"Regime\"] == regime]\n",
        "        X_reg = sm.add_constant(subset[\"excess_mkt\"])\n",
        "        y_reg = subset[\"excess_nvda\"]\n",
        "        model_reg = sm.OLS(y_reg, X_reg).fit()\n",
        "\n",
        "        beta_reg = model_reg.params[\"excess_mkt\"]\n",
        "        se_reg = model_reg.bse[\"excess_mkt\"]\n",
        "\n",
        "        # Test: β_regime = β_full\n",
        "        diff = beta_reg - beta_full\n",
        "        se_diff = np.sqrt(se_reg**2 + se_full**2)\n",
        "        t_stat = diff / se_diff\n",
        "        df_test = len(subset) - 2\n",
        "        p_value = 2 * (1 - student_t.cdf(np.abs(t_stat), df=df_test))\n",
        "\n",
        "        regime_results.append({\n",
        "            'Regime': regime,\n",
        "            'Beta': beta_reg,\n",
        "            'SE': se_reg,\n",
        "            'R2': model_reg.rsquared,\n",
        "            'N': len(subset),\n",
        "            'Diff_from_full': diff,\n",
        "            't_stat': t_stat,\n",
        "            'p_value': p_value\n",
        "        })\n",
        "\n",
        "    results_df = pd.DataFrame(regime_results)\n",
        "\n",
        "    print(\"\\n\" + \"-\"*70)\n",
        "    print(\"Regime-specific estimates:\")\n",
        "    print(\"-\"*70)\n",
        "    print(results_df.to_string(index=False))\n",
        "\n",
        "    print(\"\\n\" + \"-\"*70)\n",
        "    print(\"Hypothesis tests: H₀: β_regime = β_full\")\n",
        "    print(\"-\"*70)\n",
        "\n",
        "    for _, row in results_df.iterrows():\n",
        "        sig = \"***\" if row['p_value'] < 0.01 else \"**\" if row['p_value'] < 0.05 else \"*\" if row['p_value'] < 0.10 else \"\"\n",
        "        print(f\"{row['Regime']:20s}: t = {row['t_stat']:7.3f}, p = {row['p_value']:.4f} {sig}\")\n",
        "\n",
        "    return results_df, model_full\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# MODEL COMPARISON WITHIN REGIMES\n",
        "# ============================================================\n",
        "\n",
        "def dm_test_pairwise(errors1: np.ndarray, errors2: np.ndarray,\n",
        "                     model1_name: str, model2_name: str, lags: int = 6) -> dict:\n",
        "    \"\"\"\n",
        "    Diebold-Mariano test for equal forecast accuracy.\n",
        "    H₀: E[loss1] = E[loss2]\n",
        "    H₁: E[loss1] ≠ E[loss2] (two-sided)\n",
        "\n",
        "    Positive d = loss1 > loss2 → model2 better\n",
        "    \"\"\"\n",
        "    e1 = np.asarray(errors1, dtype=float)\n",
        "    e2 = np.asarray(errors2, dtype=float)\n",
        "\n",
        "    # Squared errors as loss function\n",
        "    se1 = e1**2\n",
        "    se2 = e2**2\n",
        "\n",
        "    # Loss differential\n",
        "    d = se1 - se2\n",
        "    d = d[~np.isnan(d)]\n",
        "\n",
        "    T = len(d)\n",
        "    if T < 30:\n",
        "        return {\n",
        "            'model1': model1_name,\n",
        "            'model2': model2_name,\n",
        "            'mean_d': np.nan,\n",
        "            'dm_stat': np.nan,\n",
        "            'p_value_2sided': np.nan,\n",
        "            'winner': 'Insufficient data',\n",
        "            'n_obs': T\n",
        "        }\n",
        "\n",
        "    # HAC variance (Newey-West)\n",
        "    d0 = d - d.mean()\n",
        "    gamma = np.sum(d0**2) / T\n",
        "\n",
        "    L = min(lags, T - 1)\n",
        "    for k in range(1, L + 1):\n",
        "        gamma += 2.0 * (1.0 - k / (L + 1.0)) * np.sum(d0[k:] * d0[:-k]) / T\n",
        "\n",
        "    var_mean = gamma / T\n",
        "\n",
        "    if var_mean <= 0:\n",
        "        return {\n",
        "            'model1': model1_name,\n",
        "            'model2': model2_name,\n",
        "            'mean_d': float(d.mean()),\n",
        "            'dm_stat': np.nan,\n",
        "            'p_value_2sided': np.nan,\n",
        "            'winner': 'Variance error',\n",
        "            'n_obs': T\n",
        "        }\n",
        "\n",
        "    dm_stat = d.mean() / np.sqrt(var_mean)\n",
        "    p_value = 2 * (1 - student_t.cdf(np.abs(dm_stat), df=T-1))\n",
        "\n",
        "    # Determine winner\n",
        "    if p_value < 0.05:\n",
        "        if d.mean() > 0:\n",
        "            winner = f\"{model2_name} (p={p_value:.4f})\"\n",
        "        else:\n",
        "            winner = f\"{model1_name} (p={p_value:.4f})\"\n",
        "    else:\n",
        "        winner = \"Tie (p≥0.05)\"\n",
        "\n",
        "    return {\n",
        "        'model1': model1_name,\n",
        "        'model2': model2_name,\n",
        "        'mean_d': float(d.mean()),\n",
        "        'dm_stat': float(dm_stat),\n",
        "        'p_value_2sided': float(p_value),\n",
        "        'winner': winner,\n",
        "        'n_obs': T\n",
        "    }\n",
        "\n",
        "\n",
        "def compare_models_all_regimes(df: pd.DataFrame, predictions: dict):\n",
        "    \"\"\"\n",
        "    Compare all model pairs using DM tests across all regimes.\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"DIEBOLD-MARIANO PAIRWISE COMPARISONS (All Regimes)\")\n",
        "    print('='*70)\n",
        "\n",
        "    y = df[\"excess_nvda\"].values\n",
        "    model_names = list(predictions.keys())\n",
        "\n",
        "    # Compute errors\n",
        "    errors = {name: y - pred for name, pred in predictions.items()}\n",
        "\n",
        "    # All pairwise comparisons\n",
        "    comparisons = []\n",
        "\n",
        "    for regime in [\"Full Sample\"] + REGIMES:\n",
        "        if regime == \"Full Sample\":\n",
        "            mask = np.ones(len(df), dtype=bool)\n",
        "        else:\n",
        "            mask = (df[\"Regime\"] == regime).values\n",
        "\n",
        "        print(f\"\\n{regime}:\")\n",
        "        print(\"-\" * 70)\n",
        "\n",
        "        for i, m1 in enumerate(model_names):\n",
        "            for m2 in model_names[i+1:]:\n",
        "                result = dm_test_pairwise(\n",
        "                    errors[m1][mask],\n",
        "                    errors[m2][mask],\n",
        "                    m1, m2, lags=6\n",
        "                )\n",
        "                result['Regime'] = regime\n",
        "                comparisons.append(result)\n",
        "\n",
        "                print(f\"  {m1} vs {m2:3s}: DM={result['dm_stat']:7.3f}, \"\n",
        "                      f\"p={result['p_value_2sided']:.4f} → {result['winner']}\")\n",
        "\n",
        "    return pd.DataFrame(comparisons)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# MAIN ANALYSIS\n",
        "# ============================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # 1. Load data\n",
        "    df = build_dataset()\n",
        "\n",
        "    # 2. Define regimes\n",
        "    df, q25, q75 = add_regimes(df)\n",
        "\n",
        "    # 3. NON-LINEARITY TESTS (Full sample + each regime)\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"STEP 1: NON-LINEARITY TESTING\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    nonlin_results = {}\n",
        "\n",
        "    # Full sample\n",
        "    X_full = df[\"excess_mkt\"].values\n",
        "    y_full = df[\"excess_nvda\"].values\n",
        "    nonlin_results['Full'], capm_full = test_nonlinearity_comprehensive(\n",
        "        X_full, y_full, \"Full Sample\"\n",
        "    )\n",
        "\n",
        "    # Each regime\n",
        "    for regime in REGIMES:\n",
        "        subset = df[df[\"Regime\"] == regime]\n",
        "        X_reg = subset[\"excess_mkt\"].values\n",
        "        y_reg = subset[\"excess_nvda\"].values\n",
        "        nonlin_results[regime], _ = test_nonlinearity_comprehensive(\n",
        "            X_reg, y_reg, regime\n",
        "        )\n",
        "\n",
        "    # 4. REGIME-SPECIFIC BETA ESTIMATION\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"STEP 2: REGIME-SPECIFIC BETA ANALYSIS\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    regime_betas, _ = estimate_regime_betas(df)\n",
        "\n",
        "    # 5. TRAIN ALL ML MODELS (on full sample)\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"STEP 3: TRAIN ALL MODELS (Full Sample)\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    X_train = df[[\"excess_mkt\"]].values\n",
        "    y_train = df[\"excess_nvda\"].values\n",
        "\n",
        "    models = {}\n",
        "\n",
        "    # OLS\n",
        "    print(\"\\nTraining OLS...\")\n",
        "    models[\"OLS\"] = LinearRegression().fit(X_train, y_train)\n",
        "\n",
        "    # Random Forest\n",
        "    print(\"Training Random Forest...\")\n",
        "    models[\"RF\"] = RandomForestRegressor(\n",
        "        n_estimators=300,\n",
        "        max_depth=None,\n",
        "        min_samples_leaf=2,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    ).fit(X_train, y_train)\n",
        "\n",
        "    # SVR\n",
        "    print(\"Training SVR...\")\n",
        "    models[\"SVR\"] = Pipeline([\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"svm\", SVR(kernel=\"rbf\", C=10, gamma=\"scale\"))\n",
        "    ]).fit(X_train, y_train)\n",
        "\n",
        "    # Gradient Boosting\n",
        "    print(\"Training GBR...\")\n",
        "    models[\"GBR\"] = GradientBoostingRegressor(\n",
        "        n_estimators=500,\n",
        "        learning_rate=0.05,\n",
        "        max_depth=3,\n",
        "        loss=\"huber\",\n",
        "        random_state=42\n",
        "    ).fit(X_train, y_train)\n",
        "\n",
        "    print(\"\\n✓ All models trained on full sample\")\n",
        "\n",
        "    # 6. PREDICTIONS\n",
        "    predictions = {name: model.predict(X_train) for name, model in models.items()}\n",
        "\n",
        "    # 7. MODEL COMPARISON\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"STEP 4: MODEL COMPARISON (DM Tests)\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    dm_results = compare_models_all_regimes(df, predictions)\n",
        "\n",
        "    # 8. SUMMARY TABLE\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"FINAL SUMMARY: R² BY MODEL AND REGIME\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    summary_rows = []\n",
        "\n",
        "    for regime in [\"Full Sample\"] + REGIMES:\n",
        "        if regime == \"Full Sample\":\n",
        "            mask = np.ones(len(df), dtype=bool)\n",
        "            y_subset = y_full\n",
        "        else:\n",
        "            mask = (df[\"Regime\"] == regime).values\n",
        "            y_subset = y_full[mask]\n",
        "\n",
        "        row = {'Regime': regime}\n",
        "        for name, pred in predictions.items():\n",
        "            pred_subset = pred[mask]\n",
        "            r2 = r2_score(y_subset, pred_subset)\n",
        "            row[name] = r2\n",
        "\n",
        "        summary_rows.append(row)\n",
        "\n",
        "    summary_df = pd.DataFrame(summary_rows)\n",
        "    print(\"\\n\" + summary_df.to_string(index=False))\n",
        "\n",
        "    # 9. INTERPRETATION\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"INTERPRETATION & RECOMMENDATIONS\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Count non-linearity rejections\n",
        "    full_rejects = sum([\n",
        "        nonlin_results['Full'].get('RESET_p2', 1) < 0.05,\n",
        "        nonlin_results['Full'].get('RESET_p3', 1) < 0.05,\n",
        "        nonlin_results['Full'].get('HC_p', 1) < 0.05,\n",
        "        nonlin_results['Full'].get('Rainbow_p', 1) < 0.05,\n",
        "        nonlin_results['Full'].get('White_p', 1) < 0.05\n",
        "    ])\n",
        "\n",
        "    print(f\"\\n1. NON-LINEARITY EVIDENCE:\")\n",
        "    print(f\"   Full sample: {full_rejects}/5 tests reject linearity at 5%\")\n",
        "\n",
        "    for regime in REGIMES:\n",
        "        regime_rejects = sum([\n",
        "            nonlin_results[regime].get('RESET_p2', 1) < 0.05,\n",
        "            nonlin_results[regime].get('RESET_p3', 1) < 0.05,\n",
        "            nonlin_results[regime].get('HC_p', 1) < 0.05,\n",
        "            nonlin_results[regime].get('Rainbow_p', 1) < 0.05,\n",
        "            nonlin_results[regime].get('White_p', 1) < 0.05\n",
        "        ])\n",
        "        print(f\"   {regime:20s}: {regime_rejects}/5 tests reject linearity\")\n",
        "\n",
        "    print(f\"\\n2. BETA STABILITY:\")\n",
        "    sig_regimes = regime_betas[regime_betas['p_value'] < 0.05]\n",
        "    if len(sig_regimes) > 0:\n",
        "        print(f\"   {len(sig_regimes)}/3 regimes have β significantly different from full sample\")\n",
        "        print(\"   → Evidence of regime-dependent beta\")\n",
        "    else:\n",
        "        print(\"   All regime betas consistent with full-sample β\")\n",
        "        print(\"   → Beta appears stable across market conditions\")\n",
        "\n",
        "    print(f\"\\n3. MODEL PERFORMANCE:\")\n",
        "    best_full = summary_df.iloc[0, 1:].idxmax()\n",
        "    best_r2 = summary_df.iloc[0, 1:].max()\n",
        "    print(f\"   Best model (full sample): {best_full} (R²={best_r2:.4f})\")\n",
        "\n",
        "    # Check if ML significantly better via DM tests\n",
        "    dm_full = dm_results[dm_results['Regime'] == 'Full Sample']\n",
        "    ml_wins = dm_full[\n",
        "        (dm_full['model2'].isin(['RF', 'GBR', 'SVR'])) &\n",
        "        (dm_full['p_value_2sided'] < 0.05) &\n",
        "        (dm_full['mean_d'] > 0)  # OLS worse\n",
        "    ]\n",
        "\n",
        "    if len(ml_wins) > 0:\n",
        "        print(f\"   ML statistically superior to OLS in {len(ml_wins)} comparisons\")\n",
        "    else:\n",
        "        print(\"   No ML model statistically superior to OLS (DM tests, p<0.05)\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"ANALYSIS COMPLETE\")\n",
        "    print(\"=\"*70)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtnC8oKKBKdY",
        "outputId": "fb64ab3e-d25b-4155-b7bc-b76a97a5e271"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "Building dataset: 2015-01-01 → 2025-09-30\n",
            "======================================================================\n",
            "Dataset ready: 2700 observations\n",
            "Date range: 2015-01-05 to 2025-09-29\n",
            "\n",
            "Regime thresholds: q25 = -0.4000%, q75 = 0.6025%\n",
            "\n",
            "Regime distribution:\n",
            "Regime\n",
            "Normal (Mid 50%)    1351\n",
            "Bull (Top 25%)       675\n",
            "Bear (Bot 25%)       674\n",
            "\n",
            "======================================================================\n",
            "STEP 1: NON-LINEARITY TESTING\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "NON-LINEARITY TEST BATTERY: Full Sample\n",
            "======================================================================\n",
            "\n",
            "1. RAMSEY RESET TEST (Omitted Non-linear Terms)\n",
            "----------------------------------------------------------------------\n",
            "   Power=2: F = 1.0592, p = 0.3035\n",
            "   Power=3: F = 4.8993, p = 0.0075\n",
            "   ✓ Cannot reject linearity (p≥0.05)\n",
            "\n",
            "2. HARVEY-COLLIER TEST (Recursive Residuals)\n",
            "----------------------------------------------------------------------\n",
            "   t-statistic = -0.1409, p = 0.8879\n",
            "   ✓ Cannot reject linearity (p≥0.05)\n",
            "\n",
            "3. RAINBOW TEST (Parameter Constancy)\n",
            "----------------------------------------------------------------------\n",
            "   F-statistic = 1.3087, p = 0.0000\n",
            "   ⚠️  REJECT linearity (p<0.05): Parameters not constant\n",
            "\n",
            "4. WHITE TEST (Non-linear Heteroskedasticity)\n",
            "----------------------------------------------------------------------\n",
            "   LM statistic = 0.6499, p = 0.7226\n",
            "   F statistic  = 0.3246, p = 0.7228\n",
            "   ✓ Cannot reject homoskedasticity (p≥0.05)\n",
            "\n",
            "======================================================================\n",
            "SUMMARY INTERPRETATION:\n",
            "======================================================================\n",
            "Tests rejecting linearity at 5% level: 2/5\n",
            "\n",
            "🟡 MODERATE EVIDENCE of non-linearity\n",
            "   → ML models may capture additional patterns\n",
            "\n",
            "======================================================================\n",
            "NON-LINEARITY TEST BATTERY: Bear (Bot 25%)\n",
            "======================================================================\n",
            "\n",
            "1. RAMSEY RESET TEST (Omitted Non-linear Terms)\n",
            "----------------------------------------------------------------------\n",
            "   Power=2: F = 5.8762, p = 0.0156\n",
            "   Power=3: F = 7.0029, p = 0.0010\n",
            "   ⚠️  REJECT linearity (p<0.05): Non-linear terms significant\n",
            "\n",
            "2. HARVEY-COLLIER TEST (Recursive Residuals)\n",
            "----------------------------------------------------------------------\n",
            "   t-statistic = -3.8281, p = 0.0001\n",
            "   ⚠️  REJECT linearity (p<0.05): Specification error detected\n",
            "\n",
            "3. RAINBOW TEST (Parameter Constancy)\n",
            "----------------------------------------------------------------------\n",
            "   F-statistic = 1.3048, p = 0.0075\n",
            "   ⚠️  REJECT linearity (p<0.05): Parameters not constant\n",
            "\n",
            "4. WHITE TEST (Non-linear Heteroskedasticity)\n",
            "----------------------------------------------------------------------\n",
            "   LM statistic = 0.0014, p = 0.9993\n",
            "   F statistic  = 0.0007, p = 0.9993\n",
            "   ✓ Cannot reject homoskedasticity (p≥0.05)\n",
            "\n",
            "======================================================================\n",
            "SUMMARY INTERPRETATION:\n",
            "======================================================================\n",
            "Tests rejecting linearity at 5% level: 4/5\n",
            "\n",
            "🔴 STRONG EVIDENCE of non-linearity\n",
            "   → Machine learning models theoretically justified\n",
            "\n",
            "======================================================================\n",
            "NON-LINEARITY TEST BATTERY: Normal (Mid 50%)\n",
            "======================================================================\n",
            "\n",
            "1. RAMSEY RESET TEST (Omitted Non-linear Terms)\n",
            "----------------------------------------------------------------------\n",
            "   Power=2: F = 0.0326, p = 0.8567\n",
            "   Power=3: F = 0.3708, p = 0.6902\n",
            "   ✓ Cannot reject linearity (p≥0.05)\n",
            "\n",
            "2. HARVEY-COLLIER TEST (Recursive Residuals)\n",
            "----------------------------------------------------------------------\n",
            "   t-statistic = -0.0313, p = 0.9751\n",
            "   ✓ Cannot reject linearity (p≥0.05)\n",
            "\n",
            "3. RAINBOW TEST (Parameter Constancy)\n",
            "----------------------------------------------------------------------\n",
            "   F-statistic = 1.0731, p = 0.1800\n",
            "   ✓ Cannot reject parameter constancy (p≥0.05)\n",
            "\n",
            "4. WHITE TEST (Non-linear Heteroskedasticity)\n",
            "----------------------------------------------------------------------\n",
            "   LM statistic = 2.2534, p = 0.3241\n",
            "   F statistic  = 1.1261, p = 0.3246\n",
            "   ✓ Cannot reject homoskedasticity (p≥0.05)\n",
            "\n",
            "======================================================================\n",
            "SUMMARY INTERPRETATION:\n",
            "======================================================================\n",
            "Tests rejecting linearity at 5% level: 0/5\n",
            "\n",
            "🟢 WEAK/NO EVIDENCE of non-linearity\n",
            "   → Linear CAPM appears adequate\n",
            "\n",
            "======================================================================\n",
            "NON-LINEARITY TEST BATTERY: Bull (Top 25%)\n",
            "======================================================================\n",
            "\n",
            "1. RAMSEY RESET TEST (Omitted Non-linear Terms)\n",
            "----------------------------------------------------------------------\n",
            "   Power=2: F = 0.0012, p = 0.9721\n",
            "   Power=3: F = 1.4830, p = 0.2277\n",
            "   ✓ Cannot reject linearity (p≥0.05)\n",
            "\n",
            "2. HARVEY-COLLIER TEST (Recursive Residuals)\n",
            "----------------------------------------------------------------------\n",
            "   t-statistic = 3.9992, p = 0.0001\n",
            "   ⚠️  REJECT linearity (p<0.05): Specification error detected\n",
            "\n",
            "3. RAINBOW TEST (Parameter Constancy)\n",
            "----------------------------------------------------------------------\n",
            "   F-statistic = 1.2054, p = 0.0436\n",
            "   ⚠️  REJECT linearity (p<0.05): Parameters not constant\n",
            "\n",
            "4. WHITE TEST (Non-linear Heteroskedasticity)\n",
            "----------------------------------------------------------------------\n",
            "   LM statistic = 0.1842, p = 0.9120\n",
            "   F statistic  = 0.0917, p = 0.9124\n",
            "   ✓ Cannot reject homoskedasticity (p≥0.05)\n",
            "\n",
            "======================================================================\n",
            "SUMMARY INTERPRETATION:\n",
            "======================================================================\n",
            "Tests rejecting linearity at 5% level: 2/5\n",
            "\n",
            "🟡 MODERATE EVIDENCE of non-linearity\n",
            "   → ML models may capture additional patterns\n",
            "\n",
            "======================================================================\n",
            "STEP 2: REGIME-SPECIFIC BETA ANALYSIS\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "REGIME-SPECIFIC BETA ESTIMATION\n",
            "======================================================================\n",
            "\n",
            "Full sample β = 1.7383 (SE = 0.0385)\n",
            "              R² = 0.4302\n",
            "              N  = 2700\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Regime-specific estimates:\n",
            "----------------------------------------------------------------------\n",
            "          Regime     Beta       SE       R2    N  Diff_from_full    t_stat  p_value\n",
            "  Bear (Bot 25%) 1.557969 0.088120 0.317477  674       -0.180367 -1.875485 0.061159\n",
            "Normal (Mid 50%) 1.680900 0.228432 0.038589 1351       -0.057435 -0.247933 0.804224\n",
            "  Bull (Top 25%) 1.562964 0.102850 0.255478  675       -0.175371 -1.596814 0.110777\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Hypothesis tests: H₀: β_regime = β_full\n",
            "----------------------------------------------------------------------\n",
            "Bear (Bot 25%)      : t =  -1.875, p = 0.0612 *\n",
            "Normal (Mid 50%)    : t =  -0.248, p = 0.8042 \n",
            "Bull (Top 25%)      : t =  -1.597, p = 0.1108 \n",
            "\n",
            "======================================================================\n",
            "STEP 3: TRAIN ALL MODELS (Full Sample)\n",
            "======================================================================\n",
            "\n",
            "Training OLS...\n",
            "Training Random Forest...\n",
            "Training SVR...\n",
            "Training GBR...\n",
            "\n",
            "✓ All models trained on full sample\n",
            "\n",
            "======================================================================\n",
            "STEP 4: MODEL COMPARISON (DM Tests)\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "DIEBOLD-MARIANO PAIRWISE COMPARISONS (All Regimes)\n",
            "======================================================================\n",
            "\n",
            "Full Sample:\n",
            "----------------------------------------------------------------------\n",
            "  OLS vs RF : DM=  7.736, p=0.0000 → RF (p=0.0000)\n",
            "  OLS vs SVR: DM=  1.403, p=0.1609 → Tie (p≥0.05)\n",
            "  OLS vs GBR: DM=  8.177, p=0.0000 → GBR (p=0.0000)\n",
            "  RF vs SVR: DM= -7.570, p=0.0000 → RF (p=0.0000)\n",
            "  RF vs GBR: DM= -3.183, p=0.0015 → RF (p=0.0015)\n",
            "  SVR vs GBR: DM=  8.147, p=0.0000 → GBR (p=0.0000)\n",
            "\n",
            "Bear (Bot 25%):\n",
            "----------------------------------------------------------------------\n",
            "  OLS vs RF : DM=  6.270, p=0.0000 → RF (p=0.0000)\n",
            "  OLS vs SVR: DM=  1.414, p=0.1578 → Tie (p≥0.05)\n",
            "  OLS vs GBR: DM=  5.992, p=0.0000 → GBR (p=0.0000)\n",
            "  RF vs SVR: DM= -6.055, p=0.0000 → RF (p=0.0000)\n",
            "  RF vs GBR: DM= -1.573, p=0.1162 → Tie (p≥0.05)\n",
            "  SVR vs GBR: DM=  5.475, p=0.0000 → GBR (p=0.0000)\n",
            "\n",
            "Normal (Mid 50%):\n",
            "----------------------------------------------------------------------\n",
            "  OLS vs RF : DM=  3.450, p=0.0006 → RF (p=0.0006)\n",
            "  OLS vs SVR: DM= -0.994, p=0.3203 → Tie (p≥0.05)\n",
            "  OLS vs GBR: DM=  4.972, p=0.0000 → GBR (p=0.0000)\n",
            "  RF vs SVR: DM= -3.388, p=0.0007 → RF (p=0.0007)\n",
            "  RF vs GBR: DM= -2.299, p=0.0217 → RF (p=0.0217)\n",
            "  SVR vs GBR: DM=  5.420, p=0.0000 → GBR (p=0.0000)\n",
            "\n",
            "Bull (Top 25%):\n",
            "----------------------------------------------------------------------\n",
            "  OLS vs RF : DM=  4.602, p=0.0000 → RF (p=0.0000)\n",
            "  OLS vs SVR: DM=  1.054, p=0.2922 → Tie (p≥0.05)\n",
            "  OLS vs GBR: DM=  5.102, p=0.0000 → GBR (p=0.0000)\n",
            "  RF vs SVR: DM= -4.479, p=0.0000 → RF (p=0.0000)\n",
            "  RF vs GBR: DM= -1.833, p=0.0673 → Tie (p≥0.05)\n",
            "  SVR vs GBR: DM=  5.289, p=0.0000 → GBR (p=0.0000)\n",
            "\n",
            "======================================================================\n",
            "FINAL SUMMARY: R² BY MODEL AND REGIME\n",
            "======================================================================\n",
            "\n",
            "          Regime      OLS       RF      SVR      GBR\n",
            "     Full Sample 0.430171 0.526990 0.433936 0.497093\n",
            "  Bear (Bot 25%) 0.309769 0.513430 0.322283 0.478854\n",
            "Normal (Mid 50%) 0.038266 0.109193 0.035877 0.062125\n",
            "  Bull (Top 25%) 0.251133 0.411524 0.259446 0.365786\n",
            "\n",
            "======================================================================\n",
            "INTERPRETATION & RECOMMENDATIONS\n",
            "======================================================================\n",
            "\n",
            "1. NON-LINEARITY EVIDENCE:\n",
            "   Full sample: 2/5 tests reject linearity at 5%\n",
            "   Bear (Bot 25%)      : 4/5 tests reject linearity\n",
            "   Normal (Mid 50%)    : 0/5 tests reject linearity\n",
            "   Bull (Top 25%)      : 2/5 tests reject linearity\n",
            "\n",
            "2. BETA STABILITY:\n",
            "   All regime betas consistent with full-sample β\n",
            "   → Beta appears stable across market conditions\n",
            "\n",
            "3. MODEL PERFORMANCE:\n",
            "   Best model (full sample): RF (R²=0.5270)\n",
            "   ML statistically superior to OLS in 3 comparisons\n",
            "\n",
            "======================================================================\n",
            "ANALYSIS COMPLETE\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gg2NkHvnBg5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now I'll modify the code to add proper time-series cross-validation. The key idea: use TimeSeriesSplit so each fold trains only on past data and predicts on future data — then collect all out-of-sample predictions and use those for the DM tests."
      ],
      "metadata": {
        "id": "AXZvnhw7KihB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# DESIGN A — WITH TIME-SERIES CROSS-VALIDATION\n",
        "# Out-of-sample errors used for Diebold-Mariano tests\n",
        "# ============================================================\n",
        "\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "\n",
        "import yfinance as yf\n",
        "from pandas_datareader import data as pdr\n",
        "\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.stats.diagnostic import (\n",
        "    het_breuschpagan,\n",
        "    linear_reset,\n",
        "    linear_harvey_collier,\n",
        "    linear_rainbow,\n",
        "    het_white\n",
        ")\n",
        "from statsmodels.stats.stattools import durbin_watson\n",
        "from scipy.stats import jarque_bera, kruskal, t as student_t\n",
        "\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "pd.set_option(\"display.width\", 120)\n",
        "np.random.seed(42)\n",
        "\n",
        "START   = \"2015-01-01\"\n",
        "END     = \"2025-09-30\"\n",
        "REGIMES = [\"Bear (Bot 25%)\", \"Normal (Mid 50%)\", \"Bull (Top 25%)\"]\n",
        "N_SPLITS = 10          # number of CV folds\n",
        "MIN_TRAIN = 252        # minimum 1 year of training data before first prediction\n",
        "\n",
        "# ============================================================\n",
        "# HELPER FUNCTIONS\n",
        "# ============================================================\n",
        "\n",
        "def _flatten_yf(px: pd.DataFrame) -> pd.DataFrame:\n",
        "    px = px.copy()\n",
        "    if isinstance(px.columns, pd.MultiIndex):\n",
        "        px.columns = [c[0] if isinstance(c, tuple) else c for c in px.columns]\n",
        "    px.columns.name = None\n",
        "    return px\n",
        "\n",
        "\n",
        "def _yf_adj_close(ticker: str, start: str, end: str) -> pd.DataFrame:\n",
        "    try:\n",
        "        df = yf.download(ticker, start=start, end=end, progress=False, auto_adjust=True)\n",
        "        df = _flatten_yf(df)\n",
        "        for col in [\"Adj Close\", \"Close\", ticker]:\n",
        "            if col in df.columns:\n",
        "                return df[[col]].rename(columns={col: ticker})\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching {ticker}: {e}\")\n",
        "    return pd.DataFrame()\n",
        "\n",
        "\n",
        "def build_dataset(start: str = START, end: str = END) -> pd.DataFrame:\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Building dataset: {start} → {end}\")\n",
        "    print('='*70)\n",
        "\n",
        "    nvda = _yf_adj_close(\"NVDA\", start, end).rename(columns={\"NVDA\": \"nvda_px\"})\n",
        "    ff_raw = pdr.DataReader(\n",
        "        \"F-F_Research_Data_Factors_daily\", \"famafrench\", start=start, end=end\n",
        "    )[0]\n",
        "    ff = ff_raw[[\"Mkt-RF\", \"RF\"]].rename(columns={\"Mkt-RF\": \"excess_mkt\", \"RF\": \"rf\"})\n",
        "\n",
        "    df = nvda.join(ff, how=\"inner\").dropna()\n",
        "    df[\"nvda_ret\"]    = df[\"nvda_px\"].pct_change() * 100.0\n",
        "    df[\"excess_nvda\"] = df[\"nvda_ret\"] - df[\"rf\"]\n",
        "    df = df.dropna()\n",
        "\n",
        "    print(f\"Dataset ready: {df.shape[0]} observations\")\n",
        "    print(f\"Date range: {df.index[0].date()} to {df.index[-1].date()}\")\n",
        "    return df\n",
        "\n",
        "\n",
        "def add_regimes(df: pd.DataFrame) -> tuple:\n",
        "    df = df.copy()\n",
        "    q25 = df[\"excess_mkt\"].quantile(0.25)\n",
        "    q75 = df[\"excess_mkt\"].quantile(0.75)\n",
        "\n",
        "    df[\"Regime\"] = \"Normal (Mid 50%)\"\n",
        "    df.loc[df[\"excess_mkt\"] < q25, \"Regime\"] = \"Bear (Bot 25%)\"\n",
        "    df.loc[df[\"excess_mkt\"] > q75, \"Regime\"] = \"Bull (Top 25%)\"\n",
        "\n",
        "    print(f\"\\nRegime thresholds: q25 = {q25:.4f}%, q75 = {q75:.4f}%\")\n",
        "    print(\"\\nRegime distribution:\")\n",
        "    print(df[\"Regime\"].value_counts().to_string())\n",
        "\n",
        "    return df, q25, q75\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# NON-LINEARITY TESTING\n",
        "# ============================================================\n",
        "\n",
        "def test_nonlinearity_comprehensive(X, y, label=\"Full Sample\"):\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"NON-LINEARITY TEST BATTERY: {label}\")\n",
        "    print('='*70)\n",
        "\n",
        "    X_const = sm.add_constant(X)\n",
        "    model = sm.OLS(y, X_const).fit()\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # 1. Ramsey RESET\n",
        "    print(\"\\n1. RAMSEY RESET TEST (Omitted Non-linear Terms)\")\n",
        "    print(\"-\" * 70)\n",
        "    try:\n",
        "        reset_2 = linear_reset(model, power=2, use_f=True)\n",
        "        reset_3 = linear_reset(model, power=3, use_f=True)\n",
        "        print(f\"   Power=2: F = {reset_2.fvalue:.4f}, p = {reset_2.pvalue:.4f}\")\n",
        "        print(f\"   Power=3: F = {reset_3.fvalue:.4f}, p = {reset_3.pvalue:.4f}\")\n",
        "        results['RESET_p2'] = reset_2.pvalue\n",
        "        results['RESET_p3'] = reset_3.pvalue\n",
        "        # Reject if EITHER power rejects\n",
        "        reset_reject = (reset_2.pvalue < 0.05) or (reset_3.pvalue < 0.05)\n",
        "        if reset_reject:\n",
        "            print(\"   ⚠️  REJECT linearity (p<0.05): Non-linear terms significant\")\n",
        "        else:\n",
        "            print(\"   ✓ Cannot reject linearity (p≥0.05)\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ⚠️  RESET test failed: {e}\")\n",
        "        results['RESET_p2'] = np.nan\n",
        "        results['RESET_p3'] = np.nan\n",
        "\n",
        "    # 2. Harvey-Collier\n",
        "    print(\"\\n2. HARVEY-COLLIER TEST (Recursive Residuals)\")\n",
        "    print(\"-\" * 70)\n",
        "    try:\n",
        "        hc_stat = linear_harvey_collier(model)\n",
        "        print(f\"   t-statistic = {hc_stat[0]:.4f}, p = {hc_stat[1]:.4f}\")\n",
        "        results['HC_p'] = hc_stat[1]\n",
        "        if hc_stat[1] < 0.05:\n",
        "            print(\"   ⚠️  REJECT linearity (p<0.05): Specification error detected\")\n",
        "        else:\n",
        "            print(\"   ✓ Cannot reject linearity (p≥0.05)\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ⚠️  Harvey-Collier test failed: {e}\")\n",
        "        results['HC_p'] = np.nan\n",
        "\n",
        "    # 3. Rainbow\n",
        "    print(\"\\n3. RAINBOW TEST (Parameter Constancy)\")\n",
        "    print(\"-\" * 70)\n",
        "    try:\n",
        "        rainbow_stat = linear_rainbow(model, frac=0.5)\n",
        "        print(f\"   F-statistic = {rainbow_stat[0]:.4f}, p = {rainbow_stat[1]:.4f}\")\n",
        "        results['Rainbow_p'] = rainbow_stat[1]\n",
        "        if rainbow_stat[1] < 0.05:\n",
        "            print(\"   ⚠️  REJECT linearity (p<0.05): Parameters not constant\")\n",
        "        else:\n",
        "            print(\"   ✓ Cannot reject parameter constancy (p≥0.05)\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ⚠️  Rainbow test failed: {e}\")\n",
        "        results['Rainbow_p'] = np.nan\n",
        "\n",
        "    # 4. White\n",
        "    print(\"\\n4. WHITE TEST (Non-linear Heteroskedasticity)\")\n",
        "    print(\"-\" * 70)\n",
        "    try:\n",
        "        white_stat = het_white(model.resid, model.model.exog)\n",
        "        print(f\"   LM statistic = {white_stat[0]:.4f}, p = {white_stat[1]:.4f}\")\n",
        "        print(f\"   F statistic  = {white_stat[2]:.4f}, p = {white_stat[3]:.4f}\")\n",
        "        results['White_p'] = white_stat[1]\n",
        "        if white_stat[1] < 0.05:\n",
        "            print(\"   ⚠️  REJECT homoskedasticity (p<0.05): Non-linear patterns in variance\")\n",
        "        else:\n",
        "            print(\"   ✓ Cannot reject homoskedasticity (p≥0.05)\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ⚠️  White test failed: {e}\")\n",
        "        results['White_p'] = np.nan\n",
        "\n",
        "    # Summary — count RESET as one test (reject if either power rejects)\n",
        "    reset_reject = (\n",
        "        (results.get('RESET_p2', 1) < 0.05) or\n",
        "        (results.get('RESET_p3', 1) < 0.05)\n",
        "    )\n",
        "    reject_count = sum([\n",
        "        reset_reject,\n",
        "        results.get('HC_p', 1) < 0.05,\n",
        "        results.get('Rainbow_p', 1) < 0.05,\n",
        "        results.get('White_p', 1) < 0.05\n",
        "    ])\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"SUMMARY INTERPRETATION:\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Tests rejecting linearity at 5% level: {reject_count}/4\")\n",
        "\n",
        "    if reject_count >= 3:\n",
        "        print(\"\\n🔴 STRONG EVIDENCE of non-linearity\")\n",
        "        print(\"   → Machine learning models theoretically justified\")\n",
        "    elif reject_count >= 2:\n",
        "        print(\"\\n🟡 MODERATE EVIDENCE of non-linearity\")\n",
        "        print(\"   → ML models may capture additional patterns\")\n",
        "    else:\n",
        "        print(\"\\n🟢 WEAK/NO EVIDENCE of non-linearity\")\n",
        "        print(\"   → Linear CAPM appears adequate\")\n",
        "\n",
        "    return results, model, reject_count\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# REGIME-SPECIFIC BETA ESTIMATION\n",
        "# ============================================================\n",
        "\n",
        "def estimate_regime_betas(df: pd.DataFrame):\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"REGIME-SPECIFIC BETA ESTIMATION\")\n",
        "    print('='*70)\n",
        "\n",
        "    X_full = sm.add_constant(df[\"excess_mkt\"])\n",
        "    y_full = df[\"excess_nvda\"]\n",
        "    model_full = sm.OLS(y_full, X_full).fit()\n",
        "\n",
        "    beta_full = model_full.params[\"excess_mkt\"]\n",
        "    se_full   = model_full.bse[\"excess_mkt\"]\n",
        "\n",
        "    print(f\"\\nFull sample β = {beta_full:.4f} (SE = {se_full:.4f})\")\n",
        "    print(f\"              R² = {model_full.rsquared:.4f}\")\n",
        "    print(f\"              N  = {len(df)}\")\n",
        "\n",
        "    regime_results = []\n",
        "\n",
        "    for regime in REGIMES:\n",
        "        subset = df[df[\"Regime\"] == regime]\n",
        "        X_reg  = sm.add_constant(subset[\"excess_mkt\"])\n",
        "        y_reg  = subset[\"excess_nvda\"]\n",
        "        model_reg = sm.OLS(y_reg, X_reg).fit()\n",
        "\n",
        "        beta_reg = model_reg.params[\"excess_mkt\"]\n",
        "        se_reg   = model_reg.bse[\"excess_mkt\"]\n",
        "\n",
        "        diff    = beta_reg - beta_full\n",
        "        se_diff = np.sqrt(se_reg**2 + se_full**2)\n",
        "        t_stat  = diff / se_diff\n",
        "        df_test = len(subset) - 2\n",
        "        p_value = 2 * (1 - student_t.cdf(np.abs(t_stat), df=df_test))\n",
        "\n",
        "        regime_results.append({\n",
        "            'Regime': regime,\n",
        "            'Beta': beta_reg,\n",
        "            'SE': se_reg,\n",
        "            'R2': model_reg.rsquared,\n",
        "            'N': len(subset),\n",
        "            'Diff_from_full': diff,\n",
        "            't_stat': t_stat,\n",
        "            'p_value': p_value\n",
        "        })\n",
        "\n",
        "    results_df = pd.DataFrame(regime_results)\n",
        "\n",
        "    print(\"\\n\" + \"-\"*70)\n",
        "    print(\"Regime-specific estimates:\")\n",
        "    print(\"-\"*70)\n",
        "    print(results_df.to_string(index=False))\n",
        "\n",
        "    print(\"\\n\" + \"-\"*70)\n",
        "    print(\"Hypothesis tests: H₀: β_regime = β_full\")\n",
        "    print(\"-\"*70)\n",
        "\n",
        "    for _, row in results_df.iterrows():\n",
        "        sig = \"***\" if row['p_value'] < 0.01 else \"**\" if row['p_value'] < 0.05 else \"*\" if row['p_value'] < 0.10 else \"\"\n",
        "        print(f\"{row['Regime']:20s}: t = {row['t_stat']:7.3f}, p = {row['p_value']:.4f} {sig}\")\n",
        "\n",
        "    return results_df, model_full\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# TIME-SERIES CROSS-VALIDATION  ← NEW\n",
        "# ============================================================\n",
        "\n",
        "def make_model_instances():\n",
        "    \"\"\"Fresh model instances for each CV fold.\"\"\"\n",
        "    return {\n",
        "        \"OLS\": LinearRegression(),\n",
        "        \"RF\":  RandomForestRegressor(\n",
        "                   n_estimators=300, max_depth=None,\n",
        "                   min_samples_leaf=2, random_state=42, n_jobs=-1),\n",
        "        \"SVR\": Pipeline([\n",
        "                   (\"scaler\", StandardScaler()),\n",
        "                   (\"svm\", SVR(kernel=\"rbf\", C=10, gamma=\"scale\"))]),\n",
        "        \"GBR\": GradientBoostingRegressor(\n",
        "                   n_estimators=500, learning_rate=0.05,\n",
        "                   max_depth=3, loss=\"huber\", random_state=42)\n",
        "    }\n",
        "\n",
        "\n",
        "def run_tscv(df: pd.DataFrame, n_splits: int = N_SPLITS,\n",
        "             min_train: int = MIN_TRAIN) -> dict:\n",
        "    \"\"\"\n",
        "    Walk-forward time-series cross-validation.\n",
        "\n",
        "    Strategy\n",
        "    --------\n",
        "    * sklearn TimeSeriesSplit with n_splits folds.\n",
        "    * Enforce a minimum training window of min_train observations.\n",
        "    * Each fold: train on [0, t), predict on [t, t+step).\n",
        "    * Collect (index, true_y, predicted_y) for every fold.\n",
        "    * Final dict maps model name → aligned arrays of\n",
        "      (oos_true, oos_pred) ordered by date.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    oos : dict  {model_name: {'true': array, 'pred': array, 'index': DatetimeIndex}}\n",
        "    fold_log : list of dicts with per-fold metadata\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"TIME-SERIES CROSS-VALIDATION  (n_splits={n_splits}, \"\n",
        "          f\"min_train={min_train})\")\n",
        "    print('='*70)\n",
        "\n",
        "    X_all = df[[\"excess_mkt\"]].values\n",
        "    y_all = df[\"excess_nvda\"].values\n",
        "    idx   = df.index\n",
        "\n",
        "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "\n",
        "    # Storage: for each model, list of (date_indices, predictions)\n",
        "    oos_storage = {name: {'true': [], 'pred': [], 'index': []}\n",
        "                   for name in [\"OLS\", \"RF\", \"SVR\", \"GBR\"]}\n",
        "    fold_log = []\n",
        "\n",
        "    for fold_num, (train_idx, test_idx) in enumerate(tscv.split(X_all), 1):\n",
        "\n",
        "        # Enforce minimum training window\n",
        "        if len(train_idx) < min_train:\n",
        "            print(f\"  Fold {fold_num:2d}: SKIPPED — train size {len(train_idx)} < {min_train}\")\n",
        "            continue\n",
        "\n",
        "        X_tr, y_tr = X_all[train_idx], y_all[train_idx]\n",
        "        X_te, y_te = X_all[test_idx],  y_all[test_idx]\n",
        "\n",
        "        train_start = idx[train_idx[0]].date()\n",
        "        train_end   = idx[train_idx[-1]].date()\n",
        "        test_start  = idx[test_idx[0]].date()\n",
        "        test_end    = idx[test_idx[-1]].date()\n",
        "\n",
        "        print(f\"  Fold {fold_num:2d}: train [{train_start} → {train_end}] \"\n",
        "              f\"({len(train_idx):4d} obs)  |  \"\n",
        "              f\"test [{test_start} → {test_end}] ({len(test_idx):3d} obs)\")\n",
        "\n",
        "        models_fold = make_model_instances()\n",
        "        fold_metrics = {'fold': fold_num, 'n_train': len(train_idx),\n",
        "                        'n_test': len(test_idx)}\n",
        "\n",
        "        for name, model in models_fold.items():\n",
        "            model.fit(X_tr, y_tr)\n",
        "            pred = model.predict(X_te)\n",
        "\n",
        "            oos_storage[name]['true'].extend(y_te.tolist())\n",
        "            oos_storage[name]['pred'].extend(pred.tolist())\n",
        "            oos_storage[name]['index'].extend(idx[test_idx].tolist())\n",
        "\n",
        "            fold_metrics[f'{name}_r2']  = r2_score(y_te, pred)\n",
        "            fold_metrics[f'{name}_mae'] = mean_absolute_error(y_te, pred)\n",
        "\n",
        "        fold_log.append(fold_metrics)\n",
        "\n",
        "    # Convert lists to arrays (already time-ordered by construction)\n",
        "    oos = {}\n",
        "    for name in oos_storage:\n",
        "        oos[name] = {\n",
        "            'true':  np.array(oos_storage[name]['true']),\n",
        "            'pred':  np.array(oos_storage[name]['pred']),\n",
        "            'index': pd.DatetimeIndex(oos_storage[name]['index'])\n",
        "        }\n",
        "\n",
        "    n_oos = len(oos['OLS']['true'])\n",
        "    print(f\"\\n✓ CV complete — {n_oos} total out-of-sample observations\")\n",
        "\n",
        "    return oos, fold_log\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# DIEBOLD-MARIANO TEST\n",
        "# ============================================================\n",
        "\n",
        "def dm_test_pairwise(errors1: np.ndarray, errors2: np.ndarray,\n",
        "                     model1_name: str, model2_name: str, lags: int = 6) -> dict:\n",
        "    \"\"\"\n",
        "    DM test using HAC variance (Newey-West).\n",
        "    Positive DM stat → model1 has larger losses → model2 better.\n",
        "    \"\"\"\n",
        "    e1 = np.asarray(errors1, dtype=float)\n",
        "    e2 = np.asarray(errors2, dtype=float)\n",
        "\n",
        "    d = e1**2 - e2**2\n",
        "    d = d[~np.isnan(d)]\n",
        "    T = len(d)\n",
        "\n",
        "    if T < 30:\n",
        "        return {'model1': model1_name, 'model2': model2_name,\n",
        "                'mean_d': np.nan, 'dm_stat': np.nan,\n",
        "                'p_value_2sided': np.nan, 'winner': 'Insufficient data', 'n_obs': T}\n",
        "\n",
        "    d0 = d - d.mean()\n",
        "    gamma = np.sum(d0**2) / T\n",
        "    L = min(lags, T - 1)\n",
        "    for k in range(1, L + 1):\n",
        "        gamma += 2.0 * (1.0 - k / (L + 1.0)) * np.sum(d0[k:] * d0[:-k]) / T\n",
        "\n",
        "    var_mean = gamma / T\n",
        "    if var_mean <= 0:\n",
        "        return {'model1': model1_name, 'model2': model2_name,\n",
        "                'mean_d': float(d.mean()), 'dm_stat': np.nan,\n",
        "                'p_value_2sided': np.nan, 'winner': 'Variance error', 'n_obs': T}\n",
        "\n",
        "    dm_stat = d.mean() / np.sqrt(var_mean)\n",
        "    p_value = 2 * (1 - student_t.cdf(np.abs(dm_stat), df=T-1))\n",
        "\n",
        "    if p_value < 0.05:\n",
        "        winner = f\"{model2_name} (p={p_value:.4f})\" if d.mean() > 0 else f\"{model1_name} (p={p_value:.4f})\"\n",
        "    else:\n",
        "        winner = \"Tie (p≥0.05)\"\n",
        "\n",
        "    return {'model1': model1_name, 'model2': model2_name,\n",
        "            'mean_d': float(d.mean()), 'dm_stat': float(dm_stat),\n",
        "            'p_value_2sided': float(p_value), 'winner': winner, 'n_obs': T}\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# OOS COMPARISON ACROSS REGIMES\n",
        "# ============================================================\n",
        "\n",
        "def compare_oos_all_regimes(df: pd.DataFrame, oos: dict) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Run DM tests on genuine out-of-sample errors, sliced by regime.\n",
        "\n",
        "    The regime label for each OOS observation is the regime\n",
        "    that date belongs to in the full dataset.\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"DIEBOLD-MARIANO — OUT-OF-SAMPLE ERRORS (Time-Series CV)\")\n",
        "    print('='*70)\n",
        "    print(\"Note: errors are from held-out test folds — models never saw this data.\\n\")\n",
        "\n",
        "    # Build a regime lookup from the full df\n",
        "    regime_map = df[\"Regime\"]\n",
        "\n",
        "    # Use OLS index as the reference (all models have same OOS dates)\n",
        "    oos_index = oos['OLS']['index']\n",
        "    model_names = list(oos.keys())\n",
        "\n",
        "    # Errors per model\n",
        "    errors = {name: oos[name]['true'] - oos[name]['pred']\n",
        "              for name in model_names}\n",
        "\n",
        "    comparisons = []\n",
        "\n",
        "    for regime in [\"Full OOS\"] + REGIMES:\n",
        "        if regime == \"Full OOS\":\n",
        "            mask = np.ones(len(oos_index), dtype=bool)\n",
        "        else:\n",
        "            mask = np.array([regime_map.get(d, None) == regime\n",
        "                             for d in oos_index])\n",
        "\n",
        "        n_regime = mask.sum()\n",
        "        print(f\"\\n{regime}  (n={n_regime}):\")\n",
        "        print(\"-\" * 70)\n",
        "\n",
        "        for i, m1 in enumerate(model_names):\n",
        "            for m2 in model_names[i+1:]:\n",
        "                result = dm_test_pairwise(\n",
        "                    errors[m1][mask], errors[m2][mask], m1, m2, lags=6\n",
        "                )\n",
        "                result['Regime'] = regime\n",
        "                comparisons.append(result)\n",
        "\n",
        "                dm_str  = f\"{result['dm_stat']:7.3f}\" if not np.isnan(result['dm_stat']) else \"    nan\"\n",
        "                p_str   = f\"{result['p_value_2sided']:.4f}\" if not np.isnan(result['p_value_2sided']) else \"  nan \"\n",
        "                print(f\"  {m1} vs {m2:3s}: DM={dm_str}, p={p_str} → {result['winner']}\")\n",
        "\n",
        "    return pd.DataFrame(comparisons)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# SUMMARY TABLES\n",
        "# ============================================================\n",
        "\n",
        "def oos_summary_table(df: pd.DataFrame, oos: dict) -> pd.DataFrame:\n",
        "    \"\"\"R² and RMSE by model and regime, computed on OOS data only.\"\"\"\n",
        "    regime_map  = df[\"Regime\"]\n",
        "    oos_index   = oos['OLS']['index']\n",
        "    model_names = list(oos.keys())\n",
        "\n",
        "    rows = []\n",
        "    for regime in [\"Full OOS\"] + REGIMES:\n",
        "        if regime == \"Full OOS\":\n",
        "            mask = np.ones(len(oos_index), dtype=bool)\n",
        "        else:\n",
        "            mask = np.array([regime_map.get(d, None) == regime\n",
        "                             for d in oos_index])\n",
        "\n",
        "        y_true = oos['OLS']['true'][mask]\n",
        "        row = {'Regime': regime, 'N': int(mask.sum())}\n",
        "\n",
        "        for name in model_names:\n",
        "            pred = oos[name]['pred'][mask]\n",
        "            row[f'{name}_R2']   = r2_score(y_true, pred)\n",
        "            row[f'{name}_RMSE'] = np.sqrt(mean_squared_error(y_true, pred))\n",
        "\n",
        "        rows.append(row)\n",
        "\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "def per_fold_r2_table(fold_log: list) -> pd.DataFrame:\n",
        "    \"\"\"Show R² per fold per model — useful to spot drift/instability.\"\"\"\n",
        "    return pd.DataFrame(fold_log)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# PLOT  ← NEW: OOS predictions vs actuals + fold R² evolution\n",
        "# ============================================================\n",
        "\n",
        "def plot_results(df: pd.DataFrame, oos: dict, fold_df: pd.DataFrame,\n",
        "                 summary_df: pd.DataFrame):\n",
        "\n",
        "    fig = plt.figure(figsize=(18, 14))\n",
        "    gs  = gridspec.GridSpec(3, 2, figure=fig, hspace=0.45, wspace=0.35)\n",
        "\n",
        "    colors = {'OLS': '#1f77b4', 'RF': '#2ca02c', 'SVR': '#ff7f0e', 'GBR': '#d62728'}\n",
        "    oos_index = oos['OLS']['index']\n",
        "    y_true    = oos['OLS']['true']\n",
        "\n",
        "    # ── Panel 1: OOS predictions vs actuals (full OOS)\n",
        "    ax1 = fig.add_subplot(gs[0, :])\n",
        "    ax1.plot(oos_index, y_true, color='black', lw=0.6, alpha=0.5, label='Actual')\n",
        "    for name in ['OLS', 'RF', 'GBR']:\n",
        "        ax1.plot(oos_index, oos[name]['pred'],\n",
        "                 color=colors[name], lw=0.8, alpha=0.7, label=name)\n",
        "    ax1.set_title('Out-of-Sample Predictions vs Actual NVDA Excess Returns',\n",
        "                  fontsize=12, fontweight='bold')\n",
        "    ax1.set_ylabel('Excess Return (%)')\n",
        "    ax1.legend(loc='upper left', fontsize=9)\n",
        "    ax1.axhline(0, color='gray', lw=0.5, ls='--')\n",
        "\n",
        "    # ── Panel 2: Per-fold R² evolution\n",
        "    ax2 = fig.add_subplot(gs[1, 0])\n",
        "    fold_cols = [c for c in fold_df.columns if c.endswith('_r2')]\n",
        "    for col in fold_cols:\n",
        "        name = col.replace('_r2', '')\n",
        "        ax2.plot(fold_df['fold'], fold_df[col],\n",
        "                 marker='o', color=colors.get(name, 'gray'),\n",
        "                 label=name, lw=1.5)\n",
        "    ax2.set_title('R² per CV Fold (Walk-Forward)', fontsize=11, fontweight='bold')\n",
        "    ax2.set_xlabel('Fold')\n",
        "    ax2.set_ylabel('R²')\n",
        "    ax2.legend(fontsize=9)\n",
        "    ax2.axhline(0, color='gray', lw=0.5, ls='--')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    # ── Panel 3: OOS R² by regime (bar chart)\n",
        "    ax3 = fig.add_subplot(gs[1, 1])\n",
        "    r2_cols   = [c for c in summary_df.columns if c.endswith('_R2')]\n",
        "    model_lbls = [c.replace('_R2', '') for c in r2_cols]\n",
        "    x   = np.arange(len(summary_df))\n",
        "    w   = 0.18\n",
        "    for i, (col, name) in enumerate(zip(r2_cols, model_lbls)):\n",
        "        ax3.bar(x + i*w, summary_df[col], width=w,\n",
        "                color=colors.get(name, 'gray'), label=name, alpha=0.85)\n",
        "    ax3.set_xticks(x + w*1.5)\n",
        "    ax3.set_xticklabels(summary_df['Regime'], rotation=15, ha='right', fontsize=8)\n",
        "    ax3.set_title('OOS R² by Model and Regime', fontsize=11, fontweight='bold')\n",
        "    ax3.set_ylabel('R²')\n",
        "    ax3.legend(fontsize=9)\n",
        "    ax3.axhline(0, color='gray', lw=0.5, ls='--')\n",
        "    ax3.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "    # ── Panel 4: Scatter OLS errors vs RF errors (full OOS)\n",
        "    ax4 = fig.add_subplot(gs[2, 0])\n",
        "    ols_err = y_true - oos['OLS']['pred']\n",
        "    rf_err  = y_true - oos['RF']['pred']\n",
        "    ax4.scatter(ols_err, rf_err, alpha=0.2, s=8,\n",
        "                color='steelblue', edgecolors='none')\n",
        "    lim = max(np.abs(ols_err).max(), np.abs(rf_err).max())\n",
        "    ax4.plot([-lim, lim], [-lim, lim], 'r--', lw=1, label='45° line (equal errors)')\n",
        "    ax4.axhline(0, color='gray', lw=0.4)\n",
        "    ax4.axvline(0, color='gray', lw=0.4)\n",
        "    ax4.set_xlabel('OLS Error (%)')\n",
        "    ax4.set_ylabel('RF Error (%)')\n",
        "    ax4.set_title('OOS Error Comparison: OLS vs RF', fontsize=11, fontweight='bold')\n",
        "    ax4.legend(fontsize=9)\n",
        "\n",
        "    # ── Panel 5: Cumulative squared error (lower = better)\n",
        "    ax5 = fig.add_subplot(gs[2, 1])\n",
        "    for name in ['OLS', 'RF', 'SVR', 'GBR']:\n",
        "        cum_sq_err = np.cumsum((y_true - oos[name]['pred'])**2)\n",
        "        ax5.plot(oos_index, cum_sq_err,\n",
        "                 color=colors[name], lw=1.2, label=name)\n",
        "    ax5.set_title('Cumulative Squared Error Over Time (OOS)', fontsize=11, fontweight='bold')\n",
        "    ax5.set_ylabel('Cumulative SE')\n",
        "    ax5.legend(fontsize=9)\n",
        "    ax5.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.suptitle('NVDA CAPM Analysis — Time-Series CV Results', fontsize=14,\n",
        "                 fontweight='bold', y=1.01)\n",
        "\n",
        "    import os\n",
        "    os.makedirs('/mnt/user-data/outputs', exist_ok=True)\n",
        "    plt.savefig('/mnt/user-data/outputs/design_a_tscv_results.png',\n",
        "                dpi=150, bbox_inches='tight')\n",
        "    print(\"\\n✓ Plot saved.\")\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# MAIN\n",
        "# ============================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # 1. Data\n",
        "    df = build_dataset()\n",
        "    df, q25, q75 = add_regimes(df)\n",
        "\n",
        "    # 2. Non-linearity tests\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"STEP 1: NON-LINEARITY TESTING\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    nonlin_results = {}\n",
        "    reject_counts  = {}\n",
        "\n",
        "    X_full = df[\"excess_mkt\"].values\n",
        "    y_full = df[\"excess_nvda\"].values\n",
        "    nonlin_results['Full'], _, reject_counts['Full'] = \\\n",
        "        test_nonlinearity_comprehensive(X_full, y_full, \"Full Sample\")\n",
        "\n",
        "    for regime in REGIMES:\n",
        "        subset = df[df[\"Regime\"] == regime]\n",
        "        nonlin_results[regime], _, reject_counts[regime] = \\\n",
        "            test_nonlinearity_comprehensive(\n",
        "                subset[\"excess_mkt\"].values,\n",
        "                subset[\"excess_nvda\"].values,\n",
        "                regime\n",
        "            )\n",
        "\n",
        "    # 3. Regime betas\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"STEP 2: REGIME-SPECIFIC BETA ANALYSIS\")\n",
        "    print(\"=\"*70)\n",
        "    regime_betas, _ = estimate_regime_betas(df)\n",
        "\n",
        "    # 4. Time-series CV  ← replaces simple full-sample training\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"STEP 3: TIME-SERIES CROSS-VALIDATION (Walk-Forward)\")\n",
        "    print(\"=\"*70)\n",
        "    oos, fold_log = run_tscv(df, n_splits=N_SPLITS, min_train=MIN_TRAIN)\n",
        "\n",
        "    # 5. Per-fold summary\n",
        "    fold_df = per_fold_r2_table(fold_log)\n",
        "    print(\"\\nPer-fold R² summary:\")\n",
        "    r2_cols = [c for c in fold_df.columns if c.endswith('_r2')]\n",
        "    print(fold_df[['fold', 'n_train', 'n_test'] + r2_cols].to_string(index=False))\n",
        "\n",
        "    # 6. OOS summary table\n",
        "    summary_df = oos_summary_table(df, oos)\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"OUT-OF-SAMPLE PERFORMANCE SUMMARY\")\n",
        "    print(\"=\"*70)\n",
        "    print(summary_df.to_string(index=False))\n",
        "\n",
        "    # 7. DM tests on OOS errors\n",
        "    dm_results = compare_oos_all_regimes(df, oos)\n",
        "\n",
        "    # 8. Plots\n",
        "    plot_results(df, oos, fold_df, summary_df)\n",
        "\n",
        "    # 9. Final interpretation\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"FINAL INTERPRETATION\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    print(\"\\n1. NON-LINEARITY EVIDENCE:\")\n",
        "    print(f\"   Full sample: {reject_counts['Full']}/4 tests reject linearity at 5%\")\n",
        "    for regime in REGIMES:\n",
        "        print(f\"   {regime:20s}: {reject_counts[regime]}/4 tests reject linearity\")\n",
        "\n",
        "    print(\"\\n2. BETA STABILITY:\")\n",
        "    sig_regimes = regime_betas[regime_betas['p_value'] < 0.05]\n",
        "    if len(sig_regimes) > 0:\n",
        "        print(f\"   {len(sig_regimes)}/3 regimes have β significantly different from full sample\")\n",
        "        print(\"   → Evidence of regime-dependent beta\")\n",
        "    else:\n",
        "        print(\"   All regime betas consistent with full-sample β\")\n",
        "        print(\"   → Beta appears stable across market conditions\")\n",
        "\n",
        "    print(\"\\n3. OUT-OF-SAMPLE MODEL PERFORMANCE:\")\n",
        "    full_row = summary_df[summary_df['Regime'] == 'Full OOS'].iloc[0]\n",
        "    r2_vals = {col.replace('_R2', ''): full_row[col]\n",
        "               for col in summary_df.columns if col.endswith('_R2')}\n",
        "    best = max(r2_vals, key=r2_vals.get)\n",
        "    print(f\"   Best OOS model: {best} (R²={r2_vals[best]:.4f})\")\n",
        "    print(f\"   OLS OOS R²:     {r2_vals['OLS']:.4f}\")\n",
        "    print(f\"   Improvement:    +{r2_vals[best] - r2_vals['OLS']:.4f} R² points\")\n",
        "\n",
        "    print(\"\\n4. DM TEST CONCLUSIONS (OOS — genuine out-of-sample):\")\n",
        "    dm_full = dm_results[dm_results['Regime'] == 'Full OOS']\n",
        "    for _, row in dm_full.iterrows():\n",
        "        sig = \"***\" if row['p_value_2sided'] < 0.01 else \\\n",
        "              \"**\"  if row['p_value_2sided'] < 0.05 else \\\n",
        "              \"*\"   if row['p_value_2sided'] < 0.10 else \"\"\n",
        "        dm_s = f\"{row['dm_stat']:6.3f}\" if not np.isnan(row['dm_stat']) else \"  nan\"\n",
        "        print(f\"   {row['model1']} vs {row['model2']:3s}: DM={dm_s}, \"\n",
        "              f\"p={row['p_value_2sided']:.4f} {sig} → {row['winner']}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"ANALYSIS COMPLETE\")\n",
        "    print(\"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuaPH3e-KjFB",
        "outputId": "8ddbc8eb-ad37-49c0-b188-a5bad730ba1f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "Building dataset: 2015-01-01 → 2025-09-30\n",
            "======================================================================\n",
            "Dataset ready: 2700 observations\n",
            "Date range: 2015-01-05 to 2025-09-29\n",
            "\n",
            "Regime thresholds: q25 = -0.4000%, q75 = 0.6025%\n",
            "\n",
            "Regime distribution:\n",
            "Regime\n",
            "Normal (Mid 50%)    1351\n",
            "Bull (Top 25%)       675\n",
            "Bear (Bot 25%)       674\n",
            "\n",
            "======================================================================\n",
            "STEP 1: NON-LINEARITY TESTING\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "NON-LINEARITY TEST BATTERY: Full Sample\n",
            "======================================================================\n",
            "\n",
            "1. RAMSEY RESET TEST (Omitted Non-linear Terms)\n",
            "----------------------------------------------------------------------\n",
            "   Power=2: F = 1.0592, p = 0.3035\n",
            "   Power=3: F = 4.8993, p = 0.0075\n",
            "   ⚠️  REJECT linearity (p<0.05): Non-linear terms significant\n",
            "\n",
            "2. HARVEY-COLLIER TEST (Recursive Residuals)\n",
            "----------------------------------------------------------------------\n",
            "   t-statistic = -0.1409, p = 0.8879\n",
            "   ✓ Cannot reject linearity (p≥0.05)\n",
            "\n",
            "3. RAINBOW TEST (Parameter Constancy)\n",
            "----------------------------------------------------------------------\n",
            "   F-statistic = 1.3087, p = 0.0000\n",
            "   ⚠️  REJECT linearity (p<0.05): Parameters not constant\n",
            "\n",
            "4. WHITE TEST (Non-linear Heteroskedasticity)\n",
            "----------------------------------------------------------------------\n",
            "   LM statistic = 0.6499, p = 0.7226\n",
            "   F statistic  = 0.3246, p = 0.7228\n",
            "   ✓ Cannot reject homoskedasticity (p≥0.05)\n",
            "\n",
            "======================================================================\n",
            "SUMMARY INTERPRETATION:\n",
            "======================================================================\n",
            "Tests rejecting linearity at 5% level: 2/4\n",
            "\n",
            "🟡 MODERATE EVIDENCE of non-linearity\n",
            "   → ML models may capture additional patterns\n",
            "\n",
            "======================================================================\n",
            "NON-LINEARITY TEST BATTERY: Bear (Bot 25%)\n",
            "======================================================================\n",
            "\n",
            "1. RAMSEY RESET TEST (Omitted Non-linear Terms)\n",
            "----------------------------------------------------------------------\n",
            "   Power=2: F = 5.8762, p = 0.0156\n",
            "   Power=3: F = 7.0029, p = 0.0010\n",
            "   ⚠️  REJECT linearity (p<0.05): Non-linear terms significant\n",
            "\n",
            "2. HARVEY-COLLIER TEST (Recursive Residuals)\n",
            "----------------------------------------------------------------------\n",
            "   t-statistic = -3.8281, p = 0.0001\n",
            "   ⚠️  REJECT linearity (p<0.05): Specification error detected\n",
            "\n",
            "3. RAINBOW TEST (Parameter Constancy)\n",
            "----------------------------------------------------------------------\n",
            "   F-statistic = 1.3048, p = 0.0075\n",
            "   ⚠️  REJECT linearity (p<0.05): Parameters not constant\n",
            "\n",
            "4. WHITE TEST (Non-linear Heteroskedasticity)\n",
            "----------------------------------------------------------------------\n",
            "   LM statistic = 0.0014, p = 0.9993\n",
            "   F statistic  = 0.0007, p = 0.9993\n",
            "   ✓ Cannot reject homoskedasticity (p≥0.05)\n",
            "\n",
            "======================================================================\n",
            "SUMMARY INTERPRETATION:\n",
            "======================================================================\n",
            "Tests rejecting linearity at 5% level: 3/4\n",
            "\n",
            "🔴 STRONG EVIDENCE of non-linearity\n",
            "   → Machine learning models theoretically justified\n",
            "\n",
            "======================================================================\n",
            "NON-LINEARITY TEST BATTERY: Normal (Mid 50%)\n",
            "======================================================================\n",
            "\n",
            "1. RAMSEY RESET TEST (Omitted Non-linear Terms)\n",
            "----------------------------------------------------------------------\n",
            "   Power=2: F = 0.0326, p = 0.8567\n",
            "   Power=3: F = 0.3708, p = 0.6902\n",
            "   ✓ Cannot reject linearity (p≥0.05)\n",
            "\n",
            "2. HARVEY-COLLIER TEST (Recursive Residuals)\n",
            "----------------------------------------------------------------------\n",
            "   t-statistic = -0.0313, p = 0.9751\n",
            "   ✓ Cannot reject linearity (p≥0.05)\n",
            "\n",
            "3. RAINBOW TEST (Parameter Constancy)\n",
            "----------------------------------------------------------------------\n",
            "   F-statistic = 1.0731, p = 0.1800\n",
            "   ✓ Cannot reject parameter constancy (p≥0.05)\n",
            "\n",
            "4. WHITE TEST (Non-linear Heteroskedasticity)\n",
            "----------------------------------------------------------------------\n",
            "   LM statistic = 2.2534, p = 0.3241\n",
            "   F statistic  = 1.1261, p = 0.3246\n",
            "   ✓ Cannot reject homoskedasticity (p≥0.05)\n",
            "\n",
            "======================================================================\n",
            "SUMMARY INTERPRETATION:\n",
            "======================================================================\n",
            "Tests rejecting linearity at 5% level: 0/4\n",
            "\n",
            "🟢 WEAK/NO EVIDENCE of non-linearity\n",
            "   → Linear CAPM appears adequate\n",
            "\n",
            "======================================================================\n",
            "NON-LINEARITY TEST BATTERY: Bull (Top 25%)\n",
            "======================================================================\n",
            "\n",
            "1. RAMSEY RESET TEST (Omitted Non-linear Terms)\n",
            "----------------------------------------------------------------------\n",
            "   Power=2: F = 0.0012, p = 0.9721\n",
            "   Power=3: F = 1.4830, p = 0.2277\n",
            "   ✓ Cannot reject linearity (p≥0.05)\n",
            "\n",
            "2. HARVEY-COLLIER TEST (Recursive Residuals)\n",
            "----------------------------------------------------------------------\n",
            "   t-statistic = 3.9992, p = 0.0001\n",
            "   ⚠️  REJECT linearity (p<0.05): Specification error detected\n",
            "\n",
            "3. RAINBOW TEST (Parameter Constancy)\n",
            "----------------------------------------------------------------------\n",
            "   F-statistic = 1.2054, p = 0.0436\n",
            "   ⚠️  REJECT linearity (p<0.05): Parameters not constant\n",
            "\n",
            "4. WHITE TEST (Non-linear Heteroskedasticity)\n",
            "----------------------------------------------------------------------\n",
            "   LM statistic = 0.1842, p = 0.9120\n",
            "   F statistic  = 0.0917, p = 0.9124\n",
            "   ✓ Cannot reject homoskedasticity (p≥0.05)\n",
            "\n",
            "======================================================================\n",
            "SUMMARY INTERPRETATION:\n",
            "======================================================================\n",
            "Tests rejecting linearity at 5% level: 2/4\n",
            "\n",
            "🟡 MODERATE EVIDENCE of non-linearity\n",
            "   → ML models may capture additional patterns\n",
            "\n",
            "======================================================================\n",
            "STEP 2: REGIME-SPECIFIC BETA ANALYSIS\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "REGIME-SPECIFIC BETA ESTIMATION\n",
            "======================================================================\n",
            "\n",
            "Full sample β = 1.7383 (SE = 0.0385)\n",
            "              R² = 0.4302\n",
            "              N  = 2700\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Regime-specific estimates:\n",
            "----------------------------------------------------------------------\n",
            "          Regime     Beta       SE       R2    N  Diff_from_full    t_stat  p_value\n",
            "  Bear (Bot 25%) 1.557969 0.088120 0.317477  674       -0.180367 -1.875485 0.061159\n",
            "Normal (Mid 50%) 1.680900 0.228432 0.038589 1351       -0.057435 -0.247933 0.804224\n",
            "  Bull (Top 25%) 1.562964 0.102850 0.255478  675       -0.175371 -1.596814 0.110777\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Hypothesis tests: H₀: β_regime = β_full\n",
            "----------------------------------------------------------------------\n",
            "Bear (Bot 25%)      : t =  -1.875, p = 0.0612 *\n",
            "Normal (Mid 50%)    : t =  -0.248, p = 0.8042 \n",
            "Bull (Top 25%)      : t =  -1.597, p = 0.1108 \n",
            "\n",
            "======================================================================\n",
            "STEP 3: TIME-SERIES CROSS-VALIDATION (Walk-Forward)\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "TIME-SERIES CROSS-VALIDATION  (n_splits=10, min_train=252)\n",
            "======================================================================\n",
            "  Fold  1: SKIPPED — train size 250 < 252\n",
            "  Fold  2: train [2015-01-05 → 2016-12-19] ( 495 obs)  |  test [2016-12-20 → 2017-12-08] (245 obs)\n",
            "  Fold  3: train [2015-01-05 → 2017-12-08] ( 740 obs)  |  test [2017-12-11 → 2018-11-29] (245 obs)\n",
            "  Fold  4: train [2015-01-05 → 2018-11-29] ( 985 obs)  |  test [2018-11-30 → 2019-11-20] (245 obs)\n",
            "  Fold  5: train [2015-01-05 → 2019-11-20] (1230 obs)  |  test [2019-11-21 → 2020-11-10] (245 obs)\n",
            "  Fold  6: train [2015-01-05 → 2020-11-10] (1475 obs)  |  test [2020-11-11 → 2021-11-01] (245 obs)\n",
            "  Fold  7: train [2015-01-05 → 2021-11-01] (1720 obs)  |  test [2021-11-02 → 2022-10-21] (245 obs)\n",
            "  Fold  8: train [2015-01-05 → 2022-10-21] (1965 obs)  |  test [2022-10-24 → 2023-10-13] (245 obs)\n",
            "  Fold  9: train [2015-01-05 → 2023-10-13] (2210 obs)  |  test [2023-10-16 → 2024-10-04] (245 obs)\n",
            "  Fold 10: train [2015-01-05 → 2024-10-04] (2455 obs)  |  test [2024-10-07 → 2025-09-29] (245 obs)\n",
            "\n",
            "✓ CV complete — 2205 total out-of-sample observations\n",
            "\n",
            "Per-fold R² summary:\n",
            " fold  n_train  n_test   OLS_r2     RF_r2   SVR_r2    GBR_r2\n",
            "    2      495     245 0.109800 -0.184682 0.103052 -0.183831\n",
            "    3      740     245 0.371827  0.237652 0.356898  0.192715\n",
            "    4      985     245 0.485847  0.342352 0.461006  0.390439\n",
            "    5     1230     245 0.621453  0.481249 0.257842  0.491800\n",
            "    6     1475     245 0.320184  0.177235 0.320637  0.293027\n",
            "    7     1720     245 0.616330  0.569967 0.631378  0.569813\n",
            "    8     1965     245 0.379863  0.275807 0.341122  0.339918\n",
            "    9     2210     245 0.348692  0.320450 0.346864  0.363590\n",
            "   10     2455     245 0.522401  0.506720 0.498243  0.546115\n",
            "\n",
            "======================================================================\n",
            "OUT-OF-SAMPLE PERFORMANCE SUMMARY\n",
            "======================================================================\n",
            "          Regime    N   OLS_R2  OLS_RMSE     RF_R2  RF_RMSE   SVR_R2  SVR_RMSE    GBR_R2  GBR_RMSE\n",
            "        Full OOS 2205 0.455492  2.351388  0.352009 2.565113 0.392325  2.484034  0.377874  2.513397\n",
            "  Bear (Bot 25%)  551 0.254629  2.491060  0.118574 2.708885 0.095035  2.744819  0.168141  2.631617\n",
            "Normal (Mid 50%) 1085 0.048865  2.185725 -0.114159 2.365638 0.045819  2.189223 -0.049350  2.295804\n",
            "  Bull (Top 25%)  569 0.237755  2.511436  0.066905 2.778676 0.096387  2.734426  0.066597  2.779134\n",
            "\n",
            "======================================================================\n",
            "DIEBOLD-MARIANO — OUT-OF-SAMPLE ERRORS (Time-Series CV)\n",
            "======================================================================\n",
            "Note: errors are from held-out test folds — models never saw this data.\n",
            "\n",
            "\n",
            "Full OOS  (n=2205):\n",
            "----------------------------------------------------------------------\n",
            "  OLS vs RF : DM= -6.731, p=0.0000 → OLS (p=0.0000)\n",
            "  OLS vs SVR: DM= -1.770, p=0.0769 → Tie (p≥0.05)\n",
            "  OLS vs GBR: DM= -4.690, p=0.0000 → OLS (p=0.0000)\n",
            "  RF vs SVR: DM=  1.369, p=0.1710 → Tie (p≥0.05)\n",
            "  RF vs GBR: DM=  2.324, p=0.0202 → GBR (p=0.0202)\n",
            "  SVR vs GBR: DM= -0.465, p=0.6417 → Tie (p≥0.05)\n",
            "\n",
            "Bear (Bot 25%)  (n=551):\n",
            "----------------------------------------------------------------------\n",
            "  OLS vs RF : DM= -2.798, p=0.0053 → OLS (p=0.0053)\n",
            "  OLS vs SVR: DM= -1.203, p=0.2295 → Tie (p≥0.05)\n",
            "  OLS vs GBR: DM= -2.272, p=0.0235 → OLS (p=0.0235)\n",
            "  RF vs SVR: DM= -0.217, p=0.8286 → Tie (p≥0.05)\n",
            "  RF vs GBR: DM=  1.551, p=0.1214 → Tie (p≥0.05)\n",
            "  SVR vs GBR: DM=  0.619, p=0.5360 → Tie (p≥0.05)\n",
            "\n",
            "Normal (Mid 50%)  (n=1085):\n",
            "----------------------------------------------------------------------\n",
            "  OLS vs RF : DM= -5.089, p=0.0000 → OLS (p=0.0000)\n",
            "  OLS vs SVR: DM= -0.826, p=0.4088 → Tie (p≥0.05)\n",
            "  OLS vs GBR: DM= -2.454, p=0.0143 → OLS (p=0.0143)\n",
            "  RF vs SVR: DM=  4.981, p=0.0000 → SVR (p=0.0000)\n",
            "  RF vs GBR: DM=  2.290, p=0.0222 → GBR (p=0.0222)\n",
            "  SVR vs GBR: DM= -2.367, p=0.0181 → SVR (p=0.0181)\n",
            "\n",
            "Bull (Top 25%)  (n=569):\n",
            "----------------------------------------------------------------------\n",
            "  OLS vs RF : DM= -4.430, p=0.0000 → OLS (p=0.0000)\n",
            "  OLS vs SVR: DM= -1.676, p=0.0942 → Tie (p≥0.05)\n",
            "  OLS vs GBR: DM= -3.265, p=0.0012 → OLS (p=0.0012)\n",
            "  RF vs SVR: DM=  0.448, p=0.6541 → Tie (p≥0.05)\n",
            "  RF vs GBR: DM= -0.011, p=0.9915 → Tie (p≥0.05)\n",
            "  SVR vs GBR: DM= -0.458, p=0.6468 → Tie (p≥0.05)\n",
            "\n",
            "✓ Plot saved.\n",
            "\n",
            "======================================================================\n",
            "FINAL INTERPRETATION\n",
            "======================================================================\n",
            "\n",
            "1. NON-LINEARITY EVIDENCE:\n",
            "   Full sample: 2/4 tests reject linearity at 5%\n",
            "   Bear (Bot 25%)      : 3/4 tests reject linearity\n",
            "   Normal (Mid 50%)    : 0/4 tests reject linearity\n",
            "   Bull (Top 25%)      : 2/4 tests reject linearity\n",
            "\n",
            "2. BETA STABILITY:\n",
            "   All regime betas consistent with full-sample β\n",
            "   → Beta appears stable across market conditions\n",
            "\n",
            "3. OUT-OF-SAMPLE MODEL PERFORMANCE:\n",
            "   Best OOS model: OLS (R²=0.4555)\n",
            "   OLS OOS R²:     0.4555\n",
            "   Improvement:    +0.0000 R² points\n",
            "\n",
            "4. DM TEST CONCLUSIONS (OOS — genuine out-of-sample):\n",
            "   OLS vs RF : DM=-6.731, p=0.0000 *** → OLS (p=0.0000)\n",
            "   OLS vs SVR: DM=-1.770, p=0.0769 * → Tie (p≥0.05)\n",
            "   OLS vs GBR: DM=-4.690, p=0.0000 *** → OLS (p=0.0000)\n",
            "   RF vs SVR: DM= 1.369, p=0.1710  → Tie (p≥0.05)\n",
            "   RF vs GBR: DM= 2.324, p=0.0202 ** → GBR (p=0.0202)\n",
            "   SVR vs GBR: DM=-0.465, p=0.6417  → Tie (p≥0.05)\n",
            "\n",
            "======================================================================\n",
            "ANALYSIS COMPLETE\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OeHPggwNKlcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The research question:\n",
        "Does the relationship between individual stock returns and the market factor behave differently across market regimes — and do non-linear ML models capture that better than linear CAPM, particularly in extreme conditions?\n",
        "The design:\n",
        "\n",
        "Train all models (OLS, RF, SVR, GBR) on the full dataset for each stock\n",
        "Evaluate performance within each regime (Bear/Normal/Bull) using squared errors\n",
        "Run non-linearity tests within each regime to see where CAPM breaks down\n",
        "Repeat across a cross-section of stock types to see if the findings generalize\n",
        "\n",
        "The stock universe — good candidates:\n",
        "CategoryStocksWhy interestingHigh-beta growth (Mag7)NVDA, TSLA, META, AMZNNon-linearity most likely hereValueBRK-B, JPM, BAC, WMTStable beta, probably linearDefensiveJNJ, PG, KO, PFELow beta, low varianceCyclicalCAT, FCX, F, XOMRegime-sensitive, asymmetricIncome/DividendT, VZ, MO, DLow beta, income-driven\n",
        "What this lets you conclude:\n",
        "\n",
        "If non-linearity and ML advantage concentrates in high-beta growth stocks but disappears in defensive/value stocks, that's a meaningful finding — it tells you when and for which assets CAPM is inadequate\n",
        "The regime breakdown tells you under what market conditions the misspecification matters most"
      ],
      "metadata": {
        "id": "v7-ojQvDXGkO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Architecture change — fully aligned with your research design:\n",
        "\n",
        "One pipeline per stock — analyze_stock() trains all 4 models on the full dataset for that stock, then slices predictions and errors into Bear/Normal/Bull and computes metrics within each slice\n",
        "20 stocks across 5 categories — Mag7 growth, Value, Defensive, Cyclical, Income\n",
        "Squared errors are the core metric — MSE and RMSE reported per model per regime, with the best model flagged per row\n",
        "Non-linearity tests run within each regime — so you can say \"for TSLA in Bear markets, 3/4 tests reject linearity\"\n",
        "DM tests on within-regime errors — OLS vs RF, SVR, GBR separately for each regime and each stock\n",
        "\n",
        "Four output plots:\n",
        "\n",
        "MSE heatmap — each ML model's MSE relative to OLS per regime (green = ML better, red = ML worse)\n",
        "R² bar chart — average within-regime R² by stock category and model\n",
        "Non-linearity bubble chart — how many tests reject linearity, by stock and regime\n",
        "DM wins chart — across 20 stocks, how often does ML beat OLS per regime\n",
        "\n",
        "Three CSV outputs — regime_summary.csv, nonlin_summary.csv, dm_summary.csv — ready for tables in a paper."
      ],
      "metadata": {
        "id": "tr9PRWm2XNSQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import matplotlib.patches as mpatches\n",
        "\n",
        "import yfinance as yf\n",
        "from pandas_datareader import data as pdr\n",
        "\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.stats.diagnostic import (\n",
        "    linear_reset,\n",
        "    linear_harvey_collier,\n",
        "    linear_rainbow,\n",
        "    het_white\n",
        ")\n",
        "from scipy.stats import t as student_t\n",
        "\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "pd.set_option(\"display.width\", 140)\n",
        "np.random.seed(42)\n",
        "\n",
        "os.makedirs(\"/mnt/user-data/outputs\", exist_ok=True)\n",
        "\n",
        "# ============================================================\n",
        "# UNIVERSE\n",
        "# ============================================================\n",
        "\n",
        "STOCK_UNIVERSE = {\n",
        "    \"High-Beta Growth (Mag7)\": [\"NVDA\", \"TSLA\", \"META\", \"AMZN\"],\n",
        "    \"Value\":                   [\"BRK-B\", \"JPM\",  \"BAC\",  \"WMT\"],\n",
        "    \"Defensive\":               [\"JNJ\",   \"PG\",   \"KO\",   \"PFE\"],\n",
        "    \"Cyclical\":                [\"CAT\",   \"FCX\",  \"F\",    \"XOM\"],\n",
        "    \"Income/Dividend\":         [\"T\",     \"VZ\",   \"MO\",   \"D\"],\n",
        "}\n",
        "\n",
        "MODEL_NAMES  = [\"OLS\", \"RF\", \"SVR\", \"GBR\"]\n",
        "REGIMES      = [\"Bear (Bot 25%)\", \"Normal (Mid 50%)\", \"Bull (Top 25%)\"]\n",
        "REGIME_SHORT = {\"Bear (Bot 25%)\": \"Bear\", \"Normal (Mid 50%)\": \"Normal\", \"Bull (Top 25%)\": \"Bull\"}\n",
        "COLORS       = {\"OLS\": \"#1f77b4\", \"RF\": \"#2ca02c\", \"SVR\": \"#ff7f0e\", \"GBR\": \"#d62728\"}\n",
        "\n",
        "START = \"2015-01-01\"\n",
        "END   = \"2025-09-30\"\n",
        "\n",
        "# ============================================================\n",
        "# DATA\n",
        "# ============================================================\n",
        "\n",
        "def _flatten_yf(px):\n",
        "    px = px.copy()\n",
        "    if isinstance(px.columns, pd.MultiIndex):\n",
        "        px.columns = [c[0] if isinstance(c, tuple) else c for c in px.columns]\n",
        "    px.columns.name = None\n",
        "    return px\n",
        "\n",
        "\n",
        "def _yf_adj_close(ticker: str, start: str, end: str) -> pd.DataFrame:\n",
        "    try:\n",
        "        df = yf.download(ticker, start=start, end=end, progress=False, auto_adjust=True)\n",
        "        df = _flatten_yf(df)\n",
        "        for col in [\"Adj Close\", \"Close\", ticker]:\n",
        "            if col in df.columns:\n",
        "                return df[[col]].rename(columns={col: ticker})\n",
        "        # If no suitable column is found after flattening, raise an error or return empty\n",
        "        raise ValueError(f\"Could not find price column for {ticker} after flattening.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching {ticker}: {e}\")\n",
        "    return pd.DataFrame()\n",
        "\n",
        "\n",
        "def build_dataset(start: str = START, end: str = END) -> tuple:\n",
        "    \"\"\"\n",
        "    Returns\n",
        "    -------\n",
        "    prices : pd.DataFrame  — adjusted close, one column per ticker\n",
        "    ff     : pd.DataFrame  — Mkt-RF, RF columns\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Downloading data: {start} → {end}\")\n",
        "    print('='*70)\n",
        "\n",
        "    all_tickers = [t for tickers in STOCK_UNIVERSE.values() for t in tickers]\n",
        "\n",
        "    # MODIFIED: Use _yf_adj_close for each ticker\n",
        "    all_px = {}\n",
        "    for ticker in all_tickers:\n",
        "        try:\n",
        "            df_ticker = _yf_adj_close(ticker, start, end)\n",
        "            if not df_ticker.empty:\n",
        "                all_px[ticker] = df_ticker\n",
        "            else:\n",
        "                print(f\"  Warning: No data for {ticker}. Skipping.\")\n",
        "        except Exception as e:\n",
        "            print(f\"  Error fetching {ticker}: {e}\")\n",
        "            continue\n",
        "\n",
        "    if not all_px:\n",
        "        print(\"No price data fetched for any ticker.\")\n",
        "        return pd.DataFrame(), pd.DataFrame() # Return empty DataFrames if no prices fetched\n",
        "\n",
        "    prices = pd.concat(all_px.values(), axis=1)\n",
        "    print(f\"Price data: {prices.shape[0]} days, {prices.shape[1]} tickers\")\n",
        "    print(f\"Available: {list(prices.columns)}\")\n",
        "\n",
        "    ff_raw = pdr.DataReader(\n",
        "        \"F-F_Research_Data_Factors_daily\", \"famafrench\", start=start, end=end\n",
        "    )[0]\n",
        "    ff = ff_raw[[\"Mkt-RF\", \"RF\"]].rename(columns={\"Mkt-RF\": \"excess_mkt\", \"RF\": \"rf\"})\n",
        "\n",
        "    return prices, ff\n",
        "\n",
        "\n",
        "def build_stock_df(ticker: str, prices: pd.DataFrame, ff: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Build a single-stock analysis dataframe.\"\"\"\n",
        "    if ticker not in prices.columns:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    px = prices[[ticker]].dropna()\n",
        "    df = px.join(ff, how=\"inner\").dropna()\n",
        "    df[\"ret\"]        = df[ticker].pct_change() * 100.0\n",
        "    df[\"excess_ret\"] = df[\"ret\"] - df[\"rf\"]\n",
        "    df = df.dropna()\n",
        "\n",
        "    # Regimes based on MARKET quantiles (consistent across stocks)\n",
        "    q25 = df[\"excess_mkt\"].quantile(0.25)\n",
        "    q75 = df[\"excess_mkt\"].quantile(0.75)\n",
        "    df[\"Regime\"] = \"Normal (Mid 50%)\"\n",
        "    df.loc[df[\"excess_mkt\"] < q25, \"Regime\"] = \"Bear (Bot 25%)\"\n",
        "    df.loc[df[\"excess_mkt\"] > q75, \"Regime\"] = \"Bull (Top 25%)\"\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# MODEL TRAINING  (full sample)\n",
        "# ============================================================\n",
        "\n",
        "def train_models(df: pd.DataFrame) -> dict:\n",
        "    \"\"\"Train all models on the full dataset.\"\"\"\n",
        "    X = df[[\"excess_mkt\"]].values\n",
        "    y = df[\"excess_ret\"].values\n",
        "\n",
        "    models = {\n",
        "        \"OLS\": LinearRegression(),\n",
        "        \"RF\":  RandomForestRegressor(\n",
        "                   n_estimators=300, max_depth=None,\n",
        "                   min_samples_leaf=2, random_state=42, n_jobs=-1),\n",
        "        \"SVR\": Pipeline([\n",
        "                   (\"scaler\", StandardScaler()),\n",
        "                   (\"svm\",    SVR(kernel=\"rbf\", C=10, gamma=\"scale\"))]),\n",
        "        \"GBR\": GradientBoostingRegressor(\n",
        "                   n_estimators=500, learning_rate=0.05,\n",
        "                   max_depth=3, loss=\"huber\", random_state=42)\n",
        "    }\n",
        "\n",
        "    for name, model in models.items():\n",
        "        model.fit(X, y)\n",
        "\n",
        "    predictions = {name: model.predict(X) for name, model in models.items()}\n",
        "    return models, predictions\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# NON-LINEARITY TESTS\n",
        "# ============================================================\n",
        "\n",
        "def nonlinearity_battery(X: np.ndarray, y: np.ndarray) -> dict:\n",
        "    \"\"\"\n",
        "    Run 4 non-linearity tests. Returns dict of p-values and reject count.\n",
        "    RESET counts as one test (reject if either power rejects).\n",
        "    \"\"\"\n",
        "    X_const = sm.add_constant(X)\n",
        "    model   = sm.OLS(y, X_const).fit()\n",
        "    results = {}\n",
        "\n",
        "    # RESET\n",
        "    try:\n",
        "        r2 = linear_reset(model, power=2, use_f=True)\n",
        "        r3 = linear_reset(model, power=3, use_f=True)\n",
        "        results['RESET_p2'] = r2.pvalue\n",
        "        results['RESET_p3'] = r3.pvalue\n",
        "    except Exception:\n",
        "        results['RESET_p2'] = np.nan\n",
        "        results['RESET_p3'] = np.nan\n",
        "\n",
        "    # Harvey-Collier\n",
        "    try:\n",
        "        hc = linear_harvey_collier(model)\n",
        "        results['HC_p'] = hc[1]\n",
        "    except Exception:\n",
        "        results['HC_p'] = np.nan\n",
        "\n",
        "    # Rainbow\n",
        "    try:\n",
        "        rb = linear_rainbow(model, frac=0.5)\n",
        "        results['Rainbow_p'] = rb[1]\n",
        "    except Exception:\n",
        "        results['Rainbow_p'] = np.nan\n",
        "\n",
        "    # White\n",
        "    try:\n",
        "        wh = het_white(model.resid, model.model.exog)\n",
        "        results['White_p'] = wh[1]\n",
        "    except Exception:\n",
        "        results['White_p'] = np.nan\n",
        "\n",
        "    # Count rejections (RESET = 1 test)\n",
        "    reset_reject = (\n",
        "        (results.get('RESET_p2', 1) < 0.05) or\n",
        "        (results.get('RESET_p3', 1) < 0.05)\n",
        "    )\n",
        "    results['reject_count'] = sum([\n",
        "        reset_reject,\n",
        "        results.get('HC_p',     1) < 0.05,\n",
        "        results.get('Rainbow_p',1) < 0.05,\n",
        "        results.get('White_p',  1) < 0.05,\n",
        "    ])\n",
        "\n",
        "    if results['reject_count'] >= 3:\n",
        "        results['verdict'] = \"🔴 STRONG\"\n",
        "    elif results['reject_count'] >= 2:\n",
        "        results['verdict'] = \"🟡 MODERATE\"\n",
        "    else:\n",
        "        results['verdict'] = \"🟢 WEAK/NONE\"\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# DIEBOLD-MARIANO TEST\n",
        "# ============================================================\n",
        "\n",
        "def dm_test(errors1: np.ndarray, errors2: np.ndarray,\n",
        "            name1: str, name2: str, lags: int = 6) -> dict:\n",
        "    \"\"\"\n",
        "    DM test on squared error loss.\n",
        "    Positive DM → model1 worse → model2 better.\n",
        "    \"\"\"\n",
        "    e1 = np.asarray(errors1, dtype=float)\n",
        "    e2 = np.asarray(errors2, dtype=float)\n",
        "    d  = e1**2 - e2**2\n",
        "    d  = d[~np.isnan(d)]\n",
        "    T  = len(d)\n",
        "\n",
        "    base = {'model1': name1, 'model2': name2, 'n_obs': T,\n",
        "            'mean_d': np.nan, 'dm_stat': np.nan,\n",
        "            'p_value': np.nan, 'winner': 'n/a'}\n",
        "\n",
        "    if T < 30:\n",
        "        base['winner'] = 'Insufficient data'\n",
        "        return base\n",
        "\n",
        "    d0    = d - d.mean()\n",
        "    gamma = np.sum(d0**2) / T\n",
        "    L     = min(lags, T - 1)\n",
        "    for k in range(1, L + 1):\n",
        "        gamma += 2.0 * (1.0 - k/(L+1.0)) * np.sum(d0[k:] * d0[:-k]) / T\n",
        "\n",
        "    var_d = gamma / T\n",
        "    if var_d <= 0:\n",
        "        base['winner'] = 'Variance error'\n",
        "        return base\n",
        "\n",
        "    dm   = d.mean() / np.sqrt(var_d)\n",
        "    pval = 2 * (1 - student_t.cdf(np.abs(dm), df=T-1))\n",
        "\n",
        "    winner = (name2 if (pval < 0.05 and d.mean() > 0)\n",
        "              else name1 if (pval < 0.05 and d.mean() < 0)\n",
        "              else \"Tie\")\n",
        "\n",
        "    return {**base,\n",
        "            'mean_d': float(d.mean()), 'dm_stat': float(dm),\n",
        "            'p_value': float(pval),    'winner': winner}\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# PER-STOCK ANALYSIS\n",
        "# ============================================================\n",
        "\n",
        "def analyze_stock(ticker: str, prices: pd.DataFrame,\n",
        "                  ff: pd.DataFrame, verbose: bool = True) -> dict:\n",
        "    \"\"\"\n",
        "    Full pipeline for a single stock:\n",
        "      - Build df\n",
        "      - Train models (full sample)\n",
        "      - Within-regime: squared errors, R², non-linearity, DM tests\n",
        "    \"\"\"\n",
        "    df = build_stock_df(ticker, prices, ff)\n",
        "    if df.empty or len(df) < 300:\n",
        "        print(f\"  {ticker}: insufficient data, skipping\")\n",
        "        return {}\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"\\n  {'─'*60}\")\n",
        "        print(f\"  {ticker}  (N={len(df)})\")\n",
        "        print(f\"  {'─'*60}\")\n",
        "\n",
        "    models, predictions = train_models(df)\n",
        "\n",
        "    X_all  = df[[\"excess_mkt\"]].values\n",
        "    y_all  = df[\"excess_ret\"].values\n",
        "    errors = {name: y_all - predictions[name] for name in MODEL_NAMES}\n",
        "\n",
        "    # ── Per-regime results\n",
        "    regime_rows   = []\n",
        "    nonlin_rows   = []\n",
        "    dm_rows       = []\n",
        "\n",
        "    for regime in [\"Full Sample\"] + REGIMES:\n",
        "        if regime == \"Full Sample\":\n",
        "            mask = np.ones(len(df), dtype=bool)\n",
        "        else:\n",
        "            mask = (df[\"Regime\"] == regime).values\n",
        "\n",
        "        y_sub = y_all[mask]\n",
        "        n_sub = mask.sum()\n",
        "\n",
        "        # Squared error metrics\n",
        "        row = {'Ticker': ticker, 'Regime': regime, 'N': int(n_sub)}\n",
        "        for name in MODEL_NAMES:\n",
        "            pred_sub  = predictions[name][mask]\n",
        "            err_sub   = errors[name][mask]\n",
        "            mse       = np.mean(err_sub**2)\n",
        "            rmse      = np.sqrt(mse)\n",
        "            r2        = r2_score(y_sub, pred_sub) if n_sub > 2 else np.nan\n",
        "            row[f'{name}_MSE']  = mse\n",
        "            row[f'{name}_RMSE'] = rmse\n",
        "            row[f'{name}_R2']   = r2\n",
        "        regime_rows.append(row)\n",
        "\n",
        "        # Non-linearity (only meaningful with ≥50 obs)\n",
        "        if n_sub >= 50:\n",
        "            nl = nonlinearity_battery(X_all[mask], y_sub)\n",
        "            nl_row = {'Ticker': ticker, 'Regime': regime, 'N': int(n_sub)}\n",
        "            nl_row.update(nl)\n",
        "            nonlin_rows.append(nl_row)\n",
        "\n",
        "        # DM tests: OLS vs each ML model\n",
        "        if n_sub >= 30:\n",
        "            for ml in [\"RF\", \"SVR\", \"GBR\"]:\n",
        "                dm = dm_test(errors[\"OLS\"][mask], errors[ml][mask], \"OLS\", ml)\n",
        "                dm['Ticker'] = ticker\n",
        "                dm['Regime'] = regime\n",
        "                dm_rows.append(dm)\n",
        "\n",
        "        if verbose and regime != \"Full Sample\":\n",
        "            best = min(MODEL_NAMES, key=lambda m: row[f'{m}_MSE'])\n",
        "            print(f\"  {REGIME_SHORT[regime]:6s}: \"\n",
        "                  + \"  \".join(f\"{m} R²={row[f'{m}_R2']:+.3f}\" for m in MODEL_NAMES)\n",
        "                  + f\"  → best MSE: {best}\")\n",
        "\n",
        "    return {\n",
        "        'ticker':       ticker,\n",
        "        'regime_df':    pd.DataFrame(regime_rows),\n",
        "        'nonlin_df':    pd.DataFrame(nonlin_rows),\n",
        "        'dm_df':        pd.DataFrame(dm_rows),\n",
        "        'predictions':  predictions,\n",
        "        'df':           df,\n",
        "    }\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# CROSS-SECTION AGGREGATION\n",
        "# ============================================================\n",
        "\n",
        "def run_cross_section(prices: pd.DataFrame, ff: pd.DataFrame) -> dict:\n",
        "    \"\"\"Run analyze_stock for every ticker in the universe.\"\"\"\n",
        "    all_results = {}\n",
        "\n",
        "    for category, tickers in STOCK_UNIVERSE.items():\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"CATEGORY: {category}\")\n",
        "        print('='*70)\n",
        "\n",
        "        for ticker in tickers:\n",
        "            res = analyze_stock(ticker, prices, ff, verbose=True)\n",
        "            if res:\n",
        "                res['category'] = category\n",
        "                all_results[ticker] = res\n",
        "\n",
        "    return all_results\n",
        "\n",
        "\n",
        "def build_summary_tables(all_results: dict) -> tuple:\n",
        "    \"\"\"Concatenate results across all stocks into summary DataFrames.\"\"\"\n",
        "\n",
        "    regime_frames = []\n",
        "    nonlin_frames = []\n",
        "    dm_frames     = []\n",
        "\n",
        "    for ticker, res in all_results.items():\n",
        "        cat = res['category']\n",
        "\n",
        "        r = res['regime_df'].copy();  r['Category'] = cat\n",
        "        n = res['nonlin_df'].copy();  n['Category'] = cat\n",
        "        d = res['dm_df'].copy();      d['Category'] = cat\n",
        "\n",
        "        regime_frames.append(r)\n",
        "        nonlin_frames.append(n)\n",
        "        dm_frames.append(d)\n",
        "\n",
        "    # Check if frames are empty before concatenating\n",
        "    if not regime_frames:\n",
        "        print(\"No regime data frames to concatenate.\")\n",
        "        regime_summary = pd.DataFrame()\n",
        "    else:\n",
        "        regime_summary = pd.concat(regime_frames, ignore_index=True)\n",
        "\n",
        "    if not nonlin_frames:\n",
        "        print(\"No non-linearity data frames to concatenate.\")\n",
        "        nonlin_summary = pd.DataFrame()\n",
        "    else:\n",
        "        nonlin_summary = pd.concat(nonlin_frames, ignore_index=True)\n",
        "\n",
        "    if not dm_frames:\n",
        "        print(\"No DM test data frames to concatenate.\")\n",
        "        dm_summary = pd.DataFrame()\n",
        "    else:\n",
        "        dm_summary     = pd.concat(dm_frames,     ignore_index=True)\n",
        "\n",
        "    return regime_summary, nonlin_summary, dm_summary\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# PRINT SUMMARY REPORTS\n",
        "# ============================================================\n",
        "\n",
        "def print_regime_report(regime_summary: pd.DataFrame):\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"WITHIN-REGIME MSE SUMMARY (Full Cross-Section)\")\n",
        "    print('='*70)\n",
        "\n",
        "    for regime in [\"Full Sample\"] + REGIMES:\n",
        "        sub = regime_summary[regime_summary['Regime'] == regime]\n",
        "        if sub.empty:\n",
        "            continue\n",
        "\n",
        "        print(f\"\\n── {regime} ──\")\n",
        "        mse_cols = [f'{m}_MSE' for m in MODEL_NAMES]\n",
        "        display  = sub[['Category', 'Ticker'] + mse_cols].copy()\n",
        "\n",
        "        # Flag best model per row\n",
        "        display['Best'] = sub[mse_cols].idxmin(axis=1).str.replace('_MSE','')\n",
        "        print(display.to_string(index=False))\n",
        "\n",
        "        # Category-level average MSE\n",
        "        print(f\"\\n  Category averages:\")\n",
        "        cat_avg = sub.groupby('Category')[mse_cols].mean()\n",
        "        cat_avg['Best'] = cat_avg.idxmin(axis=1).str.replace('_MSE','')\n",
        "        print(cat_avg.to_string())\n",
        "\n",
        "\n",
        "def print_nonlin_report(nonlin_summary: pd.DataFrame):\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"NON-LINEARITY EVIDENCE BY REGIME AND STOCK TYPE\")\n",
        "    print('='*70)\n",
        "\n",
        "    # Ensure nonlin_summary is not empty\n",
        "    if nonlin_summary.empty:\n",
        "        print(\"No non-linearity data to report.\")\n",
        "        return\n",
        "\n",
        "    pivot = nonlin_summary.pivot_table(\n",
        "        index=['Category', 'Ticker'],\n",
        "        columns='Regime',\n",
        "        values='reject_count',\n",
        "        aggfunc='first'\n",
        "    )\n",
        "    # Check if 'Full Sample' regime exists in nonlin_summary before attempting to reindex\n",
        "    if 'Full Sample' in nonlin_summary['Regime'].unique():\n",
        "        pivot['verdict'] = nonlin_summary[\n",
        "            nonlin_summary['Regime'] == 'Full Sample'\n",
        "        ].set_index('Ticker')['verdict'].reindex(\n",
        "            pivot.index.get_level_values('Ticker')\n",
        "        ).values\n",
        "    else:\n",
        "        pivot['verdict'] = 'N/A'\n",
        "\n",
        "    print(pivot.to_string())\n",
        "\n",
        "\n",
        "def print_dm_report(dm_summary: pd.DataFrame):\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"DM TEST: OLS vs ML MODELS — WITHIN REGIME\")\n",
        "    print(\"Positive DM → OLS has larger squared errors → ML better\")\n",
        "    print('='*70)\n",
        "\n",
        "    for regime in [\"Full Sample\"] + REGIMES:\n",
        "        sub = dm_summary[dm_summary['Regime'] == regime]\n",
        "        if sub.empty:\n",
        "            continue\n",
        "\n",
        "        print(f\"\\n── {regime} ──\")\n",
        "        for ml in [\"RF\", \"SVR\", \"GBR\"]:\n",
        "            ml_sub = sub[sub['model2'] == ml][\n",
        "                ['Category', 'Ticker', 'dm_stat', 'p_value', 'winner']\n",
        "            ].copy()\n",
        "            ml_sub['sig'] = ml_sub['p_value'].apply(\n",
        "                lambda p: '***' if p < 0.01 else '**' if p < 0.05\n",
        "                          else '*' if p < 0.10 else ''\n",
        "            )\n",
        "            ml_wins  = (ml_sub['winner'] == ml).sum()\n",
        "            ols_wins = (ml_sub['winner'] == 'OLS').sum()\n",
        "            ties     = (ml_sub['winner'] == 'Tie').sum()\n",
        "            print(f\"\\n  OLS vs {ml}:  ML wins={ml_wins}  OLS wins={ols_wins}  Ties={ties}\")\n",
        "            print(ml_sub.to_string(index=False))\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# PLOTS\n",
        "# ============================================================\n",
        "\n",
        "def plot_mse_heatmap(regime_summary: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Heatmap: rows = tickers grouped by category,\n",
        "             cols = model × regime combinations.\n",
        "    Color = MSE relative to OLS within each regime (< 1 = ML better).\n",
        "    \"\"\"\n",
        "    regimes_plot = REGIMES  # exclude full sample for focus\n",
        "\n",
        "    if regime_summary.empty:\n",
        "        print(\"  Skipping MSE heatmap plot: regime_summary is empty.\")\n",
        "        return\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 10), sharey=True)\n",
        "    fig.suptitle(\"MSE Relative to OLS by Regime\\n(< 1.0 = ML better than OLS)\",\n",
        "                 fontsize=13, fontweight='bold')\n",
        "\n",
        "    for ax, regime in zip(axes, regimes_plot):\n",
        "        sub = regime_summary[regime_summary['Regime'] == regime].copy()\n",
        "        if sub.empty:\n",
        "            ax.set_title(f\"No data for {REGIME_SHORT[regime]}\")\n",
        "            continue\n",
        "\n",
        "        sub = sub.sort_values(['Category', 'Ticker'])\n",
        "\n",
        "        # Relative MSE vs OLS\n",
        "        rel = pd.DataFrame(index=sub['Ticker'])\n",
        "        for ml in [\"RF\", \"SVR\", \"GBR\"]:\n",
        "            rel[ml] = sub[f'{ml}_MSE'].values / sub['OLS_MSE'].values\n",
        "\n",
        "        im = ax.imshow(rel.values, aspect='auto', cmap='RdYlGn_r',\n",
        "                       vmin=0.7, vmax=1.3)\n",
        "        ax.set_xticks(range(3))\n",
        "        ax.set_xticklabels([\"RF\", \"SVR\", \"GBR\"], fontsize=10)\n",
        "        ax.set_yticks(range(len(sub)))\n",
        "        ax.set_yticklabels(\n",
        "            [f\"{row.Category[:4]}│{row.Ticker}\"\n",
        "             for _, row in sub.iterrows()],\n",
        "            fontsize=8\n",
        "        )\n",
        "        ax.set_title(REGIME_SHORT[regime], fontsize=11, fontweight='bold')\n",
        "\n",
        "        # Annotate values\n",
        "        for i in range(len(sub)):\n",
        "            for j, ml in enumerate([\"RF\", \"SVR\", \"GBR\"]):\n",
        "                val = rel.iloc[i, j]\n",
        "                ax.text(j, i, f\"{val:.2f}\", ha='center', va='center',\n",
        "                        fontsize=7, color='black',\n",
        "                        fontweight='bold' if val < 0.95 else 'normal')\n",
        "\n",
        "        plt.colorbar(im, ax=ax, shrink=0.6, label='MSE ratio vs OLS')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('/mnt/user-data/outputs/mse_heatmap.png', dpi=150, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(\"✓ MSE heatmap saved.\")\n",
        "\n",
        "\n",
        "def plot_r2_by_category(regime_summary: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Bar chart: average R² per model per regime, faceted by stock category.\n",
        "    \"\"\"\n",
        "    if regime_summary.empty:\n",
        "        print(\"  Skipping R2 by category plot: regime_summary is empty.\")\n",
        "        return\n",
        "\n",
        "    categories = list(STOCK_UNIVERSE.keys())\n",
        "    n_cat = len(categories)\n",
        "    fig, axes = plt.subplots(1, n_cat, figsize=(20, 5), sharey=False)\n",
        "    fig.suptitle(\"Average Within-Regime R² by Stock Category and Model\",\n",
        "                 fontsize=13, fontweight='bold')\n",
        "\n",
        "    x     = np.arange(len(REGIMES))\n",
        "    width = 0.18\n",
        "\n",
        "    # Ensure axes is an array even if n_cat is 1\n",
        "    if n_cat == 1 and not isinstance(axes, np.ndarray):\n",
        "        axes = np.array([axes])\n",
        "\n",
        "    for ax, cat in zip(axes, categories):\n",
        "        sub = regime_summary[\n",
        "            (regime_summary['Category'] == cat) &\n",
        "            (regime_summary['Regime'].isin(REGIMES))\n",
        "        ]\n",
        "        if sub.empty:\n",
        "            ax.set_title(f\"No data for {cat}\")\n",
        "            ax.set_xticks(x + width*1.5)\n",
        "            ax.set_xticklabels([REGIME_SHORT[r] for r in REGIMES], fontsize=8)\n",
        "            continue\n",
        "\n",
        "        for i, name in enumerate(MODEL_NAMES):\n",
        "            avg_r2 = sub.groupby('Regime')[f'{name}_R2'].mean().reindex(REGIMES)\n",
        "            ax.bar(x + i*width, avg_r2.values, width,\n",
        "                   color=COLORS[name], label=name, alpha=0.85)\n",
        "\n",
        "        ax.set_title(cat, fontsize=9, fontweight='bold')\n",
        "        ax.set_xticks(x + width*1.5)\n",
        "        ax.set_xticklabels([REGIME_SHORT[r] for r in REGIMES], fontsize=8)\n",
        "        ax.axhline(0, color='gray', lw=0.5, ls='--')\n",
        "        ax.set_ylabel('R²' if cat == categories[0] else '')\n",
        "        ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "    handles = [mpatches.Patch(color=COLORS[m], label=m) for m in MODEL_NAMES]\n",
        "    fig.legend(handles=handles, loc='lower center', ncol=4,\n",
        "               fontsize=10, bbox_to_anchor=(0.5, -0.05))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('/mnt/user-data/outputs/r2_by_category.png',\n",
        "                dpi=150, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(\"✓ R² by category plot saved.\")\n",
        "\n",
        "\n",
        "def plot_nonlinearity_summary(nonlin_summary: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Bubble chart: x=regime, y=stock, size/color = reject count (0-4).\n",
        "    \"\"\"\n",
        "    if nonlin_summary.empty:\n",
        "        print(\"  Skipping non-linearity bubble plot: nonlin_summary is empty.\")\n",
        "        return\n",
        "\n",
        "    sub = nonlin_summary[nonlin_summary['Regime'].isin(REGIMES)].copy()\n",
        "    sub = sub.sort_values(['Category', 'Ticker'])\n",
        "\n",
        "    tickers    = sub['Ticker'].unique()\n",
        "    regime_ord = REGIMES\n",
        "\n",
        "    if len(tickers) == 0:\n",
        "        print(\"  No tickers with non-linearity data to plot.\")\n",
        "        return\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, max(6, len(tickers)*0.4)))\n",
        "\n",
        "    cmap   = plt.cm.get_cmap('RdYlGn_r', 5)\n",
        "    ticker_to_y = {t: i for i, t in enumerate(tickers)}\n",
        "    regime_to_x = {r: i for i, r in enumerate(regime_ord)}\n",
        "\n",
        "    for _, row in sub.iterrows():\n",
        "        x   = regime_to_x[row['Regime']]\n",
        "        y   = ticker_to_y[row['Ticker']]\n",
        "        cnt = row['reject_count']\n",
        "        ax.scatter(x, y, s=cnt*200+100, c=cnt, cmap='RdYlGn_r',\n",
        "                   vmin=0, vmax=4, alpha=0.8, edgecolors='gray', lw=0.5)\n",
        "        ax.text(x, y, str(int(cnt)), ha='center', va='center',\n",
        "                fontsize=8, fontweight='bold', color='white' if cnt >= 2 else 'black')\n",
        "\n",
        "    ax.set_xticks(range(len(regime_ord)))\n",
        "    ax.set_xticklabels([REGIME_SHORT[r] for r in regime_ord], fontsize=10)\n",
        "    ax.set_yticks(range(len(tickers)))\n",
        "    ticker_labels = []\n",
        "    prev_cat = None\n",
        "    for t in tickers:\n",
        "        cat = sub[sub['Ticker']==t]['Category'].iloc[0]\n",
        "        prefix = f\"[{cat[:5]}] \" if cat != prev_cat else \"         \"\n",
        "        ticker_labels.append(prefix + t)\n",
        "        prev_cat = cat\n",
        "    ax.set_yticklabels(ticker_labels, fontsize=8)\n",
        "    ax.set_title(\"Non-Linearity Test Rejections by Regime\\n(number out of 4 tests; larger/redder = more evidence)\",\n",
        "                 fontsize=11, fontweight='bold')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    sm_obj = plt.cm.ScalarMappable(cmap='RdYlGn_r',\n",
        "                                    norm=plt.Normalize(vmin=0, vmax=4))\n",
        "    sm_obj.set_array([])\n",
        "    plt.colorbar(sm_obj, ax=ax, label='Tests rejecting linearity (out of 4)',\n",
        "                 shrink=0.6)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('/mnt/user-data/outputs/nonlinearity_bubble.png',\n",
        "                dpi=150, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(\"✓ Non-linearity bubble chart saved.\")\n",
        "\n",
        "\n",
        "def plot_dm_wins(dm_summary: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Stacked bar: for each regime × ML model, count OLS wins / ties / ML wins\n",
        "    broken down by stock category.\n",
        "    \"\"\"\n",
        "    if dm_summary.empty:\n",
        "        print(\"  Skipping DM wins plot: dm_summary is empty.\")\n",
        "        return\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
        "    fig.suptitle(\"DM Test Outcomes: OLS vs ML Models Within Each Regime\",\n",
        "                 fontsize=13, fontweight='bold')\n",
        "\n",
        "    cat_colors = {\n",
        "        \"High-Beta Growth (Mag7)\": \"#e41a1c\",\n",
        "        \"Value\":                   \"#377eb8\",\n",
        "        \"Defensive\":               \"#4daf4a\",\n",
        "        \"Cyclical\":                \"#ff7f00\",\n",
        "        \"Income/Dividend\":         \"#984ea3\",\n",
        "    }\n",
        "\n",
        "    for ax, regime in zip(axes, REGIMES):\n",
        "        sub = dm_summary[dm_summary['Regime'] == regime]\n",
        "        if sub.empty:\n",
        "            ax.set_title(f\"No data for {REGIME_SHORT[regime]}\")\n",
        "            continue\n",
        "\n",
        "        ml_models = [\"RF\", \"SVR\", \"GBR\"]\n",
        "        x = np.arange(len(ml_models))\n",
        "\n",
        "        ols_wins = [((sub[sub['model2']==ml]['winner'] == 'OLS').sum()) for ml in ml_models]\n",
        "        ties     = [((sub[sub['model2']==ml]['winner'] == 'Tie').sum()) for ml in ml_models]\n",
        "        ml_wins  = [((sub[sub['model2']==ml]['winner'] == ml).sum())  for ml in ml_models]\n",
        "\n",
        "        ax.bar(x, ols_wins, label='OLS wins', color='#1f77b4', alpha=0.85)\n",
        "        ax.bar(x, ties,     bottom=ols_wins, label='Tie', color='#aec7e8', alpha=0.85)\n",
        "        ax.bar(x, ml_wins,  bottom=[o+t for o,t in zip(ols_wins,ties)],\n",
        "               label='ML wins', color='#d62728', alpha=0.85)\n",
        "\n",
        "        ax.set_title(REGIME_SHORT[regime], fontsize=11, fontweight='bold')\n",
        "        ax.set_xticks(x)\n",
        "        ax.set_xticklabels(ml_models)\n",
        "        ax.set_ylabel('Number of stocks' if regime == REGIMES[0] else '')\n",
        "        ax.grid(True, alpha=0.3, axis='y')\n",
        "        if regime == REGIMES[0]:\n",
        "            ax.legend(fontsize=9)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('/mnt/user-data/outputs/dm_wins.png', dpi=150, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(\"✓ DM wins chart saved.\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# MAIN\n",
        "# ============================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # 1. Download data\n",
        "    prices, ff = build_dataset()\n",
        "\n",
        "    # 2. Run full cross-section\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"RUNNING CROSS-SECTION ANALYSIS\")\n",
        "    print('='*70)\n",
        "    all_results = run_cross_section(prices, ff)\n",
        "\n",
        "    # 3. Build summary tables\n",
        "    regime_summary, nonlin_summary, dm_summary = build_summary_tables(all_results)\n",
        "\n",
        "    # 4. Print reports\n",
        "    print_regime_report(regime_summary)\n",
        "    print_nonlin_report(nonlin_summary)\n",
        "    print_dm_report(dm_summary)\n",
        "\n",
        "    # 5. Plots\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"GENERATING PLOTS\")\n",
        "    print('='*70)\n",
        "    plot_mse_heatmap(regime_summary)\n",
        "    plot_r2_by_category(regime_summary)\n",
        "    plot_nonlinearity_summary(nonlin_summary)\n",
        "    plot_dm_wins(dm_summary)\n",
        "\n",
        "    # 6. Save summary CSVs\n",
        "    regime_summary.to_csv('/mnt/user-data/outputs/regime_summary.csv', index=False)\n",
        "    nonlin_summary.to_csv('/mnt/user-data/outputs/nonlin_summary.csv', index=False)\n",
        "    dm_summary.to_csv('/mnt/user-data/outputs/dm_summary.csv',         index=False)\n",
        "    print(\"\\n✓ Summary CSVs saved.\")\n",
        "\n",
        "    # 7. High-level conclusions\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"HIGH-LEVEL CONCLUSIONS\")\n",
        "    print('='*70)\n",
        "\n",
        "    # Check if dm_summary and nonlin_summary are empty before proceeding\n",
        "    if dm_summary.empty:\n",
        "        print(\"No DM summary data available for high-level conclusions.\")\n",
        "    else:\n",
        "        for regime in REGIMES:\n",
        "            sub = dm_summary[dm_summary['Regime'] == regime]\n",
        "            if not sub.empty:\n",
        "                print(f\"\\n{REGIME_SHORT[regime]} market:\")\n",
        "                for ml in [\"RF\", \"SVR\", \"GBR\"]:\n",
        "                    ml_sub   = sub[sub['model2'] == ml]\n",
        "                    ml_wins  = (ml_sub['winner'] == ml).sum()\n",
        "                    ols_wins = (ml_sub['winner'] == 'OLS').sum()\n",
        "                    total    = len(ml_sub)\n",
        "                    print(f\"  OLS vs {ml}: ML wins {ml_wins}/{total}, OLS wins {ols_wins}/{total}\")\n",
        "            else:\n",
        "                print(f\"\\nNo DM data for {REGIME_SHORT[regime]} market.\")\n",
        "\n",
        "    if nonlin_summary.empty:\n",
        "        print(\"No non-linearity summary data available for high-level conclusions.\")\n",
        "    else:\n",
        "        nl_bear = nonlin_summary[nonlin_summary['Regime'] == 'Bear (Bot 25%)']\n",
        "        if not nl_bear.empty:\n",
        "            strong  = (nl_bear['reject_count'] >= 3).sum()\n",
        "            print(f\"\\nNon-linearity in Bear regime: {strong}/{len(nl_bear)} stocks show strong evidence (≥3/4 tests)\")\n",
        "        else:\n",
        "            print(\"\\nNo non-linearity data for Bear regime.\")\n",
        "\n",
        "    if regime_summary.empty:\n",
        "        print(\"No regime summary data available for high-level conclusions.\")\n",
        "    else:\n",
        "        bear_regime_summary = regime_summary[regime_summary['Regime'] == 'Bear (Bot 25%)']\n",
        "        if not bear_regime_summary.empty:\n",
        "            cat_mse = bear_regime_summary.groupby('Category')[[f'{m}_MSE' for m in MODEL_NAMES]].mean()\n",
        "            cat_mse['Best'] = cat_mse.idxmin(axis=1).str.replace('_MSE','')\n",
        "            print(\"\\nBest model by category in Bear regime (lowest avg MSE):\")\n",
        "            print(cat_mse[['Best']].to_string())\n",
        "        else:\n",
        "            print(\"\\nNo regime summary data for Bear regime.\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"ANALYSIS COMPLETE\")\n",
        "    print('='*70)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGGpBZD-XG5L",
        "outputId": "8b225b95-65af-43a2-8baa-6860269256a9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "Downloading data: 2015-01-01 → 2025-09-30\n",
            "======================================================================\n",
            "Price data: 2701 days, 20 tickers\n",
            "Available: ['NVDA', 'TSLA', 'META', 'AMZN', 'BRK-B', 'JPM', 'BAC', 'WMT', 'JNJ', 'PG', 'KO', 'PFE', 'CAT', 'FCX', 'F', 'XOM', 'T', 'VZ', 'MO', 'D']\n",
            "\n",
            "======================================================================\n",
            "RUNNING CROSS-SECTION ANALYSIS\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "CATEGORY: High-Beta Growth (Mag7)\n",
            "======================================================================\n",
            "\n",
            "  ────────────────────────────────────────────────────────────\n",
            "  NVDA  (N=2700)\n",
            "  ────────────────────────────────────────────────────────────\n",
            "  Bear  : OLS R²=+0.310  RF R²=+0.513  SVR R²=+0.322  GBR R²=+0.479  → best MSE: RF\n",
            "  Normal: OLS R²=+0.038  RF R²=+0.109  SVR R²=+0.036  GBR R²=+0.062  → best MSE: RF\n",
            "  Bull  : OLS R²=+0.251  RF R²=+0.412  SVR R²=+0.259  GBR R²=+0.366  → best MSE: RF\n",
            "\n",
            "  ────────────────────────────────────────────────────────────\n",
            "  TSLA  (N=2700)\n",
            "  ────────────────────────────────────────────────────────────\n",
            "  Bear  : OLS R²=+0.188  RF R²=+0.390  SVR R²=+0.186  GBR R²=+0.318  → best MSE: RF\n",
            "  Normal: OLS R²=+0.030  RF R²=+0.118  SVR R²=+0.026  GBR R²=+0.076  → best MSE: RF\n",
            "  Bull  : OLS R²=+0.079  RF R²=+0.320  SVR R²=+0.113  GBR R²=+0.274  → best MSE: RF\n",
            "\n",
            "  ────────────────────────────────────────────────────────────\n",
            "  META  (N=2700)\n",
            "  ────────────────────────────────────────────────────────────\n",
            "  Bear  : OLS R²=+0.246  RF R²=+0.457  SVR R²=+0.256  GBR R²=+0.437  → best MSE: RF\n",
            "  Normal: OLS R²=+0.039  RF R²=+0.111  SVR R²=+0.040  GBR R²=+0.053  → best MSE: RF\n",
            "  Bull  : OLS R²=+0.191  RF R²=+0.374  SVR R²=+0.190  GBR R²=+0.337  → best MSE: RF\n",
            "\n",
            "  ────────────────────────────────────────────────────────────\n",
            "  AMZN  (N=2700)\n",
            "  ────────────────────────────────────────────────────────────\n",
            "  Bear  : OLS R²=+0.247  RF R²=+0.481  SVR R²=+0.306  GBR R²=+0.391  → best MSE: RF\n",
            "  Normal: OLS R²=+0.069  RF R²=+0.134  SVR R²=+0.068  GBR R²=+0.086  → best MSE: RF\n",
            "  Bull  : OLS R²=+0.179  RF R²=+0.366  SVR R²=+0.196  GBR R²=+0.375  → best MSE: GBR\n",
            "\n",
            "======================================================================\n",
            "CATEGORY: Value\n",
            "======================================================================\n",
            "\n",
            "  ────────────────────────────────────────────────────────────\n",
            "  BRK-B  (N=2700)\n",
            "  ────────────────────────────────────────────────────────────\n",
            "  Bear  : OLS R²=+0.478  RF R²=+0.629  SVR R²=+0.506  GBR R²=+0.587  → best MSE: RF\n",
            "  Normal: OLS R²=+0.071  RF R²=+0.153  SVR R²=+0.073  GBR R²=+0.109  → best MSE: RF\n",
            "  Bull  : OLS R²=+0.445  RF R²=+0.584  SVR R²=+0.490  GBR R²=+0.589  → best MSE: GBR\n",
            "\n",
            "  ────────────────────────────────────────────────────────────\n",
            "  JPM  (N=2700)\n",
            "  ────────────────────────────────────────────────────────────\n",
            "  Bear  : OLS R²=+0.473  RF R²=+0.620  SVR R²=+0.500  GBR R²=+0.598  → best MSE: RF\n",
            "  Normal: OLS R²=+0.040  RF R²=+0.101  SVR R²=+0.045  GBR R²=+0.066  → best MSE: RF\n",
            "  Bull  : OLS R²=+0.348  RF R²=+0.514  SVR R²=+0.369  GBR R²=+0.531  → best MSE: GBR\n",
            "\n",
            "  ────────────────────────────────────────────────────────────\n",
            "  BAC  (N=2700)\n",
            "  ────────────────────────────────────────────────────────────\n",
            "  Bear  : OLS R²=+0.448  RF R²=+0.615  SVR R²=+0.461  GBR R²=+0.591  → best MSE: RF\n",
            "  Normal: OLS R²=+0.053  RF R²=+0.117  SVR R²=+0.053  GBR R²=+0.080  → best MSE: RF\n",
            "  Bull  : OLS R²=+0.326  RF R²=+0.493  SVR R²=+0.330  GBR R²=+0.474  → best MSE: RF\n",
            "\n",
            "  ────────────────────────────────────────────────────────────\n",
            "  WMT  (N=2700)\n",
            "  ────────────────────────────────────────────────────────────\n",
            "  Bear  : OLS R²=+0.119  RF R²=+0.364  SVR R²=+0.147  GBR R²=+0.302  → best MSE: RF\n",
            "  Normal: OLS R²=+0.010  RF R²=+0.083  SVR R²=+0.010  GBR R²=+0.035  → best MSE: RF\n",
            "  Bull  : OLS R²=+0.140  RF R²=+0.363  SVR R²=+0.146  GBR R²=+0.362  → best MSE: RF\n",
            "\n",
            "======================================================================\n",
            "CATEGORY: Defensive\n",
            "======================================================================\n",
            "\n",
            "  ────────────────────────────────────────────────────────────\n",
            "  JNJ  (N=2700)\n",
            "  ────────────────────────────────────────────────────────────\n",
            "  Bear  : OLS R²=+0.158  RF R²=+0.408  SVR R²=+0.179  GBR R²=+0.363  → best MSE: RF\n",
            "  Normal: OLS R²=+0.002  RF R²=+0.068  SVR R²=+0.007  GBR R²=+0.022  → best MSE: RF\n",
            "  Bull  : OLS R²=+0.200  RF R²=+0.421  SVR R²=+0.271  GBR R²=+0.447  → best MSE: GBR\n",
            "\n",
            "  ────────────────────────────────────────────────────────────\n",
            "  PG  (N=2700)\n",
            "  ────────────────────────────────────────────────────────────\n",
            "  Bear  : OLS R²=+0.189  RF R²=+0.476  SVR R²=+0.231  GBR R²=+0.487  → best MSE: GBR\n",
            "  Normal: OLS R²=-0.001  RF R²=+0.069  SVR R²=+0.004  GBR R²=+0.017  → best MSE: RF\n",
            "  Bull  : OLS R²=+0.223  RF R²=+0.457  SVR R²=+0.342  GBR R²=+0.462  → best MSE: GBR\n",
            "\n",
            "  ────────────────────────────────────────────────────────────\n",
            "  KO  (N=2700)\n",
            "  ────────────────────────────────────────────────────────────\n",
            "  Bear  : OLS R²=+0.271  RF R²=+0.543  SVR R²=+0.351  GBR R²=+0.553  → best MSE: GBR\n",
            "  Normal: OLS R²=+0.002  RF R²=+0.070  SVR R²=+0.007  GBR R²=+0.020  → best MSE: RF\n",
            "  Bull  : OLS R²=+0.178  RF R²=+0.413  SVR R²=+0.222  GBR R²=+0.367  → best MSE: RF\n",
            "\n",
            "  ────────────────────────────────────────────────────────────\n",
            "  PFE  (N=2700)\n",
            "  ────────────────────────────────────────────────────────────\n",
            "  Bear  : OLS R²=+0.196  RF R²=+0.427  SVR R²=+0.218  GBR R²=+0.365  → best MSE: RF\n",
            "  Normal: OLS R²=+0.012  RF R²=+0.072  SVR R²=+0.010  GBR R²=+0.026  → best MSE: RF\n",
            "  Bull  : OLS R²=+0.137  RF R²=+0.370  SVR R²=+0.163  GBR R²=+0.332  → best MSE: RF\n",
            "\n",
            "======================================================================\n",
            "CATEGORY: Cyclical\n",
            "======================================================================\n",
            "\n",
            "  ────────────────────────────────────────────────────────────\n",
            "  CAT  (N=2700)\n",
            "  ────────────────────────────────────────────────────────────\n",
            "  Bear  : OLS R²=+0.269  RF R²=+0.460  SVR R²=+0.302  GBR R²=+0.406  → best MSE: RF\n",
            "  Normal: OLS R²=+0.077  RF R²=+0.153  SVR R²=+0.082  GBR R²=+0.106  → best MSE: RF\n",
            "  Bull  : OLS R²=+0.216  RF R²=+0.441  SVR R²=+0.239  GBR R²=+0.427  → best MSE: RF\n",
            "\n",
            "  ────────────────────────────────────────────────────────────\n",
            "  FCX  (N=2700)\n",
            "  ────────────────────────────────────────────────────────────\n",
            "  Bear  : OLS R²=+0.257  RF R²=+0.444  SVR R²=+0.270  GBR R²=+0.365  → best MSE: RF\n",
            "  Normal: OLS R²=+0.053  RF R²=+0.130  SVR R²=+0.058  GBR R²=+0.098  → best MSE: RF\n",
            "  Bull  : OLS R²=+0.144  RF R²=+0.394  SVR R²=+0.132  GBR R²=+0.406  → best MSE: GBR\n",
            "\n",
            "  ────────────────────────────────────────────────────────────\n",
            "  F  (N=2700)\n",
            "  ────────────────────────────────────────────────────────────\n",
            "  Bear  : OLS R²=+0.261  RF R²=+0.466  SVR R²=+0.266  GBR R²=+0.393  → best MSE: RF\n",
            "  Normal: OLS R²=+0.022  RF R²=+0.106  SVR R²=+0.024  GBR R²=+0.053  → best MSE: RF\n",
            "  Bull  : OLS R²=+0.174  RF R²=+0.381  SVR R²=+0.193  GBR R²=+0.417  → best MSE: GBR\n",
            "\n",
            "  ────────────────────────────────────────────────────────────\n",
            "  XOM  (N=2700)\n",
            "  ────────────────────────────────────────────────────────────\n",
            "  Bear  : OLS R²=+0.286  RF R²=+0.535  SVR R²=+0.333  GBR R²=+0.485  → best MSE: RF\n",
            "  Normal: OLS R²=+0.029  RF R²=+0.123  SVR R²=+0.028  GBR R²=+0.072  → best MSE: RF\n",
            "  Bull  : OLS R²=+0.145  RF R²=+0.375  SVR R²=+0.145  GBR R²=+0.356  → best MSE: RF\n",
            "\n",
            "======================================================================\n",
            "CATEGORY: Income/Dividend\n",
            "======================================================================\n",
            "\n",
            "  ────────────────────────────────────────────────────────────\n",
            "  T  (N=2700)\n",
            "  ────────────────────────────────────────────────────────────\n",
            "  Bear  : OLS R²=+0.169  RF R²=+0.437  SVR R²=+0.214  GBR R²=+0.368  → best MSE: RF\n",
            "  Normal: OLS R²=+0.010  RF R²=+0.087  SVR R²=+0.011  GBR R²=+0.042  → best MSE: RF\n",
            "  Bull  : OLS R²=+0.171  RF R²=+0.388  SVR R²=+0.242  GBR R²=+0.365  → best MSE: RF\n",
            "\n",
            "  ────────────────────────────────────────────────────────────\n",
            "  VZ  (N=2700)\n",
            "  ────────────────────────────────────────────────────────────\n",
            "  Bear  : OLS R²=+0.116  RF R²=+0.368  SVR R²=+0.144  GBR R²=+0.312  → best MSE: RF\n",
            "  Normal: OLS R²=+0.005  RF R²=+0.077  SVR R²=+0.006  GBR R²=+0.036  → best MSE: RF\n",
            "  Bull  : OLS R²=+0.097  RF R²=+0.338  SVR R²=+0.150  GBR R²=+0.321  → best MSE: RF\n",
            "\n",
            "  ────────────────────────────────────────────────────────────\n",
            "  MO  (N=2700)\n",
            "  ────────────────────────────────────────────────────────────\n",
            "  Bear  : OLS R²=+0.138  RF R²=+0.388  SVR R²=+0.178  GBR R²=+0.323  → best MSE: RF\n",
            "  Normal: OLS R²=+0.002  RF R²=+0.084  SVR R²=+0.005  GBR R²=+0.035  → best MSE: RF\n",
            "  Bull  : OLS R²=+0.117  RF R²=+0.319  SVR R²=+0.170  GBR R²=+0.329  → best MSE: GBR\n",
            "\n",
            "  ────────────────────────────────────────────────────────────\n",
            "  D  (N=2700)\n",
            "  ────────────────────────────────────────────────────────────\n",
            "  Bear  : OLS R²=+0.149  RF R²=+0.489  SVR R²=+0.214  GBR R²=+0.439  → best MSE: RF\n",
            "  Normal: OLS R²=+0.005  RF R²=+0.078  SVR R²=+0.008  GBR R²=+0.032  → best MSE: RF\n",
            "  Bull  : OLS R²=+0.180  RF R²=+0.473  SVR R²=+0.295  GBR R²=+0.458  → best MSE: RF\n",
            "\n",
            "======================================================================\n",
            "WITHIN-REGIME MSE SUMMARY (Full Cross-Section)\n",
            "======================================================================\n",
            "\n",
            "── Full Sample ──\n",
            "               Category Ticker  OLS_MSE   RF_MSE  SVR_MSE  GBR_MSE Best\n",
            "High-Beta Growth (Mag7)   NVDA 5.415798 4.495614 5.380018 4.779757   RF\n",
            "High-Beta Growth (Mag7)   TSLA 9.888815 8.052141 9.795799 8.615761   RF\n",
            "High-Beta Growth (Mag7)   META 3.460628 2.794418 3.447289 2.948636   RF\n",
            "High-Beta Growth (Mag7)   AMZN 2.509412 1.982998 2.422421 2.123278   RF\n",
            "                  Value  BRK-B 0.630825 0.500110 0.602761 0.526680   RF\n",
            "                  Value    JPM 1.403024 1.128155 1.363141 1.148588   RF\n",
            "                  Value    BAC 1.883132 1.513944 1.866313 1.583479   RF\n",
            "                  Value    WMT 1.487280 1.190968 1.468579 1.249904   RF\n",
            "              Defensive    JNJ 1.037515 0.822919 1.001130 0.850931   RF\n",
            "              Defensive     PG 1.057859 0.815996 0.989652 0.831409   RF\n",
            "              Defensive     KO 0.916101 0.700536 0.866932 0.729579   RF\n",
            "              Defensive    PFE 1.735536 1.411401 1.707901 1.505835   RF\n",
            "               Cyclical    CAT 2.032339 1.621602 1.980357 1.718777   RF\n",
            "               Cyclical    FCX 8.115653 6.601484 8.096872 6.897354   RF\n",
            "               Cyclical      F 3.302421 2.674781 3.270003 2.788726   RF\n",
            "               Cyclical    XOM 2.156415 1.659744 2.115409 1.761496   RF\n",
            "        Income/Dividend      T 1.639236 1.284853 1.569078 1.374958   RF\n",
            "        Income/Dividend     VZ 1.295680 1.043221 1.259810 1.096393   RF\n",
            "        Income/Dividend     MO 1.659753 1.339685 1.604970 1.409360   RF\n",
            "        Income/Dividend      D 1.757936 1.267821 1.626313 1.338372   RF\n",
            "\n",
            "  Category averages:\n",
            "                          OLS_MSE    RF_MSE   SVR_MSE   GBR_MSE Best\n",
            "Category                                                            \n",
            "Cyclical                 3.901707  3.139402  3.865660  3.291588   RF\n",
            "Defensive                1.186753  0.937713  1.141404  0.979438   RF\n",
            "High-Beta Growth (Mag7)  5.318663  4.331292  5.261382  4.616858   RF\n",
            "Income/Dividend          1.588151  1.233895  1.515043  1.304771   RF\n",
            "Value                    1.351065  1.083294  1.325199  1.127163   RF\n",
            "\n",
            "── Bear (Bot 25%) ──\n",
            "               Category Ticker  OLS_MSE   RF_MSE  SVR_MSE  GBR_MSE Best\n",
            "High-Beta Growth (Mag7)   NVDA 5.775654 4.071479 5.670935 4.360800   RF\n",
            "High-Beta Growth (Mag7)   TSLA 9.942612 7.465442 9.967001 8.352964   RF\n",
            "High-Beta Growth (Mag7)   META 4.556514 3.280898 4.496115 3.405371   RF\n",
            "High-Beta Growth (Mag7)   AMZN 3.669842 2.527524 3.383144 2.967249   RF\n",
            "                  Value  BRK-B 0.867804 0.616078 0.819903 0.685943   RF\n",
            "                  Value    JPM 1.560396 1.124061 1.480366 1.190158   RF\n",
            "                  Value    BAC 2.107250 1.468972 2.056212 1.562509   RF\n",
            "                  Value    WMT 1.926257 1.391387 1.865800 1.525874   RF\n",
            "              Defensive    JNJ 1.483591 1.043031 1.446580 1.122180   RF\n",
            "              Defensive     PG 1.320556 0.852852 1.252448 0.835791  GBR\n",
            "              Defensive     KO 1.169536 0.733271 1.042355 0.717848  GBR\n",
            "              Defensive    PFE 1.983828 1.414365 1.929436 1.564847   RF\n",
            "               Cyclical    CAT 2.619669 1.934215 2.499645 2.128969   RF\n",
            "               Cyclical    FCX 7.940219 5.940065 7.802994 6.781517   RF\n",
            "               Cyclical      F 3.499101 2.527024 3.476634 2.873282   RF\n",
            "               Cyclical    XOM 2.524180 1.641020 2.355428 1.817917   RF\n",
            "        Income/Dividend      T 2.400415 1.626355 2.270401 1.826264   RF\n",
            "        Income/Dividend     VZ 1.542089 1.103390 1.493198 1.201119   RF\n",
            "        Income/Dividend     MO 2.293716 1.630067 2.188081 1.802721   RF\n",
            "        Income/Dividend      D 2.292456 1.378211 2.117576 1.511888   RF\n",
            "\n",
            "  Category averages:\n",
            "                          OLS_MSE    RF_MSE   SVR_MSE   GBR_MSE Best\n",
            "Category                                                            \n",
            "Cyclical                 4.145792  3.010581  4.033675  3.400421   RF\n",
            "Defensive                1.489378  1.010880  1.417705  1.060166   RF\n",
            "High-Beta Growth (Mag7)  5.986156  4.336336  5.879299  4.771596   RF\n",
            "Income/Dividend          2.132169  1.434506  2.017314  1.585498   RF\n",
            "Value                    1.615427  1.150124  1.555570  1.241121   RF\n",
            "\n",
            "── Normal (Mid 50%) ──\n",
            "               Category Ticker  OLS_MSE   RF_MSE  SVR_MSE  GBR_MSE Best\n",
            "High-Beta Growth (Mag7)   NVDA 5.071793 4.697751 5.084390 4.945968   RF\n",
            "High-Beta Growth (Mag7)   TSLA 8.418811 7.653119 8.456435 8.020910   RF\n",
            "High-Beta Growth (Mag7)   META 2.324930 2.152337 2.324453 2.293119   RF\n",
            "High-Beta Growth (Mag7)   AMZN 1.528470 1.422573 1.531339 1.501732   RF\n",
            "                  Value  BRK-B 0.443569 0.404494 0.442791 0.425615   RF\n",
            "                  Value    JPM 0.966364 0.904975 0.961128 0.940637   RF\n",
            "                  Value    BAC 1.406179 1.310194 1.405731 1.365436   RF\n",
            "                  Value    WMT 1.052589 0.975300 1.052031 1.025550   RF\n",
            "              Defensive    JNJ 0.757507 0.707466 0.754046 0.742288   RF\n",
            "              Defensive     PG 0.817548 0.759805 0.813291 0.802688   RF\n",
            "              Defensive     KO 0.658595 0.613485 0.655401 0.646662   RF\n",
            "              Defensive    PFE 1.459094 1.370654 1.461985 1.438850   RF\n",
            "               Cyclical    CAT 1.518242 1.393519 1.510358 1.469462   RF\n",
            "               Cyclical    FCX 7.333987 6.740365 7.296749 6.982281   RF\n",
            "               Cyclical      F 2.715709 2.482314 2.711769 2.630931   RF\n",
            "               Cyclical    XOM 1.559509 1.408669 1.561354 1.490317   RF\n",
            "        Income/Dividend      T 1.208190 1.114400 1.207106 1.169950   RF\n",
            "        Income/Dividend     VZ 1.029201 0.954629 1.028365 0.997418   RF\n",
            "        Income/Dividend     MO 1.281170 1.176095 1.278021 1.239722   RF\n",
            "        Income/Dividend      D 1.136895 1.054418 1.134222 1.106351   RF\n",
            "\n",
            "  Category averages:\n",
            "                          OLS_MSE    RF_MSE   SVR_MSE   GBR_MSE Best\n",
            "Category                                                            \n",
            "Cyclical                 3.281862  3.006217  3.270057  3.143248   RF\n",
            "Defensive                0.923186  0.862853  0.921181  0.907622   RF\n",
            "High-Beta Growth (Mag7)  4.336001  3.981445  4.349154  4.190432   RF\n",
            "Income/Dividend          1.163864  1.074885  1.161928  1.128360   RF\n",
            "Value                    0.967175  0.898741  0.965420  0.939309   RF\n",
            "\n",
            "── Bull (Top 25%) ──\n",
            "               Category Ticker   OLS_MSE   RF_MSE   SVR_MSE   GBR_MSE Best\n",
            "High-Beta Growth (Mag7)   NVDA  5.744994 4.514544  5.681226  4.865428   RF\n",
            "High-Beta Growth (Mag7)   TSLA 12.777280 9.436606 12.305563 10.068755   RF\n",
            "High-Beta Growth (Mag7)   META  4.639444 3.593771  4.647352  3.804582   RF\n",
            "High-Beta Growth (Mag7)   AMZN  3.314040 2.560957  3.246607  2.524571  GBR\n",
            "                  Value  BRK-B  0.768985 0.575687  0.706119  0.569932  GBR\n",
            "                  Value    JPM  2.119854 1.578935  2.050711  1.523292  GBR\n",
            "                  Value    BAC  2.613959 1.966653  2.598545  2.040829   RF\n",
            "                  Value    WMT  1.918978 1.422504  1.905658  1.423383   RF\n",
            "              Defensive    JNJ  1.152530 0.834209  1.050874  0.797529  GBR\n",
            "              Defensive     PG  1.276529 0.891661  1.080227  0.884517  GBR\n",
            "              Defensive     KO  1.178435 0.842082  1.115144  0.907249   RF\n",
            "              Defensive    PFE  2.040905 1.489998  1.978889  1.580978   RF\n",
            "               Cyclical    CAT  2.474834 1.765954  2.402532  1.808192   RF\n",
            "               Cyclical    FCX  9.855318 6.983953  9.991747  6.843040  GBR\n",
            "               Cyclical      F  4.280328 3.207536  4.180974  3.020120  GBR\n",
            "               Cyclical    XOM  2.983889 2.180962  2.984676  2.247921   RF\n",
            "        Income/Dividend      T  1.741913 1.285018  1.593275  1.334641   RF\n",
            "        Income/Dividend     VZ  1.582987 1.160456  1.490001  1.189918   RF\n",
            "        Income/Dividend     MO  1.784456 1.377154  1.677105  1.356111  GBR\n",
            "        Income/Dividend      D  2.467211 1.584716  2.120690  1.629500   RF\n",
            "\n",
            "  Category averages:\n",
            "                          OLS_MSE    RF_MSE   SVR_MSE   GBR_MSE Best\n",
            "Category                                                            \n",
            "Cyclical                 4.898592  3.534601  4.889982  3.479818  GBR\n",
            "Defensive                1.412100  1.014487  1.306283  1.042568   RF\n",
            "High-Beta Growth (Mag7)  6.618940  5.026470  6.470187  5.315834   RF\n",
            "Income/Dividend          1.894142  1.351836  1.720268  1.377542   RF\n",
            "Value                    1.855444  1.385945  1.815258  1.389359   RF\n",
            "\n",
            "======================================================================\n",
            "NON-LINEARITY EVIDENCE BY REGIME AND STOCK TYPE\n",
            "======================================================================\n",
            "Regime                          Bear (Bot 25%)  Bull (Top 25%)  Full Sample  Normal (Mid 50%)      verdict\n",
            "Category                Ticker                                                                            \n",
            "Cyclical                CAT                  2               1            2                 2   🟡 MODERATE\n",
            "                        F                    0               2            1                 0  🟢 WEAK/NONE\n",
            "                        FCX                  2               4            2                 1   🟡 MODERATE\n",
            "                        XOM                  3               2            2                 0   🟡 MODERATE\n",
            "Defensive               JNJ                  2               3            2                 0   🟡 MODERATE\n",
            "                        KO                   3               3            2                 0   🟡 MODERATE\n",
            "                        PFE                  2               3            1                 0  🟢 WEAK/NONE\n",
            "                        PG                   3               3            2                 0   🟡 MODERATE\n",
            "High-Beta Growth (Mag7) AMZN                 3               2            3                 1     🔴 STRONG\n",
            "                        META                 2               1            0                 0  🟢 WEAK/NONE\n",
            "                        NVDA                 3               2            2                 0   🟡 MODERATE\n",
            "                        TSLA                 1               3            2                 0   🟡 MODERATE\n",
            "Income/Dividend         D                    2               3            2                 2   🟡 MODERATE\n",
            "                        MO                   3               2            2                 0   🟡 MODERATE\n",
            "                        T                    2               3            2                 0   🟡 MODERATE\n",
            "                        VZ                   2               3            2                 0   🟡 MODERATE\n",
            "Value                   BAC                  2               2            1                 1  🟢 WEAK/NONE\n",
            "                        BRK-B                3               2            2                 0   🟡 MODERATE\n",
            "                        JPM                  3               2            2                 0   🟡 MODERATE\n",
            "                        WMT                  1               3            2                 1   🟡 MODERATE\n",
            "\n",
            "======================================================================\n",
            "DM TEST: OLS vs ML MODELS — WITHIN REGIME\n",
            "Positive DM → OLS has larger squared errors → ML better\n",
            "======================================================================\n",
            "\n",
            "── Full Sample ──\n",
            "\n",
            "  OLS vs RF:  ML wins=20  OLS wins=0  Ties=0\n",
            "               Category Ticker  dm_stat      p_value winner sig\n",
            "High-Beta Growth (Mag7)   NVDA 7.735539 1.443290e-14     RF ***\n",
            "High-Beta Growth (Mag7)   TSLA 6.815849 1.151546e-11     RF ***\n",
            "High-Beta Growth (Mag7)   META 3.942406 8.271868e-05     RF ***\n",
            "High-Beta Growth (Mag7)   AMZN 5.293444 1.296831e-07     RF ***\n",
            "                  Value  BRK-B 7.763777 1.154632e-14     RF ***\n",
            "                  Value    JPM 5.570350 2.793252e-08     RF ***\n",
            "                  Value    BAC 7.143520 1.165290e-12     RF ***\n",
            "                  Value    WMT 5.177246 2.417500e-07     RF ***\n",
            "              Defensive    JNJ 5.645794 1.815629e-08     RF ***\n",
            "              Defensive     PG 4.986959 6.522485e-07     RF ***\n",
            "              Defensive     KO 6.023826 1.935017e-09     RF ***\n",
            "              Defensive    PFE 7.407964 1.705303e-13     RF ***\n",
            "               Cyclical    CAT 7.754841 1.243450e-14     RF ***\n",
            "               Cyclical    FCX 6.474913 1.123077e-10     RF ***\n",
            "               Cyclical      F 6.961703 4.205081e-12     RF ***\n",
            "               Cyclical    XOM 7.727910 1.532108e-14     RF ***\n",
            "        Income/Dividend      T 7.240151 5.817569e-13     RF ***\n",
            "        Income/Dividend     VZ 7.654693 2.686740e-14     RF ***\n",
            "        Income/Dividend     MO 7.332931 2.962075e-13     RF ***\n",
            "        Income/Dividend      D 3.985873 6.901467e-05     RF ***\n",
            "\n",
            "  OLS vs SVR:  ML wins=7  OLS wins=0  Ties=13\n",
            "               Category Ticker  dm_stat  p_value winner sig\n",
            "High-Beta Growth (Mag7)   NVDA 1.402548 0.160867    Tie    \n",
            "High-Beta Growth (Mag7)   TSLA 0.642064 0.520886    Tie    \n",
            "High-Beta Growth (Mag7)   META 0.494415 0.621053    Tie    \n",
            "High-Beta Growth (Mag7)   AMZN 1.494341 0.135203    Tie    \n",
            "                  Value  BRK-B 2.297878 0.021645    SVR  **\n",
            "                  Value    JPM 1.641726 0.100763    Tie    \n",
            "                  Value    BAC 0.838006 0.402101    Tie    \n",
            "                  Value    WMT 0.718205 0.472693    Tie    \n",
            "              Defensive    JNJ 2.300199 0.021513    SVR  **\n",
            "              Defensive     PG 2.202295 0.027729    SVR  **\n",
            "              Defensive     KO 2.429247 0.015195    SVR  **\n",
            "              Defensive    PFE 2.014164 0.044091    SVR  **\n",
            "               Cyclical    CAT 1.920393 0.054914    Tie   *\n",
            "               Cyclical    FCX 0.219199 0.826512    Tie    \n",
            "               Cyclical      F 0.907645 0.364147    Tie    \n",
            "               Cyclical    XOM 1.387554 0.165387    Tie    \n",
            "        Income/Dividend      T 2.850106 0.004404    SVR ***\n",
            "        Income/Dividend     VZ 1.874049 0.061032    Tie   *\n",
            "        Income/Dividend     MO 1.772939 0.076352    Tie   *\n",
            "        Income/Dividend      D 2.031085 0.042344    SVR  **\n",
            "\n",
            "  OLS vs GBR:  ML wins=20  OLS wins=0  Ties=0\n",
            "               Category Ticker  dm_stat      p_value winner sig\n",
            "High-Beta Growth (Mag7)   NVDA 8.176759 4.440892e-16    GBR ***\n",
            "High-Beta Growth (Mag7)   TSLA 5.388262 7.728345e-08    GBR ***\n",
            "High-Beta Growth (Mag7)   META 3.258664 1.133270e-03    GBR ***\n",
            "High-Beta Growth (Mag7)   AMZN 4.412323 1.062779e-05    GBR ***\n",
            "                  Value  BRK-B 5.873807 4.780953e-09    GBR ***\n",
            "                  Value    JPM 4.307498 1.710106e-05    GBR ***\n",
            "                  Value    BAC 6.104184 1.181683e-09    GBR ***\n",
            "                  Value    WMT 3.817337 1.379322e-04    GBR ***\n",
            "              Defensive    JNJ 4.255490 2.157028e-05    GBR ***\n",
            "              Defensive     PG 3.608147 3.139922e-04    GBR ***\n",
            "              Defensive     KO 5.308913 1.192504e-07    GBR ***\n",
            "              Defensive    PFE 6.884380 7.190915e-12    GBR ***\n",
            "               Cyclical    CAT 5.996346 2.287318e-09    GBR ***\n",
            "               Cyclical    FCX 4.503159 6.979305e-06    GBR ***\n",
            "               Cyclical      F 4.115089 3.986242e-05    GBR ***\n",
            "               Cyclical    XOM 6.129195 1.012281e-09    GBR ***\n",
            "        Income/Dividend      T 6.298699 3.492882e-10    GBR ***\n",
            "        Income/Dividend     VZ 6.258664 4.501783e-10    GBR ***\n",
            "        Income/Dividend     MO 5.553485 3.073356e-08    GBR ***\n",
            "        Income/Dividend      D 3.160555 1.592132e-03    GBR ***\n",
            "\n",
            "── Bear (Bot 25%) ──\n",
            "\n",
            "  OLS vs RF:  ML wins=20  OLS wins=0  Ties=0\n",
            "               Category Ticker  dm_stat      p_value winner sig\n",
            "High-Beta Growth (Mag7)   NVDA 6.270157 6.447261e-10     RF ***\n",
            "High-Beta Growth (Mag7)   TSLA 4.601489 5.013821e-06     RF ***\n",
            "High-Beta Growth (Mag7)   META 2.242380 2.526167e-02     RF  **\n",
            "High-Beta Growth (Mag7)   AMZN 3.857106 1.257590e-04     RF ***\n",
            "                  Value  BRK-B 4.904463 1.175826e-06     RF ***\n",
            "                  Value    JPM 5.086857 4.726681e-07     RF ***\n",
            "                  Value    BAC 6.443582 2.223621e-10     RF ***\n",
            "                  Value    WMT 4.412894 1.187668e-05     RF ***\n",
            "              Defensive    JNJ 4.471004 9.135540e-06     RF ***\n",
            "              Defensive     PG 4.361987 1.490993e-05     RF ***\n",
            "              Defensive     KO 4.626096 4.469986e-06     RF ***\n",
            "              Defensive    PFE 4.777737 2.177657e-06     RF ***\n",
            "               Cyclical    CAT 5.195270 2.712824e-07     RF ***\n",
            "               Cyclical    FCX 5.125693 3.878628e-07     RF ***\n",
            "               Cyclical      F 5.700821 1.784866e-08     RF ***\n",
            "               Cyclical    XOM 5.673038 2.084429e-08     RF ***\n",
            "        Income/Dividend      T 4.901591 1.192543e-06     RF ***\n",
            "        Income/Dividend     VZ 5.357284 1.161273e-07     RF ***\n",
            "        Income/Dividend     MO 4.804019 1.918571e-06     RF ***\n",
            "        Income/Dividend      D 4.375835 1.401852e-05     RF ***\n",
            "\n",
            "  OLS vs SVR:  ML wins=4  OLS wins=0  Ties=16\n",
            "               Category Ticker   dm_stat  p_value winner sig\n",
            "High-Beta Growth (Mag7)   NVDA  1.414074 0.157803    Tie    \n",
            "High-Beta Growth (Mag7)   TSLA -0.261003 0.794170    Tie    \n",
            "High-Beta Growth (Mag7)   META  1.141427 0.254098    Tie    \n",
            "High-Beta Growth (Mag7)   AMZN  1.562090 0.118737    Tie    \n",
            "                  Value  BRK-B  2.341297 0.019508    SVR  **\n",
            "                  Value    JPM  1.511659 0.131090    Tie    \n",
            "                  Value    BAC  1.318694 0.187720    Tie    \n",
            "                  Value    WMT  0.951361 0.341763    Tie    \n",
            "              Defensive    JNJ  1.041085 0.298210    Tie    \n",
            "              Defensive     PG  2.042186 0.041522    SVR  **\n",
            "              Defensive     KO  1.745485 0.081357    Tie   *\n",
            "              Defensive    PFE  1.887700 0.059496    Tie   *\n",
            "               Cyclical    CAT  1.257392 0.209048    Tie    \n",
            "               Cyclical    FCX  1.170130 0.242363    Tie    \n",
            "               Cyclical      F  0.935929 0.349646    Tie    \n",
            "               Cyclical    XOM  1.803563 0.071747    Tie   *\n",
            "        Income/Dividend      T  2.547062 0.011085    SVR  **\n",
            "        Income/Dividend     VZ  1.978663 0.048261    SVR  **\n",
            "        Income/Dividend     MO  1.313100 0.189597    Tie    \n",
            "        Income/Dividend      D  1.759770 0.078901    Tie   *\n",
            "\n",
            "  OLS vs GBR:  ML wins=20  OLS wins=0  Ties=0\n",
            "               Category Ticker  dm_stat      p_value winner sig\n",
            "High-Beta Growth (Mag7)   NVDA 5.992171 3.373852e-09    GBR ***\n",
            "High-Beta Growth (Mag7)   TSLA 4.858268 1.474367e-06    GBR ***\n",
            "High-Beta Growth (Mag7)   META 1.975333 4.863855e-02    GBR  **\n",
            "High-Beta Growth (Mag7)   AMZN 2.877355 4.137295e-03    GBR ***\n",
            "                  Value  BRK-B 4.578791 5.571305e-06    GBR ***\n",
            "                  Value    JPM 3.759768 1.848567e-04    GBR ***\n",
            "                  Value    BAC 5.543433 4.261827e-08    GBR ***\n",
            "                  Value    WMT 3.550838 4.107751e-04    GBR ***\n",
            "              Defensive    JNJ 3.575271 3.749045e-04    GBR ***\n",
            "              Defensive     PG 3.299997 1.017854e-03    GBR ***\n",
            "              Defensive     KO 3.777122 1.726931e-04    GBR ***\n",
            "              Defensive    PFE 5.103877 4.335001e-07    GBR ***\n",
            "               Cyclical    CAT 3.051512 2.366510e-03    GBR ***\n",
            "               Cyclical    FCX 5.272489 1.815546e-07    GBR ***\n",
            "               Cyclical      F 5.001713 7.258776e-07    GBR ***\n",
            "               Cyclical    XOM 4.897345 1.217685e-06    GBR ***\n",
            "        Income/Dividend      T 4.805986 1.900427e-06    GBR ***\n",
            "        Income/Dividend     VZ 4.869953 1.392601e-06    GBR ***\n",
            "        Income/Dividend     MO 4.193968 3.108045e-05    GBR ***\n",
            "        Income/Dividend      D 3.780260 1.705761e-04    GBR ***\n",
            "\n",
            "── Normal (Mid 50%) ──\n",
            "\n",
            "  OLS vs RF:  ML wins=20  OLS wins=0  Ties=0\n",
            "               Category Ticker  dm_stat  p_value winner sig\n",
            "High-Beta Growth (Mag7)   NVDA 3.450451 0.000577     RF ***\n",
            "High-Beta Growth (Mag7)   TSLA 4.230046 0.000025     RF ***\n",
            "High-Beta Growth (Mag7)   META 2.415116 0.015862     RF  **\n",
            "High-Beta Growth (Mag7)   AMZN 3.444346 0.000590     RF ***\n",
            "                  Value  BRK-B 4.527604 0.000006     RF ***\n",
            "                  Value    JPM 4.063631 0.000051     RF ***\n",
            "                  Value    BAC 4.523059 0.000007     RF ***\n",
            "                  Value    WMT 3.526608 0.000435     RF ***\n",
            "              Defensive    JNJ 3.994797 0.000068     RF ***\n",
            "              Defensive     PG 3.643055 0.000280     RF ***\n",
            "              Defensive     KO 3.585631 0.000348     RF ***\n",
            "              Defensive    PFE 3.702336 0.000222     RF ***\n",
            "               Cyclical    CAT 4.222847 0.000026     RF ***\n",
            "               Cyclical    FCX 3.759632 0.000177     RF ***\n",
            "               Cyclical      F 3.509196 0.000464     RF ***\n",
            "               Cyclical    XOM 4.447309 0.000009     RF ***\n",
            "        Income/Dividend      T 4.378090 0.000013     RF ***\n",
            "        Income/Dividend     VZ 4.064519 0.000051     RF ***\n",
            "        Income/Dividend     MO 4.429259 0.000010     RF ***\n",
            "        Income/Dividend      D 4.038287 0.000057     RF ***\n",
            "\n",
            "  OLS vs SVR:  ML wins=0  OLS wins=1  Ties=19\n",
            "               Category Ticker   dm_stat  p_value winner sig\n",
            "High-Beta Growth (Mag7)   NVDA -0.994108 0.320349    Tie    \n",
            "High-Beta Growth (Mag7)   TSLA -2.700071 0.007019    OLS ***\n",
            "High-Beta Growth (Mag7)   META  0.094267 0.924911    Tie    \n",
            "High-Beta Growth (Mag7)   AMZN -1.226218 0.220330    Tie    \n",
            "                  Value  BRK-B  0.334135 0.738330    Tie    \n",
            "                  Value    JPM  1.464523 0.143284    Tie    \n",
            "                  Value    BAC  0.115799 0.907829    Tie    \n",
            "                  Value    WMT  0.338786 0.734824    Tie    \n",
            "              Defensive    JNJ  1.606924 0.108305    Tie    \n",
            "              Defensive     PG  1.631860 0.102942    Tie    \n",
            "              Defensive     KO  1.491030 0.136187    Tie    \n",
            "              Defensive    PFE -0.689821 0.490425    Tie    \n",
            "               Cyclical    CAT  1.160585 0.246016    Tie    \n",
            "               Cyclical    FCX  1.512670 0.130598    Tie    \n",
            "               Cyclical      F  0.687282 0.492023    Tie    \n",
            "               Cyclical    XOM -0.341990 0.732412    Tie    \n",
            "        Income/Dividend      T  0.606146 0.544520    Tie    \n",
            "        Income/Dividend     VZ  0.622739 0.533561    Tie    \n",
            "        Income/Dividend     MO  0.668256 0.504084    Tie    \n",
            "        Income/Dividend      D  0.823786 0.410206    Tie    \n",
            "\n",
            "  OLS vs GBR:  ML wins=20  OLS wins=0  Ties=0\n",
            "               Category Ticker  dm_stat      p_value winner sig\n",
            "High-Beta Growth (Mag7)   NVDA 4.971689 7.488777e-07    GBR ***\n",
            "High-Beta Growth (Mag7)   TSLA 6.397210 2.176563e-10    GBR ***\n",
            "High-Beta Growth (Mag7)   META 3.314999 9.407551e-04    GBR ***\n",
            "High-Beta Growth (Mag7)   AMZN 4.593276 4.770524e-06    GBR ***\n",
            "                  Value  BRK-B 5.269668 1.589339e-07    GBR ***\n",
            "                  Value    JPM 3.933294 8.804738e-05    GBR ***\n",
            "                  Value    BAC 4.455477 9.062735e-06    GBR ***\n",
            "                  Value    WMT 5.237643 1.884743e-07    GBR ***\n",
            "              Defensive    JNJ 4.506664 7.155151e-06    GBR ***\n",
            "              Defensive     PG 4.429130 1.022549e-05    GBR ***\n",
            "              Defensive     KO 3.135100 1.754997e-03    GBR ***\n",
            "              Defensive    PFE 4.273676 2.057678e-05    GBR ***\n",
            "               Cyclical    CAT 4.845086 1.411832e-06    GBR ***\n",
            "               Cyclical    FCX 5.417918 7.132026e-08    GBR ***\n",
            "               Cyclical      F 6.081497 1.547499e-09    GBR ***\n",
            "               Cyclical    XOM 5.377077 8.911452e-08    GBR ***\n",
            "        Income/Dividend      T 5.594822 2.670615e-08    GBR ***\n",
            "        Income/Dividend     VZ 6.043538 1.947341e-09    GBR ***\n",
            "        Income/Dividend     MO 4.417262 1.079468e-05    GBR ***\n",
            "        Income/Dividend      D 5.174562 2.629648e-07    GBR ***\n",
            "\n",
            "── Bull (Top 25%) ──\n",
            "\n",
            "  OLS vs RF:  ML wins=20  OLS wins=0  Ties=0\n",
            "               Category Ticker  dm_stat      p_value winner sig\n",
            "High-Beta Growth (Mag7)   NVDA 4.601730 5.006881e-06     RF ***\n",
            "High-Beta Growth (Mag7)   TSLA 3.809039 1.522480e-04     RF ***\n",
            "High-Beta Growth (Mag7)   META 3.071222 2.217630e-03     RF ***\n",
            "High-Beta Growth (Mag7)   AMZN 3.696681 2.361927e-04     RF ***\n",
            "                  Value  BRK-B 5.022684 6.532363e-07     RF ***\n",
            "                  Value    JPM 3.085301 2.116694e-03     RF ***\n",
            "                  Value    BAC 3.688006 2.442240e-04     RF ***\n",
            "                  Value    WMT 3.191230 1.482461e-03     RF ***\n",
            "              Defensive    JNJ 2.948193 3.306941e-03     RF ***\n",
            "              Defensive     PG 2.589856 9.809014e-03     RF ***\n",
            "              Defensive     KO 3.616240 3.212308e-04     RF ***\n",
            "              Defensive    PFE 4.390897 1.310407e-05     RF ***\n",
            "               Cyclical    CAT 4.782341 2.129324e-06     RF ***\n",
            "               Cyclical    FCX 3.525560 4.512063e-04     RF ***\n",
            "               Cyclical      F 3.765857 1.804774e-04     RF ***\n",
            "               Cyclical    XOM 4.333121 1.694177e-05     RF ***\n",
            "        Income/Dividend      T 4.867891 1.406250e-06     RF ***\n",
            "        Income/Dividend     VZ 4.647991 4.033042e-06     RF ***\n",
            "        Income/Dividend     MO 4.825139 1.731599e-06     RF ***\n",
            "        Income/Dividend      D 2.147377 3.211927e-02     RF  **\n",
            "\n",
            "  OLS vs SVR:  ML wins=0  OLS wins=0  Ties=20\n",
            "               Category Ticker   dm_stat  p_value winner sig\n",
            "High-Beta Growth (Mag7)   NVDA  1.054032 0.292246    Tie    \n",
            "High-Beta Growth (Mag7)   TSLA  0.736586 0.461630    Tie    \n",
            "High-Beta Growth (Mag7)   META -0.088183 0.929757    Tie    \n",
            "High-Beta Growth (Mag7)   AMZN  0.542375 0.587740    Tie    \n",
            "                  Value  BRK-B  1.464310 0.143576    Tie    \n",
            "                  Value    JPM  1.049118 0.294500    Tie    \n",
            "                  Value    BAC  0.252653 0.800614    Tie    \n",
            "                  Value    WMT  0.145484 0.884373    Tie    \n",
            "              Defensive    JNJ  1.383764 0.166889    Tie    \n",
            "              Defensive     PG  1.414574 0.157655    Tie    \n",
            "              Defensive     KO  1.626150 0.104385    Tie    \n",
            "              Defensive    PFE  1.154498 0.248705    Tie    \n",
            "               Cyclical    CAT  1.717340 0.086376    Tie   *\n",
            "               Cyclical    FCX -0.586009 0.558066    Tie    \n",
            "               Cyclical      F  0.762057 0.446293    Tie    \n",
            "               Cyclical    XOM -0.012452 0.990068    Tie    \n",
            "        Income/Dividend      T  1.817583 0.069572    Tie   *\n",
            "        Income/Dividend     VZ  1.232619 0.218148    Tie    \n",
            "        Income/Dividend     MO  1.300410 0.193905    Tie    \n",
            "        Income/Dividend      D  1.489952 0.136705    Tie    \n",
            "\n",
            "  OLS vs GBR:  ML wins=19  OLS wins=0  Ties=1\n",
            "               Category Ticker  dm_stat      p_value winner sig\n",
            "High-Beta Growth (Mag7)   NVDA 5.102123 4.372187e-07    GBR ***\n",
            "High-Beta Growth (Mag7)   TSLA 3.093002 2.063283e-03    GBR ***\n",
            "High-Beta Growth (Mag7)   META 3.825865 1.424208e-04    GBR ***\n",
            "High-Beta Growth (Mag7)   AMZN 3.261181 1.165375e-03    GBR ***\n",
            "                  Value  BRK-B 3.775057 1.740788e-04    GBR ***\n",
            "                  Value    JPM 3.027214 2.562328e-03    GBR ***\n",
            "                  Value    BAC 3.698966 2.341189e-04    GBR ***\n",
            "                  Value    WMT 2.415006 1.600010e-02    GBR  **\n",
            "              Defensive    JNJ 2.698501 7.139695e-03    GBR ***\n",
            "              Defensive     PG 2.143454 3.243396e-02    GBR  **\n",
            "              Defensive     KO 3.823322 1.438670e-04    GBR ***\n",
            "              Defensive    PFE 4.611392 4.786474e-06    GBR ***\n",
            "               Cyclical    CAT 4.770974 2.248916e-06    GBR ***\n",
            "               Cyclical    FCX 2.880871 4.091787e-03    GBR ***\n",
            "               Cyclical      F 2.548949 1.102531e-02    GBR  **\n",
            "               Cyclical    XOM 3.739884 1.997602e-04    GBR ***\n",
            "        Income/Dividend      T 4.015145 6.608322e-05    GBR ***\n",
            "        Income/Dividend     VZ 3.568570 3.843994e-04    GBR ***\n",
            "        Income/Dividend     MO 3.457687 5.790880e-04    GBR ***\n",
            "        Income/Dividend      D 1.866206 6.244666e-02    Tie   *\n",
            "\n",
            "======================================================================\n",
            "GENERATING PLOTS\n",
            "======================================================================\n",
            "✓ MSE heatmap saved.\n",
            "✓ R² by category plot saved.\n",
            "✓ Non-linearity bubble chart saved.\n",
            "✓ DM wins chart saved.\n",
            "\n",
            "✓ Summary CSVs saved.\n",
            "\n",
            "======================================================================\n",
            "HIGH-LEVEL CONCLUSIONS\n",
            "======================================================================\n",
            "\n",
            "Bear market:\n",
            "  OLS vs RF: ML wins 20/20, OLS wins 0/20\n",
            "  OLS vs SVR: ML wins 4/20, OLS wins 0/20\n",
            "  OLS vs GBR: ML wins 20/20, OLS wins 0/20\n",
            "\n",
            "Normal market:\n",
            "  OLS vs RF: ML wins 20/20, OLS wins 0/20\n",
            "  OLS vs SVR: ML wins 0/20, OLS wins 1/20\n",
            "  OLS vs GBR: ML wins 20/20, OLS wins 0/20\n",
            "\n",
            "Bull market:\n",
            "  OLS vs RF: ML wins 20/20, OLS wins 0/20\n",
            "  OLS vs SVR: ML wins 0/20, OLS wins 0/20\n",
            "  OLS vs GBR: ML wins 19/20, OLS wins 0/20\n",
            "\n",
            "Non-linearity in Bear regime: 8/20 stocks show strong evidence (≥3/4 tests)\n",
            "\n",
            "Best model by category in Bear regime (lowest avg MSE):\n",
            "                        Best\n",
            "Category                    \n",
            "Cyclical                  RF\n",
            "Defensive                 RF\n",
            "High-Beta Growth (Mag7)   RF\n",
            "Income/Dividend           RF\n",
            "Value                     RF\n",
            "\n",
            "======================================================================\n",
            "ANALYSIS COMPLETE\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What's new vs the previous version:\n",
        "OOS walk-forward CV runs for every stock alongside the in-sample analysis. Each stock gets 8 folds, train always starts from 2015, test window rolls forward ~1 year at a time. The OOS predictions are collected and used for both MSE/R² metrics and DM tests — so you have a direct IS vs OOS comparison for every stock × regime combination.\n",
        "6 LaTeX tables saved as .tex files, ready to drop into your paper:\n",
        "\n",
        "table_IS_R2.tex / table_IS_MSE.tex — in-sample performance\n",
        "table_OOS_R2.tex / table_OOS_MSE.tex — out-of-sample performance\n",
        "table_IS_DM.tex / table_OOS_DM.tex — DM test results with significance stars and win counts\n",
        "\n",
        "Master figure (master_figure.png) — 9 panels in a single 22×18 publication-ready figure:\n",
        "\n",
        "Row 1: IS R² bars by stock category for each regime (Bear/Normal/Bull)\n",
        "Row 2: MSE heatmaps IS vs OOS in Bear, plus DM wins IS vs OOS comparison\n",
        "Row 3: Non-linearity bubble chart, OOS DM wins by regime, IS vs OOS R² scatter\n",
        "\n",
        "The IS vs OOS scatter (bottom right) is the most telling panel — it shows directly whether stocks with high in-sample RF/GBR advantage also have high OOS advantage, or whether the in-sample gain evaporates. Based on the NVDA results we saw earlier, expect most points to fall below the 45° line."
      ],
      "metadata": {
        "id": "WIhDAsBMaJPY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# DESIGN B — FINAL VERSION\n",
        "# In-sample regime analysis + OOS walk-forward CV\n",
        "# Paper-ready tables (LaTeX) + presentation master figure\n",
        "#\n",
        "# Research question:\n",
        "#   How well does each model (OLS, RF, SVR, GBR) describe the\n",
        "#   return-generating process in each market regime (Bear/Normal/Bull)?\n",
        "#   Does the in-sample advantage of ML models survive out-of-sample?\n",
        "#   Does this vary across stock types?\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "import matplotlib.gridspec as gridspec\n",
        "from matplotlib.colors import Normalize\n",
        "from matplotlib.cm import ScalarMappable\n",
        "\n",
        "import yfinance as yf\n",
        "from pandas_datareader import data as pdr\n",
        "\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.stats.diagnostic import (\n",
        "    linear_reset, linear_harvey_collier,\n",
        "    linear_rainbow, het_white\n",
        ")\n",
        "from scipy.stats import t as student_t\n",
        "\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "pd.set_option(\"display.width\", 140)\n",
        "np.random.seed(42)\n",
        "\n",
        "OUT = \"/mnt/user-data/outputs\"\n",
        "os.makedirs(OUT, exist_ok=True)\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "# UNIVERSE & CONSTANTS\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "\n",
        "STOCK_UNIVERSE = {\n",
        "    \"High-Beta Growth\": [\"NVDA\", \"TSLA\", \"META\", \"AMZN\"],\n",
        "    \"Value\":            [\"BRK-B\", \"JPM\",  \"BAC\",  \"WMT\"],\n",
        "    \"Defensive\":        [\"JNJ\",   \"PG\",   \"KO\",   \"PFE\"],\n",
        "    \"Cyclical\":         [\"CAT\",   \"FCX\",  \"F\",    \"XOM\"],\n",
        "    \"Income/Dividend\":  [\"T\",     \"VZ\",   \"MO\",   \"D\"],\n",
        "}\n",
        "\n",
        "MODEL_NAMES  = [\"OLS\", \"RF\", \"SVR\", \"GBR\"]\n",
        "REGIMES      = [\"Bear (Bot 25%)\", \"Normal (Mid 50%)\", \"Bull (Top 25%)\"]\n",
        "REGIME_SHORT = {\"Bear (Bot 25%)\": \"Bear\", \"Normal (Mid 50%)\": \"Normal\", \"Bull (Top 25%)\": \"Bull\"}\n",
        "COLORS       = {\"OLS\": \"#4878CF\", \"RF\": \"#6ACC65\", \"SVR\": \"#D65F5F\", \"GBR\": \"#B47CC7\"}\n",
        "CAT_ORDER    = list(STOCK_UNIVERSE.keys())\n",
        "\n",
        "START    = \"2015-01-01\"\n",
        "END      = \"2025-09-30\"\n",
        "N_SPLITS = 8\n",
        "MIN_TRAIN = 252\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "# DATA\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "\n",
        "def fetch_prices(tickers, start, end):\n",
        "    raw = yf.download(tickers, start=start, end=end,\n",
        "                      progress=False, auto_adjust=True)\n",
        "    if isinstance(raw.columns, pd.MultiIndex):\n",
        "        level0 = raw.columns.get_level_values(0).unique()\n",
        "        field  = \"Close\" if \"Close\" in level0 else level0[0]\n",
        "        px = raw[field].copy()\n",
        "        px.columns.name = None\n",
        "        return px\n",
        "    for col in [\"Close\", \"Adj Close\"]:\n",
        "        if col in raw.columns:\n",
        "            name = tickers[0] if len(tickers) == 1 else col\n",
        "            return raw[[col]].rename(columns={col: name})\n",
        "    return raw\n",
        "\n",
        "\n",
        "def build_dataset(start=START, end=END):\n",
        "    print(f\"\\n{'='*70}\\nDownloading data: {start} → {end}\\n{'='*70}\")\n",
        "    all_tickers = [t for ts in STOCK_UNIVERSE.values() for t in ts]\n",
        "    prices = fetch_prices(all_tickers, start, end)\n",
        "    print(f\"Prices: {prices.shape[0]} days × {prices.shape[1]} tickers\")\n",
        "\n",
        "    ff_raw = pdr.DataReader(\"F-F_Research_Data_Factors_daily\",\n",
        "                            \"famafrench\", start=start, end=end)[0]\n",
        "    ff = ff_raw[[\"Mkt-RF\",\"RF\"]].rename(\n",
        "        columns={\"Mkt-RF\":\"excess_mkt\",\"RF\":\"rf\"})\n",
        "    return prices, ff\n",
        "\n",
        "\n",
        "def build_stock_df(ticker, prices, ff):\n",
        "    if ticker not in prices.columns:\n",
        "        return pd.DataFrame()\n",
        "    df = prices[[ticker]].dropna().join(ff, how=\"inner\").dropna()\n",
        "    df[\"ret\"]        = df[ticker].pct_change() * 100.0\n",
        "    df[\"excess_ret\"] = df[\"ret\"] - df[\"rf\"]\n",
        "    df = df.dropna()\n",
        "    q25 = df[\"excess_mkt\"].quantile(0.25)\n",
        "    q75 = df[\"excess_mkt\"].quantile(0.75)\n",
        "    df[\"Regime\"] = \"Normal (Mid 50%)\"\n",
        "    df.loc[df[\"excess_mkt\"] < q25, \"Regime\"] = \"Bear (Bot 25%)\"\n",
        "    df.loc[df[\"excess_mkt\"] > q75, \"Regime\"] = \"Bull (Top 25%)\"\n",
        "    return df\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "# MODELS\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "\n",
        "def fresh_models():\n",
        "    return {\n",
        "        \"OLS\": LinearRegression(),\n",
        "        \"RF\":  RandomForestRegressor(n_estimators=300, min_samples_leaf=2,\n",
        "                                     random_state=42, n_jobs=-1),\n",
        "        \"SVR\": Pipeline([(\"sc\", StandardScaler()),\n",
        "                         (\"sv\", SVR(kernel=\"rbf\", C=10, gamma=\"scale\"))]),\n",
        "        \"GBR\": GradientBoostingRegressor(n_estimators=500, learning_rate=0.05,\n",
        "                                         max_depth=3, loss=\"huber\",\n",
        "                                         random_state=42)\n",
        "    }\n",
        "\n",
        "def train_full(df):\n",
        "    X = df[[\"excess_mkt\"]].values\n",
        "    y = df[\"excess_ret\"].values\n",
        "    models = fresh_models()\n",
        "    for m in models.values():\n",
        "        m.fit(X, y)\n",
        "    return models, {n: m.predict(X) for n,m in models.items()}\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "# NON-LINEARITY TESTS\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "\n",
        "def nonlin_battery(X, y):\n",
        "    res = {}\n",
        "    try:\n",
        "        Xc = sm.add_constant(X)\n",
        "        ols = sm.OLS(y, Xc).fit()\n",
        "        res['RESET_p2'] = linear_reset(ols, power=2, use_f=True).pvalue\n",
        "        res['RESET_p3'] = linear_reset(ols, power=3, use_f=True).pvalue\n",
        "        res['HC_p']     = linear_harvey_collier(ols)[1]\n",
        "        res['Rainbow_p']= linear_rainbow(ols, frac=0.5)[1]\n",
        "        res['White_p']  = het_white(ols.resid, ols.model.exog)[1]\n",
        "    except Exception:\n",
        "        for k in ['RESET_p2','RESET_p3','HC_p','Rainbow_p','White_p']:\n",
        "            res.setdefault(k, np.nan)\n",
        "\n",
        "    reset_rej = (res.get('RESET_p2',1)<0.05) or (res.get('RESET_p3',1)<0.05)\n",
        "    res['reject_count'] = int(sum([\n",
        "        reset_rej,\n",
        "        res.get('HC_p',1)<0.05,\n",
        "        res.get('Rainbow_p',1)<0.05,\n",
        "        res.get('White_p',1)<0.05,\n",
        "    ]))\n",
        "    return res\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "# DIEBOLD-MARIANO\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "\n",
        "def dm_test(e1, e2, name1, name2, lags=6):\n",
        "    d = np.asarray(e1)**2 - np.asarray(e2)**2\n",
        "    d = d[~np.isnan(d)]\n",
        "    T = len(d)\n",
        "    if T < 30:\n",
        "        return dict(dm_stat=np.nan, p_value=np.nan, winner=\"n/a\",\n",
        "                    model1=name1, model2=name2)\n",
        "    d0    = d - d.mean()\n",
        "    gamma = np.sum(d0**2)/T\n",
        "    L     = min(lags, T-1)\n",
        "    for k in range(1, L+1):\n",
        "        gamma += 2*(1-k/(L+1))*np.sum(d0[k:]*d0[:-k])/T\n",
        "    if gamma/T <= 0:\n",
        "        return dict(dm_stat=np.nan, p_value=np.nan, winner=\"var_err\",\n",
        "                    model1=name1, model2=name2)\n",
        "    dm   = d.mean()/np.sqrt(gamma/T)\n",
        "    pval = 2*(1-student_t.cdf(abs(dm), df=T-1))\n",
        "    winner = (name2 if pval<0.05 and d.mean()>0\n",
        "              else name1 if pval<0.05 and d.mean()<0\n",
        "              else \"Tie\")\n",
        "    return dict(dm_stat=float(dm), p_value=float(pval),\n",
        "                winner=winner, model1=name1, model2=name2)\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "# IN-SAMPLE REGIME ANALYSIS\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "\n",
        "def insample_analysis(ticker, df):\n",
        "    \"\"\"Train on full sample, evaluate errors within each regime.\"\"\"\n",
        "    _, preds = train_full(df)\n",
        "    X   = df[[\"excess_mkt\"]].values\n",
        "    y   = df[\"excess_ret\"].values\n",
        "    err = {n: y - preds[n] for n in MODEL_NAMES}\n",
        "\n",
        "    rows_perf, rows_nl, rows_dm = [], [], []\n",
        "\n",
        "    for regime in [\"Full Sample\"] + REGIMES:\n",
        "        mask = (np.ones(len(df), bool) if regime == \"Full Sample\"\n",
        "                else (df[\"Regime\"] == regime).values)\n",
        "        y_s  = y[mask]\n",
        "        n_s  = mask.sum()\n",
        "\n",
        "        row = {\"Ticker\": ticker, \"Regime\": regime, \"N\": int(n_s)}\n",
        "        for m in MODEL_NAMES:\n",
        "            e = err[m][mask]\n",
        "            row[f\"{m}_MSE\"]  = float(np.mean(e**2))\n",
        "            row[f\"{m}_RMSE\"] = float(np.sqrt(np.mean(e**2)))\n",
        "            row[f\"{m}_R2\"]   = float(r2_score(y_s, preds[m][mask])) if n_s > 2 else np.nan\n",
        "        rows_perf.append(row)\n",
        "\n",
        "        if n_s >= 50:\n",
        "            nl = nonlin_battery(X[mask], y_s)\n",
        "            rows_nl.append({\"Ticker\": ticker, \"Regime\": regime,\n",
        "                            \"N\": int(n_s), **nl})\n",
        "\n",
        "        if n_s >= 30:\n",
        "            for ml in [\"RF\",\"SVR\",\"GBR\"]:\n",
        "                dm = dm_test(err[\"OLS\"][mask], err[ml][mask], \"OLS\", ml)\n",
        "                rows_dm.append({\"Ticker\": ticker, \"Regime\": regime, **dm})\n",
        "\n",
        "    return (pd.DataFrame(rows_perf),\n",
        "            pd.DataFrame(rows_nl),\n",
        "            pd.DataFrame(rows_dm))\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "# OUT-OF-SAMPLE WALK-FORWARD CV\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "\n",
        "def oos_cv(ticker, df, n_splits=N_SPLITS, min_train=MIN_TRAIN):\n",
        "    \"\"\"\n",
        "    Walk-forward CV: train on past, predict on future.\n",
        "    Collect OOS predictions for every observation (except\n",
        "    the first min_train rows which are always in training).\n",
        "    Returns DataFrame with columns: date, true, OLS, RF, SVR, GBR, Regime\n",
        "    \"\"\"\n",
        "    X   = df[[\"excess_mkt\"]].values\n",
        "    y   = df[\"excess_ret\"].values\n",
        "    idx = df.index\n",
        "    reg = df[\"Regime\"].values\n",
        "\n",
        "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "    store = {m: np.full(len(df), np.nan) for m in MODEL_NAMES}\n",
        "    y_oos = np.full(len(df), np.nan)\n",
        "    r_oos = np.full(len(df), None, dtype=object)\n",
        "\n",
        "    for train_idx, test_idx in tscv.split(X):\n",
        "        if len(train_idx) < min_train:\n",
        "            continue\n",
        "        mods = fresh_models()\n",
        "        for m in mods.values():\n",
        "            m.fit(X[train_idx], y[train_idx])\n",
        "        for m_name, m in mods.items():\n",
        "            store[m_name][test_idx] = m.predict(X[test_idx])\n",
        "        y_oos[test_idx] = y[test_idx]\n",
        "        r_oos[test_idx] = reg[test_idx]\n",
        "\n",
        "    # Keep only rows that have OOS predictions\n",
        "    valid = ~np.isnan(y_oos)\n",
        "    oos_df = pd.DataFrame({\n",
        "        \"date\":   idx[valid],\n",
        "        \"true\":   y_oos[valid],\n",
        "        \"Regime\": r_oos[valid],\n",
        "        **{m: store[m][valid] for m in MODEL_NAMES}\n",
        "    })\n",
        "    return oos_df\n",
        "\n",
        "\n",
        "def oos_metrics(oos_df, ticker):\n",
        "    \"\"\"Compute MSE / R² per model per regime on OOS data.\"\"\"\n",
        "    rows_perf, rows_dm = [], []\n",
        "\n",
        "    for regime in [\"Full OOS\"] + REGIMES:\n",
        "        mask = (np.ones(len(oos_df), bool) if regime == \"Full OOS\"\n",
        "                else (oos_df[\"Regime\"] == regime).values)\n",
        "        y_s = oos_df[\"true\"].values[mask]\n",
        "        n_s = mask.sum()\n",
        "        if n_s < 10:\n",
        "            continue\n",
        "\n",
        "        row = {\"Ticker\": ticker, \"Regime\": regime, \"N\": int(n_s)}\n",
        "        err = {}\n",
        "        for m in MODEL_NAMES:\n",
        "            p = oos_df[m].values[mask]\n",
        "            e = y_s - p\n",
        "            err[m] = e\n",
        "            row[f\"{m}_MSE\"]  = float(np.mean(e**2))\n",
        "            row[f\"{m}_RMSE\"] = float(np.sqrt(np.mean(e**2)))\n",
        "            row[f\"{m}_R2\"]   = float(r2_score(y_s, p)) if n_s > 2 else np.nan\n",
        "        rows_perf.append(row)\n",
        "\n",
        "        if n_s >= 30:\n",
        "            for ml in [\"RF\",\"SVR\",\"GBR\"]:\n",
        "                dm = dm_test(err[\"OLS\"], err[ml], \"OLS\", ml)\n",
        "                rows_dm.append({\"Ticker\": ticker, \"Regime\": regime, **dm})\n",
        "\n",
        "    return pd.DataFrame(rows_perf), pd.DataFrame(rows_dm)\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "# CROSS-SECTION RUNNER\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "\n",
        "def run_all(prices, ff):\n",
        "    IS_perf, IS_nl, IS_dm = [], [], []\n",
        "    OOS_perf, OOS_dm      = [], []\n",
        "\n",
        "    for cat, tickers in STOCK_UNIVERSE.items():\n",
        "        print(f\"\\n{'='*60}\\n{cat}\\n{'='*60}\")\n",
        "        for ticker in tickers:\n",
        "            df = build_stock_df(ticker, prices, ff)\n",
        "            if df.empty or len(df) < 400:\n",
        "                print(f\"  {ticker}: skip\"); continue\n",
        "\n",
        "            print(f\"  {ticker} (N={len(df)}) ...\", end=\" \")\n",
        "\n",
        "            # In-sample\n",
        "            ip, inl, idm = insample_analysis(ticker, df)\n",
        "            ip[\"Category\"] = inl[\"Category\"] = idm[\"Category\"] = cat\n",
        "            IS_perf.append(ip); IS_nl.append(inl); IS_dm.append(idm)\n",
        "\n",
        "            # OOS\n",
        "            oos_df = oos_cv(ticker, df)\n",
        "            op, odm = oos_metrics(oos_df, ticker)\n",
        "            op[\"Category\"] = odm[\"Category\"] = cat\n",
        "            OOS_perf.append(op); OOS_dm.append(odm)\n",
        "\n",
        "            # Quick print\n",
        "            bear_is  = ip[ip[\"Regime\"]==\"Bear (Bot 25%)\"]\n",
        "            bear_oos = op[op[\"Regime\"]==\"Bear (Bot 25%)\"] if len(op) else pd.DataFrame()\n",
        "            if not bear_is.empty:\n",
        "                best_is  = min(MODEL_NAMES, key=lambda m: bear_is[f\"{m}_MSE\"].values[0])\n",
        "                best_oos = (min(MODEL_NAMES, key=lambda m: bear_oos[f\"{m}_MSE\"].values[0])\n",
        "                            if not bear_oos.empty else \"n/a\")\n",
        "                print(f\"Bear IS best={best_is}  OOS best={best_oos}\")\n",
        "            else:\n",
        "                print()\n",
        "\n",
        "    return (pd.concat(IS_perf,  ignore_index=True),\n",
        "            pd.concat(IS_nl,    ignore_index=True),\n",
        "            pd.concat(IS_dm,    ignore_index=True),\n",
        "            pd.concat(OOS_perf, ignore_index=True),\n",
        "            pd.concat(OOS_dm,   ignore_index=True))\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "# LATEX TABLE HELPERS\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "\n",
        "def sig_star(p):\n",
        "    if pd.isna(p): return \"\"\n",
        "    return \"***\" if p<0.01 else \"**\" if p<0.05 else \"*\" if p<0.10 else \"\"\n",
        "\n",
        "\n",
        "def make_latex_perf_table(df, caption, label, metric=\"R2\"):\n",
        "    \"\"\"\n",
        "    Table: rows = Category avg × Regime, cols = models.\n",
        "    \"\"\"\n",
        "    rows = []\n",
        "    for cat in CAT_ORDER:\n",
        "        sub = df[df[\"Category\"]==cat]\n",
        "        for regime in ([\"Full Sample\",\"Bear (Bot 25%)\",\"Normal (Mid 50%)\",\"Bull (Top 25%)\"]\n",
        "                       if \"Full Sample\" in df[\"Regime\"].unique()\n",
        "                       else [\"Full OOS\",\"Bear (Bot 25%)\",\"Normal (Mid 50%)\",\"Bull (Top 25%)\"]):\n",
        "            s = sub[sub[\"Regime\"]==regime]\n",
        "            if s.empty: continue\n",
        "            vals = {m: s[f\"{m}_{metric}\"].mean() for m in MODEL_NAMES}\n",
        "            best = max(vals, key=vals.get)\n",
        "            row  = {\"Category\": cat, \"Regime\": REGIME_SHORT.get(regime, regime)}\n",
        "            for m in MODEL_NAMES:\n",
        "                cell = f\"{vals[m]:.3f}\"\n",
        "                if m == best:\n",
        "                    cell = f\"\\\\textbf{{{cell}}}\"\n",
        "                row[m] = cell\n",
        "            rows.append(row)\n",
        "\n",
        "    tdf = pd.DataFrame(rows)\n",
        "    lines = [\n",
        "        \"\\\\begin{table}[htbp]\",\n",
        "        \"\\\\centering\",\n",
        "        f\"\\\\caption{{{caption}}}\",\n",
        "        f\"\\\\label{{{label}}}\",\n",
        "        \"\\\\small\",\n",
        "        \"\\\\begin{tabular}{llrrrr}\",\n",
        "        \"\\\\toprule\",\n",
        "        \"Category & Regime & OLS & RF & SVR & GBR \\\\\\\\\",\n",
        "        \"\\\\midrule\",\n",
        "    ]\n",
        "    prev_cat = None\n",
        "    for _, r in tdf.iterrows():\n",
        "        cat_cell = r[\"Category\"] if r[\"Category\"] != prev_cat else \"\"\n",
        "        prev_cat = r[\"Category\"]\n",
        "        lines.append(f\"{cat_cell} & {r['Regime']} & \"\n",
        "                     f\"{r['OLS']} & {r['RF']} & {r['SVR']} & {r['GBR']} \\\\\\\\\")\n",
        "        if cat_cell != \"\" and cat_cell != CAT_ORDER[0]:\n",
        "            lines.insert(-1, \"\\\\midrule\")\n",
        "    lines += [\"\\\\bottomrule\",\n",
        "              \"\\\\end{tabular}\",\n",
        "              \"\\\\end{table}\"]\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "\n",
        "def make_latex_dm_table(dm_df, caption, label):\n",
        "    \"\"\"\n",
        "    DM test table: rows = Category × ML model, cols = Bear/Normal/Bull.\n",
        "    Shows DM stat with significance stars.\n",
        "    \"\"\"\n",
        "    rows = []\n",
        "    for cat in CAT_ORDER:\n",
        "        sub = dm_df[dm_df[\"Category\"]==cat]\n",
        "        for ml in [\"RF\",\"SVR\",\"GBR\"]:\n",
        "            ml_sub = sub[sub[\"model2\"]==ml]\n",
        "            row = {\"Category\": cat, \"vs\": f\"OLS vs {ml}\"}\n",
        "            for regime in REGIMES:\n",
        "                rs = ml_sub[ml_sub[\"Regime\"]==regime]\n",
        "                if rs.empty:\n",
        "                    row[REGIME_SHORT[regime]] = \"—\"\n",
        "                    continue\n",
        "                dm   = rs[\"dm_stat\"].mean()\n",
        "                pval = rs[\"p_value\"].mean()\n",
        "                wins = (rs[\"winner\"]==ml).sum()\n",
        "                total= len(rs)\n",
        "                star = sig_star(pval)\n",
        "                row[REGIME_SHORT[regime]] = (\n",
        "                    f\"{dm:.2f}{star} ({wins}/{total})\"\n",
        "                )\n",
        "            rows.append(row)\n",
        "\n",
        "    tdf = pd.DataFrame(rows)\n",
        "    lines = [\n",
        "        \"\\\\begin{table}[htbp]\",\n",
        "        \"\\\\centering\",\n",
        "        f\"\\\\caption{{{caption}}}\",\n",
        "        f\"\\\\label{{{label}}}\",\n",
        "        \"\\\\small\",\n",
        "        \"\\\\begin{tabular}{llccc}\",\n",
        "        \"\\\\toprule\",\n",
        "        \"Category & Comparison & Bear & Normal & Bull \\\\\\\\\",\n",
        "        \"\\\\midrule\",\n",
        "    ]\n",
        "    prev_cat = None\n",
        "    for _, r in tdf.iterrows():\n",
        "        cat_cell = r[\"Category\"] if r[\"Category\"] != prev_cat else \"\"\n",
        "        prev_cat = r[\"Category\"]\n",
        "        lines.append(f\"{cat_cell} & {r['vs']} & \"\n",
        "                     f\"{r['Bear']} & {r['Normal']} & {r['Bull']} \\\\\\\\\")\n",
        "        if cat_cell != \"\" and cat_cell != CAT_ORDER[0]:\n",
        "            lines.insert(-1, \"\\\\midrule\")\n",
        "    lines += [\"\\\\bottomrule\",\n",
        "              \"\\\\multicolumn{5}{l}{\\\\footnotesize DM statistic (avg across stocks). \"\n",
        "              \"Stars: $^{*}$p$<$0.10, $^{**}$p$<$0.05, $^{***}$p$<$0.01. \"\n",
        "              \"Wins = stocks where ML significantly beats OLS.} \\\\\\\\\",\n",
        "              \"\\\\end{tabular}\",\n",
        "              \"\\\\end{table}\"]\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "\n",
        "def save_latex_tables(IS_perf, IS_dm, OOS_perf, OOS_dm):\n",
        "    tables = {\n",
        "        \"table_IS_R2.tex\": make_latex_perf_table(\n",
        "            IS_perf,\n",
        "            \"In-Sample Within-Regime R\\\\textsuperscript{2} by Model and Stock Category\",\n",
        "            \"tab:is_r2\", metric=\"R2\"),\n",
        "        \"table_IS_MSE.tex\": make_latex_perf_table(\n",
        "            IS_perf,\n",
        "            \"In-Sample Within-Regime MSE by Model and Stock Category\",\n",
        "            \"tab:is_mse\", metric=\"MSE\"),\n",
        "        \"table_OOS_R2.tex\": make_latex_perf_table(\n",
        "            OOS_perf,\n",
        "            \"Out-of-Sample (Walk-Forward CV) R\\\\textsuperscript{2} by Model and Stock Category\",\n",
        "            \"tab:oos_r2\", metric=\"R2\"),\n",
        "        \"table_OOS_MSE.tex\": make_latex_perf_table(\n",
        "            OOS_perf,\n",
        "            \"Out-of-Sample (Walk-Forward CV) MSE by Model and Stock Category\",\n",
        "            \"tab:oos_mse\", metric=\"MSE\"),\n",
        "        \"table_IS_DM.tex\": make_latex_dm_table(\n",
        "            IS_dm,\n",
        "            \"Diebold-Mariano Tests: OLS vs ML Models (In-Sample, Within-Regime)\",\n",
        "            \"tab:is_dm\"),\n",
        "        \"table_OOS_DM.tex\": make_latex_dm_table(\n",
        "            OOS_dm,\n",
        "            \"Diebold-Mariano Tests: OLS vs ML Models (Out-of-Sample, Walk-Forward CV)\",\n",
        "            \"tab:oos_dm\"),\n",
        "    }\n",
        "    for fname, content in tables.items():\n",
        "        path = os.path.join(OUT, fname)\n",
        "        with open(path, \"w\") as f:\n",
        "            f.write(content)\n",
        "    print(f\"✓ {len(tables)} LaTeX tables saved to {OUT}/\")\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "# MASTER PRESENTATION FIGURE\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "\n",
        "def plot_master(IS_perf, IS_dm, OOS_perf, OOS_dm, IS_nl):\n",
        "    \"\"\"\n",
        "    Single 3×3 presentation figure:\n",
        "\n",
        "    Row 1: R² by category — Bear / Normal / Bull  (IS vs OOS side by side)\n",
        "    Row 2: MSE heatmap IS  |  MSE heatmap OOS  |  DM wins Bear IS vs OOS\n",
        "    Row 3: Non-linearity bubble  |  OOS DM wins by regime  |  IS vs OOS R² scatter\n",
        "    \"\"\"\n",
        "    fig = plt.figure(figsize=(22, 18))\n",
        "    fig.patch.set_facecolor(\"#FAFAFA\")\n",
        "\n",
        "    gs = gridspec.GridSpec(3, 3, figure=fig,\n",
        "                           hspace=0.52, wspace=0.38,\n",
        "                           left=0.07, right=0.97,\n",
        "                           top=0.93, bottom=0.06)\n",
        "\n",
        "    title_kw = dict(fontsize=10, fontweight=\"bold\", pad=8)\n",
        "\n",
        "    # ── helpers ──────────────────────────────────────────────\n",
        "\n",
        "    def cat_avg_r2(df, regime_label):\n",
        "        sub = df[df[\"Regime\"]==regime_label]\n",
        "        return sub.groupby(\"Category\")[[f\"{m}_R2\" for m in MODEL_NAMES]].mean()\n",
        "\n",
        "    def rel_mse(df, regime_label):\n",
        "        sub = df[df[\"Regime\"]==regime_label].copy()\n",
        "        sub = sub.sort_values([\"Category\",\"Ticker\"])\n",
        "        rel = pd.DataFrame(index=sub[\"Ticker\"])\n",
        "        for ml in [\"RF\",\"SVR\",\"GBR\"]:\n",
        "            rel[ml] = sub[f\"{ml}_MSE\"].values / sub[\"OLS_MSE\"].values\n",
        "        rel[\"cat\"] = sub[\"Category\"].values\n",
        "        return rel, sub\n",
        "\n",
        "    # ── Panel A: grouped bar — IS Bear ───────────────────────\n",
        "    ax_a = fig.add_subplot(gs[0, 0])\n",
        "    _plot_cat_r2_bars(ax_a, IS_perf, \"Bear (Bot 25%)\",\n",
        "                      \"IS R² — Bear Market\", title_kw)\n",
        "\n",
        "    # ── Panel B: grouped bar — IS Normal ─────────────────────\n",
        "    ax_b = fig.add_subplot(gs[0, 1])\n",
        "    _plot_cat_r2_bars(ax_b, IS_perf, \"Normal (Mid 50%)\",\n",
        "                      \"IS R² — Normal Market\", title_kw)\n",
        "\n",
        "    # ── Panel C: grouped bar — IS Bull ───────────────────────\n",
        "    ax_c = fig.add_subplot(gs[0, 2])\n",
        "    _plot_cat_r2_bars(ax_c, IS_perf, \"Bull (Top 25%)\",\n",
        "                      \"IS R² — Bull Market\", title_kw)\n",
        "\n",
        "    # ── Panel D: MSE heatmap IS Bear ─────────────────────────\n",
        "    ax_d = fig.add_subplot(gs[1, 0])\n",
        "    _plot_mse_heatmap_ax(ax_d, IS_perf, \"Bear (Bot 25%)\",\n",
        "                         \"IS MSE Ratio vs OLS — Bear\", title_kw)\n",
        "\n",
        "    # ── Panel E: MSE heatmap OOS Bear ────────────────────────\n",
        "    ax_e = fig.add_subplot(gs[1, 1])\n",
        "    _plot_mse_heatmap_ax(ax_e, OOS_perf, \"Bear (Bot 25%)\",\n",
        "                         \"OOS MSE Ratio vs OLS — Bear\", title_kw)\n",
        "\n",
        "    # ── Panel F: DM wins — IS vs OOS in Bear ─────────────────\n",
        "    ax_f = fig.add_subplot(gs[1, 2])\n",
        "    _plot_dm_wins_comparison(ax_f, IS_dm, OOS_dm, \"Bear (Bot 25%)\",\n",
        "                             \"DM Wins in Bear: IS vs OOS\", title_kw)\n",
        "\n",
        "    # ── Panel G: non-linearity bubble ────────────────────────\n",
        "    ax_g = fig.add_subplot(gs[2, 0])\n",
        "    _plot_nonlin_bubble(ax_g, IS_nl, title_kw)\n",
        "\n",
        "    # ── Panel H: OOS DM wins by regime ───────────────────────\n",
        "    ax_h = fig.add_subplot(gs[2, 1])\n",
        "    _plot_oos_dm_by_regime(ax_h, OOS_dm, title_kw)\n",
        "\n",
        "    # ── Panel I: IS vs OOS R² scatter (RF, Bear) ─────────────\n",
        "    ax_i = fig.add_subplot(gs[2, 2])\n",
        "    _plot_is_vs_oos_scatter(ax_i, IS_perf, OOS_perf, title_kw)\n",
        "\n",
        "    # Legend\n",
        "    handles = [mpatches.Patch(color=COLORS[m], label=m) for m in MODEL_NAMES]\n",
        "    fig.legend(handles=handles, loc=\"upper center\", ncol=4,\n",
        "               fontsize=11, bbox_to_anchor=(0.5, 0.965),\n",
        "               framealpha=0.9, edgecolor=\"gray\")\n",
        "\n",
        "    fig.suptitle(\n",
        "        \"CAPM Model Performance Across Market Regimes and Stock Categories\\n\"\n",
        "        \"In-Sample (Full Dataset) vs Out-of-Sample (Walk-Forward CV)\",\n",
        "        fontsize=14, fontweight=\"bold\", y=0.99)\n",
        "\n",
        "    path = os.path.join(OUT, \"master_figure.png\")\n",
        "    fig.savefig(path, dpi=180, bbox_inches=\"tight\", facecolor=fig.get_facecolor())\n",
        "    plt.close()\n",
        "    print(f\"✓ Master figure saved → {path}\")\n",
        "\n",
        "\n",
        "# ── sub-plot helpers ──────────────────────────────────────────\n",
        "\n",
        "def _plot_cat_r2_bars(ax, df, regime_label, title, title_kw):\n",
        "    sub = df[df[\"Regime\"]==regime_label]\n",
        "    cats = CAT_ORDER\n",
        "    x = np.arange(len(cats))\n",
        "    w = 0.18\n",
        "    for i, m in enumerate(MODEL_NAMES):\n",
        "        vals = [sub[sub[\"Category\"]==c][f\"{m}_R2\"].mean() for c in cats]\n",
        "        ax.bar(x + i*w, vals, w, color=COLORS[m], alpha=0.88, label=m)\n",
        "    ax.set_xticks(x + w*1.5)\n",
        "    ax.set_xticklabels([c.replace(\" \",\"\\n\") for c in cats], fontsize=6.5)\n",
        "    ax.set_ylabel(\"R²\", fontsize=8)\n",
        "    ax.axhline(0, color=\"gray\", lw=0.5, ls=\"--\")\n",
        "    ax.grid(True, alpha=0.25, axis=\"y\")\n",
        "    ax.set_title(title, **title_kw)\n",
        "    ax.tick_params(axis=\"both\", labelsize=7)\n",
        "\n",
        "\n",
        "def _plot_mse_heatmap_ax(ax, df, regime_label, title, title_kw):\n",
        "    sub = df[df[\"Regime\"]==regime_label].copy()\n",
        "    if sub.empty:\n",
        "        ax.set_visible(False); return\n",
        "    sub = sub.sort_values([\"Category\",\"Ticker\"])\n",
        "    ml_cols = [\"RF\",\"SVR\",\"GBR\"]\n",
        "    rel = np.array([[row[f\"{ml}_MSE\"]/row[\"OLS_MSE\"] for ml in ml_cols]\n",
        "                    for _, row in sub.iterrows()])\n",
        "    im = ax.imshow(rel, aspect=\"auto\", cmap=\"RdYlGn_r\", vmin=0.7, vmax=1.3)\n",
        "    ax.set_xticks(range(3)); ax.set_xticklabels(ml_cols, fontsize=8)\n",
        "    ax.set_yticks(range(len(sub)))\n",
        "    ax.set_yticklabels(\n",
        "        [f\"{r.Category[:3]}│{r.Ticker}\" for _,r in sub.iterrows()],\n",
        "        fontsize=6)\n",
        "    for i in range(len(sub)):\n",
        "        for j in range(3):\n",
        "            v = rel[i,j]\n",
        "            ax.text(j, i, f\"{v:.2f}\", ha=\"center\", va=\"center\",\n",
        "                    fontsize=5.5,\n",
        "                    color=\"white\" if (v<0.82 or v>1.18) else \"black\")\n",
        "    ax.set_title(title, **title_kw)\n",
        "    plt.colorbar(im, ax=ax, shrink=0.55, label=\"MSE ratio\")\n",
        "\n",
        "\n",
        "def _plot_dm_wins_comparison(ax, IS_dm, OOS_dm, regime, title, title_kw):\n",
        "    ml_models = [\"RF\",\"SVR\",\"GBR\"]\n",
        "    x = np.arange(len(ml_models))\n",
        "    w = 0.3\n",
        "\n",
        "    def wins(dm_df, reg):\n",
        "        sub = dm_df[dm_df[\"Regime\"]==reg]\n",
        "        return [(sub[sub[\"model2\"]==ml][\"winner\"]==ml).sum() for ml in ml_models]\n",
        "\n",
        "    is_w  = wins(IS_dm,  regime)\n",
        "    oos_w = wins(OOS_dm, regime)\n",
        "\n",
        "    bars1 = ax.bar(x-w/2, is_w,  w, color=\"#4878CF\", alpha=0.85, label=\"IS ML wins\")\n",
        "    bars2 = ax.bar(x+w/2, oos_w, w, color=\"#D65F5F\", alpha=0.85, label=\"OOS ML wins\")\n",
        "    ax.set_xticks(x); ax.set_xticklabels(ml_models, fontsize=9)\n",
        "    ax.set_ylabel(\"# stocks where ML wins DM test\", fontsize=7)\n",
        "    ax.set_ylim(0, 22)\n",
        "    ax.axhline(20, color=\"gray\", lw=0.8, ls=\"--\", alpha=0.5)\n",
        "    ax.text(2.4, 20.3, \"n=20\", fontsize=6.5, color=\"gray\")\n",
        "    ax.legend(fontsize=7.5, loc=\"lower right\")\n",
        "    ax.grid(True, alpha=0.25, axis=\"y\")\n",
        "    ax.set_title(title, **title_kw)\n",
        "    ax.tick_params(labelsize=7)\n",
        "\n",
        "\n",
        "def _plot_nonlin_bubble(ax, nl_df, title_kw):\n",
        "    sub = nl_df[nl_df[\"Regime\"].isin(REGIMES)].copy()\n",
        "    sub = sub.sort_values([\"Category\",\"Ticker\"])\n",
        "    tickers = sub[\"Ticker\"].unique()\n",
        "    t2y = {t:i for i,t in enumerate(tickers)}\n",
        "    r2x = {r:i for i,r in enumerate(REGIMES)}\n",
        "\n",
        "    for _, row in sub.iterrows():\n",
        "        cnt = row[\"reject_count\"]\n",
        "        ax.scatter(r2x[row[\"Regime\"]], t2y[row[\"Ticker\"]],\n",
        "                   s=cnt*180+80, c=cnt, cmap=\"RdYlGn_r\",\n",
        "                   vmin=0, vmax=4, alpha=0.82,\n",
        "                   edgecolors=\"gray\", lw=0.4)\n",
        "        ax.text(r2x[row[\"Regime\"]], t2y[row[\"Ticker\"]],\n",
        "                str(int(cnt)), ha=\"center\", va=\"center\",\n",
        "                fontsize=6, color=\"white\" if cnt>=2 else \"black\",\n",
        "                fontweight=\"bold\")\n",
        "\n",
        "    ax.set_xticks(range(3))\n",
        "    ax.set_xticklabels([REGIME_SHORT[r] for r in REGIMES], fontsize=8)\n",
        "    ax.set_yticks(range(len(tickers)))\n",
        "    labels, prev = [], None\n",
        "    for t in tickers:\n",
        "        cat = sub[sub[\"Ticker\"]==t][\"Category\"].iloc[0]\n",
        "        pref = f\"[{cat[:3]}] \" if cat!=prev else \"      \"\n",
        "        labels.append(pref+t); prev = cat\n",
        "    ax.set_yticklabels(labels, fontsize=5.8)\n",
        "    ax.grid(True, alpha=0.2)\n",
        "    ax.set_title(\"Non-Linearity Rejections\\n(out of 4 tests per regime)\", **title_kw)\n",
        "\n",
        "    sm = ScalarMappable(cmap=\"RdYlGn_r\", norm=Normalize(0,4))\n",
        "    sm.set_array([])\n",
        "    plt.colorbar(sm, ax=ax, shrink=0.5, label=\"Rejections\")\n",
        "\n",
        "\n",
        "def _plot_oos_dm_by_regime(ax, OOS_dm, title_kw):\n",
        "    ml_models = [\"RF\",\"SVR\",\"GBR\"]\n",
        "    x = np.arange(len(ml_models))\n",
        "    w = 0.22\n",
        "    regime_colors = {\"Bear (Bot 25%)\":\"#c0392b\",\n",
        "                     \"Normal (Mid 50%)\":\"#2980b9\",\n",
        "                     \"Bull (Top 25%)\":\"#27ae60\"}\n",
        "    for i, regime in enumerate(REGIMES):\n",
        "        sub = OOS_dm[OOS_dm[\"Regime\"]==regime]\n",
        "        wins = [(sub[sub[\"model2\"]==ml][\"winner\"]==ml).sum() for ml in ml_models]\n",
        "        ax.bar(x + (i-1)*w, wins, w,\n",
        "               color=regime_colors[regime], alpha=0.85,\n",
        "               label=REGIME_SHORT[regime])\n",
        "\n",
        "    ax.set_xticks(x); ax.set_xticklabels(ml_models, fontsize=9)\n",
        "    ax.set_ylabel(\"# stocks where ML wins (OOS)\", fontsize=7)\n",
        "    ax.set_ylim(0, 22)\n",
        "    ax.axhline(20, color=\"gray\", lw=0.8, ls=\"--\", alpha=0.5)\n",
        "    ax.legend(fontsize=7.5)\n",
        "    ax.grid(True, alpha=0.25, axis=\"y\")\n",
        "    ax.set_title(\"OOS DM Wins by Regime\\n(out of 20 stocks)\", **title_kw)\n",
        "    ax.tick_params(labelsize=7)\n",
        "\n",
        "\n",
        "def _plot_is_vs_oos_scatter(ax, IS_perf, OOS_perf, title_kw):\n",
        "    \"\"\"Scatter: IS R² vs OOS R² for each stock × model in Bear regime.\"\"\"\n",
        "    cat_colors_map = {c: col for c, col in zip(CAT_ORDER,\n",
        "        [\"#e74c3c\",\"#3498db\",\"#2ecc71\",\"#f39c12\",\"#9b59b6\"])}\n",
        "\n",
        "    is_sub  = IS_perf[IS_perf[\"Regime\"]==\"Bear (Bot 25%)\"]\n",
        "    oos_sub = OOS_perf[OOS_perf[\"Regime\"]==\"Bear (Bot 25%)\"]\n",
        "\n",
        "    plotted = False\n",
        "    for m in [\"RF\",\"GBR\"]:\n",
        "        merged = is_sub[[\"Ticker\",\"Category\",f\"{m}_R2\"]].merge(\n",
        "            oos_sub[[\"Ticker\",f\"{m}_R2\"]], on=\"Ticker\", suffixes=(\"_IS\",\"_OOS\"))\n",
        "        for _, row in merged.iterrows():\n",
        "            col = cat_colors_map.get(row[\"Category\"],\"gray\")\n",
        "            ax.scatter(row[f\"{m}_R2_IS\"], row[f\"{m}_R2_OOS\"],\n",
        "                       color=col, s=55 if m==\"RF\" else 35,\n",
        "                       marker=\"o\" if m==\"RF\" else \"^\",\n",
        "                       alpha=0.8, edgecolors=\"white\", lw=0.4)\n",
        "            plotted = True\n",
        "\n",
        "    lims = [-0.3, 0.7]\n",
        "    ax.plot(lims, lims, \"k--\", lw=0.8, alpha=0.5, label=\"IS = OOS\")\n",
        "    ax.axhline(0, color=\"gray\", lw=0.4, ls=\":\")\n",
        "    ax.axvline(0, color=\"gray\", lw=0.4, ls=\":\")\n",
        "    ax.set_xlim(lims); ax.set_ylim(lims)\n",
        "    ax.set_xlabel(\"In-Sample R²\", fontsize=8)\n",
        "    ax.set_ylabel(\"Out-of-Sample R²\", fontsize=8)\n",
        "    ax.set_title(\"IS vs OOS R² — Bear Regime\\n(● RF, ▲ GBR; colour = category)\",\n",
        "                 **title_kw)\n",
        "\n",
        "    # Category legend patches\n",
        "    cat_patches = [mpatches.Patch(color=cat_colors_map[c],\n",
        "                   label=c[:10]) for c in CAT_ORDER]\n",
        "    marker_patches = [\n",
        "        plt.Line2D([0],[0], marker=\"o\", color=\"gray\", ls=\"none\",\n",
        "                   ms=6, label=\"RF\"),\n",
        "        plt.Line2D([0],[0], marker=\"^\", color=\"gray\", ls=\"none\",\n",
        "                   ms=6, label=\"GBR\"),\n",
        "    ]\n",
        "    ax.legend(handles=cat_patches+marker_patches,\n",
        "              fontsize=5.5, loc=\"upper left\", ncol=2,\n",
        "              framealpha=0.8)\n",
        "    ax.tick_params(labelsize=7)\n",
        "    ax.grid(True, alpha=0.2)\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "# MAIN\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # 1. Data\n",
        "    prices, ff = build_dataset()\n",
        "\n",
        "    # 2. Full analysis\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"RUNNING IN-SAMPLE + OOS ANALYSIS (all stocks)\")\n",
        "    print(\"=\"*70)\n",
        "    IS_perf, IS_nl, IS_dm, OOS_perf, OOS_dm = run_all(prices, ff)\n",
        "\n",
        "    # 3. Save CSVs\n",
        "    for name, df in [(\"IS_perf\", IS_perf), (\"IS_nl\",   IS_nl),\n",
        "                     (\"IS_dm\",   IS_dm),   (\"OOS_perf\",OOS_perf),\n",
        "                     (\"OOS_dm\",  OOS_dm)]:\n",
        "        df.to_csv(os.path.join(OUT, f\"{name}.csv\"), index=False)\n",
        "    print(\"✓ CSVs saved.\")\n",
        "\n",
        "    # 4. LaTeX tables\n",
        "    save_latex_tables(IS_perf, IS_dm, OOS_perf, OOS_dm)\n",
        "\n",
        "    # 5. Master figure\n",
        "    print(\"\\nGenerating master figure...\")\n",
        "    plot_master(IS_perf, IS_dm, OOS_perf, OOS_dm, IS_nl)\n",
        "\n",
        "    # 6. Quick summary\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"SUMMARY: DM TEST WIN COUNTS\")\n",
        "    print(\"=\"*70)\n",
        "    for label, dm_df in [(\"IN-SAMPLE\", IS_dm), (\"OUT-OF-SAMPLE\", OOS_dm)]:\n",
        "        print(f\"\\n{label}:\")\n",
        "        for regime in ([\"Full Sample\"] + REGIMES if \"Full Sample\" in dm_df[\"Regime\"].values\n",
        "                       else [\"Full OOS\"] + REGIMES):\n",
        "            sub = dm_df[dm_df[\"Regime\"]==regime]\n",
        "            if sub.empty: continue\n",
        "            parts = []\n",
        "            for ml in [\"RF\",\"SVR\",\"GBR\"]:\n",
        "                wins = (sub[sub[\"model2\"]==ml][\"winner\"]==ml).sum()\n",
        "                total = (sub[\"model2\"]==ml).sum()\n",
        "                parts.append(f\"{ml}:{wins}/{total}\")\n",
        "            print(f\"  {REGIME_SHORT.get(regime,regime):8s}: {' | '.join(parts)}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"DONE\")\n",
        "    print(\"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-YTYRmyXRhj",
        "outputId": "d9e7ecaf-2a60-4298-9fb9-b1b49489a0fd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "Downloading data: 2015-01-01 → 2025-09-30\n",
            "======================================================================\n",
            "Prices: 2701 days × 20 tickers\n",
            "\n",
            "======================================================================\n",
            "RUNNING IN-SAMPLE + OOS ANALYSIS (all stocks)\n",
            "======================================================================\n",
            "\n",
            "============================================================\n",
            "High-Beta Growth\n",
            "============================================================\n",
            "  NVDA (N=2700) ... Bear IS best=RF  OOS best=OLS\n",
            "  TSLA (N=2700) ... Bear IS best=RF  OOS best=OLS\n",
            "  META (N=2700) ... Bear IS best=RF  OOS best=OLS\n",
            "  AMZN (N=2700) ... Bear IS best=RF  OOS best=SVR\n",
            "\n",
            "============================================================\n",
            "Value\n",
            "============================================================\n",
            "  BRK-B (N=2700) ... Bear IS best=RF  OOS best=OLS\n",
            "  JPM (N=2700) ... Bear IS best=RF  OOS best=OLS\n",
            "  BAC (N=2700) ... Bear IS best=RF  OOS best=OLS\n",
            "  WMT (N=2700) ... Bear IS best=RF  OOS best=OLS\n",
            "\n",
            "============================================================\n",
            "Defensive\n",
            "============================================================\n",
            "  JNJ (N=2700) ... Bear IS best=RF  OOS best=OLS\n",
            "  PG (N=2700) ... Bear IS best=GBR  OOS best=OLS\n",
            "  KO (N=2700) ... Bear IS best=GBR  OOS best=OLS\n",
            "  PFE (N=2700) ... Bear IS best=RF  OOS best=OLS\n",
            "\n",
            "============================================================\n",
            "Cyclical\n",
            "============================================================\n",
            "  CAT (N=2700) ... Bear IS best=RF  OOS best=OLS\n",
            "  FCX (N=2700) ... Bear IS best=RF  OOS best=OLS\n",
            "  F (N=2700) ... Bear IS best=RF  OOS best=OLS\n",
            "  XOM (N=2700) ... Bear IS best=RF  OOS best=OLS\n",
            "\n",
            "============================================================\n",
            "Income/Dividend\n",
            "============================================================\n",
            "  T (N=2700) ... Bear IS best=RF  OOS best=OLS\n",
            "  VZ (N=2700) ... Bear IS best=RF  OOS best=OLS\n",
            "  MO (N=2700) ... Bear IS best=RF  OOS best=OLS\n",
            "  D (N=2700) ... Bear IS best=RF  OOS best=OLS\n",
            "✓ CSVs saved.\n",
            "✓ 6 LaTeX tables saved to /mnt/user-data/outputs/\n",
            "\n",
            "Generating master figure...\n",
            "✓ Master figure saved → /mnt/user-data/outputs/master_figure.png\n",
            "\n",
            "======================================================================\n",
            "SUMMARY: DM TEST WIN COUNTS\n",
            "======================================================================\n",
            "\n",
            "IN-SAMPLE:\n",
            "  Full Sample: RF:20/20 | SVR:7/20 | GBR:20/20\n",
            "  Bear    : RF:20/20 | SVR:4/20 | GBR:20/20\n",
            "  Normal  : RF:20/20 | SVR:0/20 | GBR:20/20\n",
            "  Bull    : RF:20/20 | SVR:0/20 | GBR:19/20\n",
            "\n",
            "OUT-OF-SAMPLE:\n",
            "  Full OOS: RF:0/20 | SVR:0/20 | GBR:0/20\n",
            "  Bear    : RF:0/20 | SVR:0/20 | GBR:0/20\n",
            "  Normal  : RF:0/20 | SVR:1/20 | GBR:0/20\n",
            "  Bull    : RF:0/20 | SVR:0/20 | GBR:0/20\n",
            "\n",
            "======================================================================\n",
            "DONE\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find / -name \"master_figure.png\" 2>/dev/null"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-D1dMt8Ccq3W",
        "outputId": "40675d58-8b14-4861-8792-bbfd8238f8ec"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/mnt/user-data/outputs/master_figure.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YKw0RqEwcss6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}